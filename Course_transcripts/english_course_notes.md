# üìò English Course Notes (Lang En Vs Only)

# üóÇÔ∏è Section 1: Analyzing Results Subtitles

## üìñ Lesson 1: 1   Introduction   Lang En Vs3

Okay. In the lessons up to now, you've figured out what metrics to use to evaluate your change. You've characterized those metrics, you've designed and at least preliminarily sized your experiment. Now that you've run your experiment, something we're not really covering the mechanics of here, you want to see what you can conclude from the data that you've captured. In this lesson, we'll go over how to analyze the results from your experiment, and determine what you can and what you cannot conclude. We'll start with the sanity checks we've discussed before, and then look at how to handle evaluation, whether you have a single or multiple metrics. Finally, we'll talk about analysis gotchas. As always, remember that running A/B tests is an iterative process. What you'll learn from this lesson may inform the steps that you've done up to now in designing your test as well.

## üìñ Lesson 2: 10   Checking Invariants, Part 2 Solution   Lang En Vs3

The standard deviation in this case comes to 0.0029. And, that means the margin of error is 0.0056. The confidence interval balance are then 0.4944 and 0.5056. The actual fraction in the control group is 0.5006. Which is within what is expected. So, this does pass the sanity check. Notice that the actual fraction in the control group wasn't exactly 0.5. And looking at the day by day data again, the number of events in the control and experiment groups wasn't exactly equal on any day. That's expected. And, since the fraction in the control group is within the bounds of the confidence interval, there's no reason to be alarmed. Since the total numbers pass the sanity check this time, there's no need to dig into the day-by-day data to figure out what's going wrong.

## üìñ Lesson 3: 11   Sanity Checking: Wrapup   Lang En Vs3

So what happens if one of your sanity checks fails? &gt;&gt; Well it's kind of like monopoly. If your sanity checks fail, do not pass go. Do not proceed, go straight to analyzing why your sanity checks fail. You really have to debug and understand what's going on before you can actually analyze your experiment. Because, if you try and move on past that, your conclusions are almost certainly wrong. &gt;&gt; Okay, so how do you figure out what went wrong? &gt;&gt; Well, there's a couple different avenues that you can try. First, something might have gone wrong technically, and you want to work with your engineers to understand, is there something going on with the experiment infrastructure? Did I get the experiment set up correctly? Is something wrong with experiment diversion? You really want to debug the experiment setup with the engineers. The second thing that you can try and do is a retrospective analysis. Try and recreate experiment diversion from the data capture, and understand that this is something endemic to what you're trying to do that may be causing the situation. The third thing is, we can try and use those pre and those post periods we talked about earlier in lesson four. Right? If you're in a pre-period, then you can say, did I see the same changes in those invariance in my pre-period? If I saw them in the pre-period and the experiment, that points to a problem with the experiment infrastructure, the set up, something along those lines. On the other hand, if you see the change only in your experiment but not in the pre-period, that points to something with the experiment itself, maybe the data capture or something along those lines. &gt;&gt; Okay, what are the most common reasons you've seen for data not matching up like this? &gt;&gt; Well, the most common one that I really see is data capture, especially when you want to capture a new experience that the user is undergoing. Right? And so maybe you just didn't capture it correctly. Maybe the change triggers very rarely, and you capture it correctly in the experiment, but you don't capture quickly in the control, and so you're not comparing like with like. That's probably the most common. But other reasons could be, the experiment's set up. So, for example, what happens if you have a filter to English only? Maybe you set up for the experiment, but not the control, and now your publishes aren't comparable. More rarely, it could be your infrastructure, or the ex, you know, the experiment system, something along those lines. Maybe there's something going on to really sort of reset cookies, and that's what's sort of screwing things up. You just have to go through and test all of them. &gt;&gt; Okay. And is it ever just a real difference? &gt;&gt; It could be, but it's probably not a good thing if it is. For example, there's something in the infrastructure that's going on to cause the cookies to churn. You probably don't want that, your user's probably not having a good experience, and it's not good for the experiment itself. Right? Now the key thing to remember is that all of these comparisons, they're approximations. So you're not going to get an exactly the same number. It's just going to be approximately the same. The other thing that I get asked a lot is, well, maybe my users are learning over time, maybe it's, you know, users are adapting to the change. Now, the key thing there is that, if there really is a learning effect, then you're going to see not very much change in the beginning, and it's going to be increasing over time. So if you're seeing a big change right from the beginning, it's probably not a learning effect. &gt;&gt; Okay. And then if all the sanity checks do pass, then we can analyze the experiment. &gt;&gt; Yes. Then you can analyze the experiment.

## üìñ Lesson 4: 12   Single Metric: Introduction   Lang En Vs3

Carey, first let's suppose you have a single evaluation metric, what do you do then? &gt;&gt; Well, the goal is to make a business decision about whether your experiment has favorably impacted your metrics. Analytically, that means you want to decide if you've observed a statistically significant result of your experiment. Now, typically we also want to estimate the magnitude and the direction of the change. Then once you have all that information you can make a decision about whether you want to recommend that your business actually launch this experiment. &gt;&gt; Let's talk about the first piece. How do you decide if the change was statistically significant? &gt;&gt; Well we're really just putting together all the pieces we've talked about earlier. In Lesson 3, we talked about characterizing the metric, understanding how it behaves. And in Lesson 4, we used that information about variability to estimate how long we needed to run the experiment for and size our experiment appropriately. Now we're going to use the results of both of those steps to try to estimate the variability we need to analyze the A B experiment as we talked about at way back at the beginning of the course. &gt;&gt; And what if our results are not statistically significant? What do you do then? &gt;&gt; Well that's a good time to take a much longer look at your results, especially if you were expecting a really noticeable difference. So for example, you might want to break it down into different platforms, different days of the week. This can not only help you find bugs in your experiment setup, but it might give you a new hypothesis about how people are reacting to the experiment. Now, if this is your first go around, you may also want to try cross checking your results with other methods. For example, you could use the non parametric sign tests that we talked about earlier. To compare the results to what you got from your parametric hypothesis test. &gt;&gt; Sounds good. Now let's go over some examples

## üìñ Lesson 5: 13   Single Metric: Example   Lang En Vs3

Okay. Let's suppose Audacity runs an experiment where they test changing the color and placement of the Start Now button. Their metric is click-through-rate and they divert by cookie. Their practical significance boundary is .01 and they use an alpha of .05 and a beta of 0.2. Now here are the results of the experiment which was run over seven days. And the total numbers are on the last row. I won't go over it now, but if you do a sanity check, you'll find that the number of page views is comparable between the two groups. So let's start analyzing. Now, in lesson one, we analyzed a similar experiment, but we measured click-through probability instead of click-through rate, so we could assume a binomial distribution. With click-through rate, like I mentioned in lesson four, the distribution is more likely to be Poisson, which is harder to deal with analytically than the binomial. So this time, in order to calculate a confidence interval, it's a good idea to estimate the variance empirically. In fact, Audacity already calculated the empirical standard error when they decided what size to use for their experiment. With a sample size of 10,000 page views in each group, they had measured the standard error to be 0.0035. As in lesson four, I can assume that the standard error is proportional to one over the square root of N. However, N here was the sample size of one group, which works well if there's the same size in both groups, as in the estimation of the standard error. But in our results, there's not the same number of page views in both groups. In this case, I can assume that the standard error is proportional to the square root of 1 over N1 plus 1 over n2, and this did work well if the ends are fairly close. This should also look familiar. It comes up in the formula for pooled standard error in the binomial distribution. So then the standard error for our experiment can be determined using this equation, where the empirical standard error divided by the empirical scaling factor is equal to the standard error for our experiment, divided by the scaling factor for our experiment, which involves the number of page views in both the control and the experiment group for our experiment. So then the standard error for our experiment comes out to 0.0041. Now we'll replace this table of results with just the total numbers, to make a little more room. Now I'll estimate the difference between the click-through rates and the control and experiment groups by subtracting the control estimated click-through rate from the experiment estimated click-through rate, and this comes out to 0.0300. Then, the margin of error is the standard error times 1.96, which comes to 0.0080. Then, the confidence interval is from 0.0020 to 0.0380. Based on this, I would recommend launching the experiment. The confidence interval does not include the practical significance boundary of 0.01, meaning I can be confident at a 95% confidence level that the true change is large enough to be worth launching. However, just to double check, let's look at the results of the sign test. To do this, I'll need to bring up the day by day data again. Here, to the clicks columns, I've also added the click through rate, so that's just the clicks divided by the page views for each row. To do a sign test, I need to know the number of days, which in this case is seven, and I also need to know the number of days with a positive change. So comparing the click through rate in each row, I can see that the experiment group actually had a higher click through rate on all seven days. This certainly looks good for the experiment, but what will be the chance of this happening randomly? If there was no difference, then there would be a 50% chance of a positive change on each day. So the question is if you flip a fair coin seven times, what is the chance it comes up heads seven times? This is the same calculation I did earlier to do a population count sanity check. Earlier, I used a binomial distribution where each event was a cookie, and the two outcomes were being assigned to the control group or the experiment group. Here, the event is a day, and the two outcomes are a positive change or a negative change, and the probability of each outcome is 50%. The main difference is that here, we cannot assume a normal distribution, since seven days is not enough for the binomial to closely approximate a normal. You can do the calculation by hand using the probability density of the binomial distribution, but there's also an online calculator that you can use which is linked in the instructor's notes. I'll use the online calculator. First, I'll need to enter the number of successes I observed. I'll define a positive difference, or the experiment group having a higher click-through-rate than the control as a success, so there were seven successes out of seven total days. Then I'll leave the probability at 0.5 and click Calculate Probabilities. The number I'm interested in here is the two-tail P value, which is 0.0156. That's the probability of observing a result at least this extreme by chance. Since this is less than the chosen alpha of 0.05, the sign test agrees with the hypothesis test, that this result was unlikely to come about by chance. Given this, my recommendation would not change. I would definitely launch the experiment with increased confidence in the experiment results. Now, I want you to do the same analysis for this table of results. Again, the metric for the experiment was click-through rate, and the practical significance boundary is 0.01, and the alpha is 0.05. These are the results for the first week of the experiment, and these are the results for the second week. I've put the click-through rate instead of the number of clicks in these columns to make the analysis a little easier. You can also find these results in the spreadsheet linked in the instructor's notes. Audacity also empirically estimated that the standard error for the click through rate to be .0062 with a sample size of 5,000 page views in each group. Like I did, you should both do a sign test on the day by day data and computer confidence interval of the effect size, that is, the difference between the click through rate in the control group and the experiment group. Click the lower and upper balance of your confidence interval in these boxes and the the two tailed p value for your sign test in this box, all to four decimal places. For each test, are the results statistically significant that the alpha equals 0.05 level? Finally, discuss on the forums what recommendation you would give if you saw these results in an experiment you were running. Would you recommend to launch? Not launch? Dig deeper into something strange looking? Would it be a judgment call that depended on the business strategy? Once you've discussed your thoughts, check this box.

## üìñ Lesson 6: 14   Single Metric: Example Solution   Lang En Vs6

I'll start by calculating the confidence interval of the effect size for which I'll only need the total numbers. First, I'll calculate the estimated difference, which is the experiment click through rate minus the control click through rate and comes to 0.0116. Next, I'll need to calculate the standard error for the sample size we have. I can do this using the same method as before, assuming that the standard error is proportional to the square root of 1 over n1 plus 1 over n2. This gives that the standard error for this experiment is 0.0026. To get the margin of error, I'll multiply the standard error by the z score and I get 0.0051. Then the confidence interval goes from 0.0065 to 0.0167. Since the confidence interval does not include zero, these results are statistically significant. That is, it's unlikely that there was no real difference. However, the confidence interval does include the practical significance boundary, meaning that I can't be confident at the 95% level that the size of this effect is something that I care about. Now to do the sign test, I'll need to look at the day-by-day data again, and I'll count the number of days where the click through rate is higher in the experiment group. I've checked the days where this is true, and it's turned out to be the case for 9 out of 14 days. Putting 9 and 14 into the sign test calculator, and clicking Calculate Probabilities, I get that the two-tailed value is 0.4240. This is pretty likely, and it's definitely not significant at an alpha equals 0.05 level. So the hypothesis test on the effect size showed statistically significant results, but the sign test didn't. Why could this happen? Well, first off, the sign test has lower power than the effect size test, which is frequently the case for nonparametric tests. That's the price you pay for not making any assumptions. So, this isn't necessarily a red flag, but it's worth digging deeper and seeing if we can figure out what's going on. I'll take a look again at which days showed a positive effect size. It looks like it was Tuesday, Saturday, Sunday, Monday, Tuesday, Thursday, Friday, Saturday, Sunday. Now these aren't necessarily strange results. But one thing to notice is that all four weekend days were positive. In fact, if you look at the click-through rate in the experiment group on the weekends, it's much higher than the click-through rate on any other day, even the other days that did show a positive change. This suggests that maybe the change has a small effect or no effect during the week, but a larger effect on weekends. I won't step through calculating the confidence interval for weekdays and weekends separately, but if you do, these are the results you'll get. It looks like the weekdays don't have a statistically significant difference, but the weekends do for at least the difference of 0.036. That's consistent with the hypothesis that the weekends have a large effect and the weekdays have no effect. The signed test results are also consistent with this hypothesis. If there were no effect on weekdays, you would expect half the weekdays to be positive, which is exactly what we see, and all the weekends to be positive, which again, is what we see. Now, there are two main questions to answer in deciding what recommendation to make for this experiment. The first is whether it's okay that only weekends seem to have improved, and the second is whether the magnitude of the change makes it worth launching. Remember, the practical significance level was 0.01, and the confidence interval for the overall effect included this boundary. The weekend effect was stronger than the practical significance level, but you may want a bigger practical significance level for a change that only affects weekends. Based on this, I would recommend not launching the experiment at this point. Instead, I would dig deeper into why the change didn't affect weekday visitors. Once I understood that, I might have an idea for how to iterate on the change to help it affect more of the users. If not, then I'd talk to the decision makers about whether a change of this magnitude on weekend traffic is worth launching.

## üìñ Lesson 7: 15   Single Metric: Gotchas   Lang En Vs3

What do you do when the sign test and the hypothesis test don't agree? Or what if you have significant results on weekends but not on weekdays? &gt;&gt; Well, the first thing you want to do as we talked about is take a really critical look about how your feature functions. Is it really operating differently for different subgroups for example on a particular platform. But there are real statistical reasons that you might get counter-intuitive results. One of them that shows off common lane experiments is called Simpson's paradox. What it means is that there's a bunch of different subgroups in your data like user populations. And within each subgroup, the results are stable but when you aggregate them all together, it's the mix of subgroups that actually drives your result. &gt;&gt; That doesn't even sound possible. Can you give me an example? &gt;&gt; So the example that most people like was actually a real example about graduate admissions at Berkeley. So what was happening is that in aggregate, if you looked at the number of people who are accepted divided by the number that applied, the rate of women being accepted was statistically significantly lower than men being accepted, which seemed bad. But when you looked at it by department, there were actually departments where women were accepted at a higher rate than men. So how can that be? The answer turned out to be that you had to look at it by department, because the acceptance rates by department were variable, from 6% to as much as 60%. And what was happening is that more women were applying to the smaller departments that had a very low acceptance rate. So when you aggregated the numbers, ignoring department, you saw women accepted at a lower rate. But if you looked at each department individually, the rates were very comparable between men and women. &gt;&gt; Okay, and how does this apply to our experiment? &gt;&gt; Well, you can get this happening on our experiment because you have these subgroups like people who use it more on weekdays or people use it more on weekends. And you may find that, for example, new users are correlated with weekend use and you know, experienced users who react differently are correlated with weekday use. And so what sometimes you find in these cases is that what drives the results of your experiment are how many people from each group you get. Within each group, their behavior is stable, you can get a statistically significant result or an insignificant result. But when you add them all together, all the changes in your traffic mix are driving the results. In a minute, Caroline's going to take you through an example of how that happens when you do an experiment analysis.

## üìñ Lesson 8: 16   Gotchas: Simpson‚ÄôS Paradox   Lang En Vs3

Now lets take a look at a simplified version of the Berkley paradox with only two departments. As you can see more men applied to to department A than department B and vice versa for women. Now if I calculate the percentage of men who were accepted to department A I get 62% and if I do the same for women, I get 82%. So acceptance is fairly high and more women are accepted. Now if I do the same for department B I get that the overall acceptance rate is lower but still more women were accepted than men as a percentage of the applicants. However now let's look at the total numbers of men and women who applied and were accepted across both departments. Overall, a greater percentage of men have been accepted than women. How can this happen? Like Kary said, the explanation is that more women applied to department B, which has a lower acceptance rate. So that a lower percentage of women are accepted overall. Now the actual Berkeley data was more complicated than this. There were more than two departments, and in some departments men had a higher acceptance rate than women. But not a statistically significant difference, and most departments had a small but significant bias in favor of women. Now, how can this apply to AB testing? Well, you could see the same sort of think happen for click through rate. Where if you look at your total or aggregate experiment results, then the experiment group has a lower click through rate than the control group. But then if you break it down by new users and experienced users, you see the reverse. How could this happen? Actually, I want you to fill in the numbers. You should be able to use a similar strategy to the Berkeley example. Once you're done, you can take a minute to think about what those results would mean but while you're coming up with the numbers don't worry too much about how plausible they are. Fill in any numbers such that the click through rate is higher in the experiment group than the control group for both new and experienced users. But overall, the click through rate is lower in the experiment group. You only need to fill in these two rows but if you also want to fill in the total row to help keep track of your work you can do that too.

## üìñ Lesson 9: 17   Gotchas: Simpson‚ÄôS Paradox Solution   Lang En Vs2

Here are some numbers that show the pattern I mentioned. To make the pattern easier to see I've added the click-through-rate in parentheses here. By the way, you could also have used the same numbers from the Berkeley example and gotten the similar pattern. So you can see that overall the click-through-rate is lower in the experiment group than in the control group. But, for both new users and experienced users, the click-through-rate is higher in the experiment group. The new users also have a higher overall click-through-rate than the experienced users, which explains why the click-through-rate is higher in the control group, which had more new users. Now wait a minute, why are there more page views from new users in the control group than in the experiment group? If the assignment to the control in the experiment group is random, then shouldn't the new users be evenly split between the control and experiment? And same for the experienced users. The answer is they probably should be evenly split. Earlier, you learned it's a good idea to make sure the number of page views is the same in the experiment group and the control group as a sanity check. Checking that breakdown across different slices could also be a good sanity check. The most likely reason you would see a result like this is that something is wrong with your experiment setup. However, it's also possible to get skewed numbers like this, even if your setup is correct, if your change, or experiment, affects new users and experienced users differently. Suppose you're diverting based on user ID and the change makes new users generate fewer page views, for example, they refresh the page less, and experienced users generate more page views. That explains why there are more page views in the experiment group for experienced users and more page views in the control group for new users. Now when making a recommendation for this experiment it might seem tempting to say that the experiment was a success. After all, it was an improvement for both new users and experienced users. The reason it wasn't an improvement overall is because of the fact that the new users and experienced users weren't split evenly between the experiment and control group. However, you really need to dig deeper and figure out why the page views for new and experienced users are not being split evenly between the two groups. Whether it's a faulty experiment set up, or something where your change affects new and experienced users differently, you won't be able to make a valid conclusion until you understand what's going on.

## üìñ Lesson 10: 18   Multiple Metrics: Introduction   Lang En Vs2

What changes when you have multiple evaluation metrics instead of just one? &gt;&gt; Well one thing that comes up when you run evaluations of multiple metrics at the same time, is that the more things you test, the more likely you are to see significant differences just by chance. So if you're testing 20 metrics, and you have a 95% confidence level. You would expect to see one case at least that time where you got a result that says it's significant but it's only concurring by chance. So this is a problem, but you're not sunk because it shouldn't be repeatable. That is if you did the same experiment on another day or you divide or just slices or you did some bootstrap analysis, you wouldn't see the same metric showing up as significant differences every time, it should occur randomly. There's another technique for this called multiple comparisons that adjusts your significance level, so that it accounts for how many metrics or how many different tests you're doing. &gt;&gt; All right when would I use that? &gt;&gt; One thing a lot of people like to do an experiment frameworks is do automatic detection of differences. So if you're doing exploratory data analysis you can reanalyze your data and make sure the same metric isn't popping up every time and see if the differences are repeatable. But if you want to set up say automatic alerting that tells you, one of my metrics was significantly different on this experiment. You probably want to use multiple comparisons and there's notes for that in the instructor's notes.

## üìñ Lesson 11: 19   Multiple Metrics: Example   Lang En Vs2

Let's take a look at an experiment that tracks multiple metrics. And suppose that Audacity sometimes prompts students when they miss quizzes, asking if they'd like to contact a coach. And they run an experiment that makes this message appear more frequently. There are a few different metrics they could track here. First, they might track the probability that a student signs up for coaching at any point during the course. Second, they might track how early in the course students sign up for coaching. So something like the average amount of progress a student makes before enrolling for coaching. If a student never signs up for coaching, their progress might be 100%. Finally, if coaching is priced differently, depending on how early in the course the student signs up, Audacity might track the average price paid per student. If Audacity tracks all three metrics and then does three separate significant tests, with alpha equals 0.05 for each. Then what is the probability that at least one metric will show a significant difference, given that there is no true difference? In other words, for three metrics, what is the chance of at least one false positive? To make the problem easier, I'll first calculate the probability that there are no false positives. That is FP, for false positive, equals 0. The chance that each individual metric does not show a false positive is 95%. Now in order for none of the metrics to have a false positive, the first one can't show a false positive, which happens with probability 0.95. And neither can the second, so I need to multiply by 0.95. And the third can't have a false positive either, so I multiply by 0.95 again. So the chance that none of the metrics shows a false positive is 0.95 cubed, which is about 0.857. Then the probability that there is at least one false positive is 1- 0.857. Which is a 14.3% chance of any false positive. Now I made an assumption in this calculation. When I multiplied the probabilities together, I was assuming that the metrics were independent. In fact, this isn't true here. These three metrics are all related and more likely to move together. So 14.3% is an overestimate of the probability of a false positive. But assuming independence is an easy way to get a conservative estimate. Now suppose you ran an experiment with ten metrics and you used a 95% confidence level for each metric. Then what would be the probability that any of those metrics would show a false positive? Or what if you still had ten metrics but you used a 99% confidence level for each metric? Write your answers in these boxes. Each should be a number between 0 and 1, not a percent chance. And you should give each answer to three decimal places. Assume that the ten metrics are independent of each other.

## üìñ Lesson 12: 2   Sanity Checks   Lang En Vs3

Diane, I've got data from my experiment. Can we look at the click-through rate and see whether the change was an improvement? &gt;&gt; Hold your horsies, there! Not quite yet. Before we can look at click-through rates to determine what happened in your experiment, first we have to do all those sanity checks in order to ensure that your experiment was run properly. &gt;&gt; Hm, what kind of sanity checks? &gt;&gt; Well, there are all sorts of things that can go wonky, that could really invalidate your results. For example, maybe something went wrong in the experiment diversion and now your experiment and your control aren't comparable. Or maybe you used some filters and you set up the filter differently in the experiment and the control. Or you set up data capture and your data capture isn't actually capturing the events that you want to look for in your experiment. Or you know I could just kind of keep going. Remember in lesson three when we talked about counting clicks? Well all those ways that counting clicks can go wrong? We have to test for all of those before you can look at the results of your experiment. &gt;&gt; Okay, how do I test for those things? &gt;&gt; So do you remember back in lessons three and lessons four when we talked about invariant metrics? You're going to check those. And there are really two main types of checks. The first one is going to be the population sizing metrics, based on your unit of diversion. Now what you're really checking there is that your experiment population and your control populations are actually comparable. The other check are those actual invariants, those metrics that shouldn't change when you run your experiment. And what you want to do is you want to test that they didn't change. Now, once you've done those sanity checks, then, and only then, should you be doing the more complicated analysis to determine, hey, was my experiment good, was it bad, what's my recommendation going to be?

## üìñ Lesson 13: 20   Multiple Metrics: Example Solution   Lang En Vs3

For three metrics and a confidence level of 95% the answer was one minus 0.95 cubed. In general the overall probability of at least one false positive, which I'll call alpha overall, will be one minus the confidence level for that test, raised to the nth power. The confidence level is equal to one minus the individual alpha. And N is the number of metrics you are measuring. Again, this formula assumes independence. Thus in the first case the answer is 1 minus 0.95 to the tenth. Which is 0.401. And in the second case, the answer is one minus 0.99 to the tenth. Which is 0.096. So with a 95% confidence level, the overall chance of a false positive is quite large. Getting close to half, and even the 99% confidence level isn't enough to limit the overall chance of an error to 5% or less. It's closer to 10%. This plot shows the overall alpha. That is, the chance that any metric shows a false first as the number of independent metrics being tested for three different confidence levels. Alpha equals 0.1, 0.05, and 0.01. For alpha equals 0.01 or 0.05. The over all alpha blows up quickly. By the time you have five metrics with an individual alpha of 0.05. Your overall alphas almost a quarter. One alpha equals 0.01 or 99% confidence level. The overall the alpha looks more manageable by comparison, but like you just saw if you have ten metrics you have a total false positive rate of almost 105 and that keeps steadily going up as you increase to 20 metrics.

## üìñ Lesson 14: 21   Multiple Metrics: Example 2   Lang En Vs2

Like you just saw, the main problem with tracking multiple metrics is that a false positive, which should be a rare event, becomes more common as you increase the number of metrics you're measuring. As you might guess, the main way to fix this is to use a higher confidence level for each individual metric in order to bring down the overall probability of a false positive. Exactly what confidence level you use depends on how conservative you want to be. The first method you might think of is to assume that the metrics are independent of each other. Then you can use the same equation that I used earlier to calculate the overall alpha. But instead, this time you would set the overall alpha to be what you wanted, maybe .05. And then solve for the individual alpha you needed in order to get the overall alpha you're happy with. However, there's a different method that people are more likely to use in practice called the Bonferroni correction. The Bonferroni correction has two main advantages. It's very simple to calculate and it doesn't make any assumptions. That is, it doesn't assume independence like our method one did. The Bonferroni method is also very conservative. It's guaranteed to give an alpha overall at least as small as you specified. And again, it does this without making any assumptions. Depending on your use case, this might or might not be what you want. To use the Bonferroni correction, you calculate the individual alpha you should use for each metric by taking the overall alpha you want and dividing by the number of metrics. So if you have N equals three metrics, and you want your overall probability of a false positive to be 0.05 or less than the individual alpha that you would use for the significance test of each metrics. Would be 0.05 divided by 3 or 0.0167. If you want to see a proof of why this method works, see the instructor's notes. The main problem with the Bonferroni correction is that often, you'll be tracking metrics that are correlated and all tend to move at the same time. In which case, this method is too conservative. In our coaching example from earlier. If users are adopting coaching earlier in the course, it's likely that the probability of adoption is also going up and the price is probably more likely to change also. So, in this case, the Bonferroni method is probably too conservative for Audacity's need. That will lead to them launching fewer experiments than they would like. Now, suppose Audacity runs an experiment where they update one of the descriptions on the course list. We talked about some metrics that might be good for this experiment in lesson three. They almost certainly want to track the probability of clicking through to the course overview page from the course list. But it's also a good idea to track continued progression down the funnel to catch whether the updated description is misleading. So suppose, Audacity also measures the average time spent reading the course overview page. The probability of enrolling in the course, given that you reached the course overview page, and the average time spent in the classroom for this course during the first week after enrollment. Now, here is the measured difference as well as the standard error for each metric. You can assume that each metric is normally distributed. Now, which of these metrics showed a statistically significant change? First, determine which metrics showed us statistically significant difference, with the individual alpha for each metric at 0.05. Next, set your overall alpha to 0.05 and use the Bonferroni correction to calculate the individual alpha. As a reminder, here's how to calculate the individual alpha using the Bonferroni correction. For each method and each metric, check the box if that metric showed a statistically significant difference. Do you think the Bonferroni method is overly conservative here or is it appropriate?

## üìñ Lesson 15: 22   Multiple Metrics: Example 2 Solution   Lang En Vs2

At a 95% confidence level or an alpha of 0.05, the Z-score is 1.96. I'll multiply this Z-score by each standard error to get the margin of error in each case. Now, for the first case, the margin of error is smaller than the observed difference, which means the confidence interval won't include zero. So, the difference is significant. In the second two cases, the margin of error is also smaller than the magnitude of the observed difference. So, these changes are significant also. In the last case, the margin of error is wider than the observed difference, so this is not statistically significant. Using the Bonferroni method, I get an individual Z-score of 2.5. It's actually between 2.49 and 2.5, but it's closer to 2.5, so i'll use that. To get this, I divided the overall alpha of .05 by 4, and looked up the result in a Z-score table. Multiplying this by the standard errors, I get these margins of error. Now, it turns out that each margin of error is now larger than the corresponding difference, so none of these metrics have a significant change using the Bonferroni method. In this case, the Bonferroni method is probably too conservative. If the course description was an improvement, then it makes sense that it could cause more than one of these metrics to move and they're probably more likely to move together.

## üìñ Lesson 16: 23   Multiple Metrics: Example 3   Lang En Vs2

We just saw an example experiment where three out of four metrics had a statistically significant difference at the alpha = 0.05 level for each metric individually. But when I combined the metrics and tested for significance using the Bonferroni correction, none of the metrics were significant. So how do I actually make a recommendation here? The Bonferroni method is likely to be too conservative, since I expect the metrics to be correlated, but my rigorous results say that the change isn't significant. The rigorous answer here is to use a more sophisticated method than Bonferroni, ideally one that takes into account the fact that the metrics are likely to be correlated. There's a list of other possible methods in the instructor's notes. There's also another type of strategy here that's different than just choosing an individual alpha, and I'll talk about that in a minute. Ideally, I would have decided what method to use when designing the experiment, and taken that into account when sizing. Practically speaking, this type of situation may come down to a judgment call. If I have a lot of experience running different experiments with these metrics, then I would have a good intuition for whether these results are likely to persist or not. In this case, I would need to communicate that to the decision makers to make sure they understand the risk. There may also be other factors here such as business strategy. The decision makers may want to launch the change if there are strong, strategical reasons to launch it. Or otherwise they may choose to abandon it, given the uncertainty of the results. Or if it's not urgent, they may want to run another set of experiments that are more adequately powered. So what's the other strategy that I mentioned a minute ago? Well, the methods we've used up to now try to control the probability that any metric shows a false positive. I've been calling this probability the overall alpha. But it's more commonly called the family wise error rate, or FWER. Another approach is to loosen that and say you're okay with some false positives, even with a high probability, as long as there's not too many. In this case, you would try to control what is called the false discovery rate, or FDR. The false discovery rate is the number of false positives, that is, the number of times that you reject the null, even when the null was true, divided by the total number of rejections of the null, both valid and invalid. And actually, it's the expected value of this quantity. What the false discovery rate is measuring is, out of all of the rejections of the null, that is, all of the metrics that you declare to have a statistically significant difference. How many of them had a real difference as opposed to how many were false positives? This really only makes sense if you have a huge number of metrics, say hundreds. So suppose that you have 200 metrics that you're measuring and you capped the false discovery rate at 0.05. What this means is that you're okay with having 5 false positives and 95 true positives in every experiment. The family wise error rate, or the overall alpha in this case, would be one, since you have at least one false positive every time. But the false discovery rate is 0.05. Since most of the metrics that you're claiming have a significant difference actually do. So if you're trying to detect significant changes across a large number of metrics, then capping the false discovery rate instead of the family wise error rate can be a lot more lenient. See the instructor's notes for more details about controlling the false discovery rate and some methods you can use to do so.

## üìñ Lesson 17: 24   Analyzing Multiple Metrics   Lang En Vs2

Let's say I've settled on a bunch of metrics. How do I actually make a recommendation? &gt;&gt; So especially when you have multiple metrics, it can be a little bit tricky because you have multiple comparisons or things along those lines. Now what you're really hoping for is that related metrics are all going to move in the same direction. For example, if you're measuring both click-through rate and click-through probability, hopefully they'll move in the same direction. Another thing might happen when you have a composite metric for example. RPM which is revenue per thousand queries. Is composed of both click through rate. As well as cost per clicks and so if you see. RPM of them one direction. Hopefully you can look at click through rate in cost per click to understand why it moved in that direction. Now that said, multiple metrics can be unruly. Let's say you've decided that reading time on the page or stay time is a good signal. People like your page. But clicks are also a good signal. And so then you might see that when you make a UI change to the page people spend less time reading, but more time clicking. And then you really have to understand how people are reacting to the change because you can't quantitatively evaluate which one is better. That makes sense. I'm guessing that's why sometimes people want to single overall evaluation criteria, so they can make the decision based on that? &gt;&gt; That's right. I think the main question when you try and come up with an OEC, or an overall evaluation criteria, is how do you find a good one? You really need to understand what your company is doing and what the problems are in order to try and come up with saying. Ok well how do I balance for example state time and clicks is it seventy percent and thirty percent. Is it twenty five and seventy-five. Who knows or maybe. Quite frankly. Clicks is a bad measure for. User experience. On your web site all together. Right you really have to understand. The thing about an OEC is that it doesn't absolve you of understanding why stay time and clicks are moving in these different directions. What it can be helpful with this balancing long term investment like return visits to the site with short term day to day metrics like increased clicks. You don't want to lose one in pursuing the other one. And so often the best OICs give you a good balance between those two things. &gt;&gt; Okay. And how do you come up with one? &gt;&gt; Well typically there's a lot of validation in the process but they usually start with some kind of business analysis right so you might &gt;&gt; Our company, as a whole, wants to look at 25% revenue plus 75% increase usage of the site. So usually you start from there. And once you have a couple of candidates, you want to actually run a whole bunch of different experiments. And validate how they steer you. You know, are they steering you in the right direction. Now the downside is you want to make sure that you don't plan so much around what the company thinks should happen with those experiments that you steer yourself in a way that you hide other changes that you weren't expecting. &gt;&gt; At Google we didn't up doing a little bit. Bit differently. We actually took hundreds of experiments that we had already run and we had made launch decisions about. And we got all of the decision makers into a room and we gave them all of those experiment results. We didn't tell them what the experiment was testing. And we didn't give them the actual launch decision. And we made them re-decide based solely on the experiment results whether they would launch change or not. From there we could say ok well. How much did each of you agree with each other and how did you agree with the actual launch decision, and using that we actually re-derived the weights for an OAC. Now for us, we came up with an OAC that way, but whenever we tried using it to make a launch decision everyone sort of look at the OAC and then be like, what were the individual metrics that actually changed. So we never actually ended up using the OIC to actually make a decision. Now what the sort of tells us is that having an OIC doesn't have to be a formal number. It's really just trying to encapsulate what your company cares about. And how much you're going to be balancing something like stay time and clicks.

## üìñ Lesson 18: 25   Drawing Conclusions   Lang En Vs2

Once I've figured out which metrics have significant changes, what comes next? &gt;&gt; Well, now you have to decide what your results do and don't tell you. If you have statistically significant results, then that means that you're unlikely to have zero impact on the user experience. But now the questions come down to A, do you understand the change? And B, do you want to launch the change? Okay, so what if I have a statistically significant change in some metrics but then in others, I don't. Well, that goes back to A, do you understand the change? And having intuition and experience with lots of other experiments can really help here. For example, maybe you know that for small changes, a change in one metric but no change all the other metrics is perfectly fine. But if you were to see those same results for a big change, that would probably indicate that there's something going wrong. &gt;&gt; Okay, and the same question for slices. What if your change has a positive impact on one slice of your users, but then, for another slice, there's no impact or there's a negative impact? &gt;&gt; Again, do you understand why? Is it a question about having different users, and how much they like or don't like the change? Have you seen that effect in other experiments? Do you have a bug? These are all possibilities when you have changes in one slice versus another. Let me give you an example. Bolding, right, we like to basically bold certain words in order to give them emphasis. Now, bolding in like English or German, or those types of alphabets, works really well for providing emphasis. But if you try and bold in Japanese, or Korean, or Chinese, a bolded character is actually really hard to read. And it actually makes it harder for a user to read the word than if it was not bolded. And so in that situation, you may actually want to want something different for that slice. Maybe use a different color as opposed to doing a bold. &gt;&gt; That makes sense. And then, bottom line, how do you decide whether to launch your change or not? &gt;&gt; You really have to ask yourself a few questions. One, do I have statistically significant and practically significant results in order to justify the change? The second question is, do I understand what that change has actually done with regards to user experience? And the third one is, is it worth it? Now the key thing to remember is that the end goal is actually making that recommendation that shows your judgment. It's not about designing the experiment and running it and sizing it properly, and having all of the metric chosen correctly, and all those different things. Those are all signposts towards your end goal of actually making a recommendation. And what's going to be remembered is, did you recommend to launch it or not and was it the right recommendation?

## üìñ Lesson 19: 26   Gotchas: Changes Over Time   Lang En Vs2

Is it ever a good idea to ramp up your experiment? That is, maybe you start with 1% of your traffic as being diverted to the experiment group, but then you gradually increase that until your feature is fully launched. &gt;&gt; Actually, I think it's good practice to always do a ramp-up when you actually want to launch a change. In fact, we do that for all of our launches at Google. The other thing that we do is that we remove all of the filters. So if you're only testing your change on English, for example, you want to test your change during your ramp-up on all users. Because what you want to know is if there's been any incidental impact to unaffected users that you didn't test in the original experiment. Now, there's one interesting complication or gotcha that we do have to deal with in our ramp-up wherein so that the effect may actually flatten out as you ramp up the change. &gt;&gt; Wait, even if the initial effect was statically significant? I thought the point of statistical significance is that your results will be repeatable. &gt;&gt; Well, there can be some really simple reasons as to why the effects are not repeatable. For example, there's all sorts of seasonality effects. Students, for example. When students go on summer vacation, the behavior across broad swaths of the Internet changes. And when they come back from vacation, it changes again. There might be other seasonal effects like holidays. Everyone's heard of Black Friday and Cyber Monday. So shopping behavior can change a lot around the holidays. Now, one way to try and capture these seasonal or event-driven impacts is something that we call a holdback. And the idea is that you launch your change to everyone except for a small holdback, or, basically, a set of users, that don't get the change, and you continue comparing their behavior to the control. Now, in that case you should see the reverse of the impact that you saw in your experiment. And what you can do is you can track that over time until you're confident that your results are actually repeatable. And that can really help capture a lot of your seasonal or event-driven impacts. &gt;&gt; Okay, is there anything else that can cause this disappearing launch effect? &gt;&gt; Well, we've talked about this a couple times in the other lessons but there can be a novelty effect or change aversion. Those humans they're so troublesome. They keep changing their behavior, right? But as users discover or adopt or change their adoption of your change, then their behavior can change and therefore the measured effects can change. And this is where a cohort analysis can really be helpful. Now, another situation that may also happen is that if you have advertisers that have budgets, a lot of times, if you don't control for the budgets properly, the effect can change as you ramp up. &gt;&gt; Okay, and how would I catch things like this during the experiment, rather than after the launch has happened and something went wrong? &gt;&gt; This is where you get into the more complicated analyses and in the more complicated experiment designs. We had talked in lesson four about using pre- and post-periods. And so if you're really concerned about a learning effect, you probably want to use those pre- and those post-periods, combined with a cohort analysis, to try and understand how your users are basically adapting to the change over time.

## üìñ Lesson 20: 27   Lessons Learned   Lang En Vs2

Let's wrap up. What are the main things we've learned? &gt;&gt; First you need to check. Double check, triple check that your experiment was set up properly. Look at your end variance. Check that your experiment metrics are actually looking sane. Right, you have to check that before you try and do anything else. &gt;&gt; And remember you aren't just looking for statistical significance. You're really making a business decision. So what if you have an experiment that statistically significantly improves the experience for 30% of users but it's neutral for everyone else. Do you necessarily want to launch it as it is or do you want to try to make it better. And what if you're in a situation where it improves things for 70% but makes it worse for 30%. So you actually need to make a decision about whether you want to launch the feature as it is, or if you want to try to fine tune it first. &gt;&gt; And you can't actually forget the overall business analysis as well. For example, what's the engineering cost of maintaining the change? Are there customer support or sales issues? Overall what's the opportunity cost if you actually choose to launch the change relative to the rewards you're going to get from from the change or potentially not launching the change? This is all about the judgment call with regards to the user experience and the business. And ultimately that's what your commission has to be based on. &gt;&gt; That makes sense, any other last thoughts? &gt;&gt; Well, if you haven't actually tested your change on all traffic, even if it only impacts a small slice, now's a good time to test for that incidental impact. &gt;&gt; Yeah and if this is the first experiment you run, especially if it's a really big or important feature, you might want to actually try running a couple of different experiments. Analyzing the results and then seeing how you feel about the results of your first experiment before you decide to launch it.

## üìñ Lesson 21: 28   Course Conclusion   Lang En Vs2

Congratulations, you finished lesson five. In this lesson we covered the fundamentals of the handling the results of your experiment. Some gotchas for single and multiple metrics. And suggestions for converting significant results into a real business case for your experiments. &gt;&gt; This is also the end of the course. We hope you enjoyed it. And we want to hear what you thought. Please go to the forums and tell us what was your favorite part about the course. And do you have any suggestions for what we could improve? &gt;&gt; Absolutely and now that you have all the basic pieces try it out on your own. Experiment and see how it works.

## üìñ Lesson 22: 3   Choosing Invariants   Lang En Vs3

The first step to sanity checking our results is choosing a set of invariant metrics. Like Diane mentioned, there are two types of invariant metrics. Population sizing metrics, and any other metrics you don't expect to change. Now, I'm going to describe two experiments and for each one, I want you to think about what metrics should be invariant. That is, what would be the same in the control and experiment group? In the first experiment, Audacity changes the order of courses in the course list to see how much this affects which courses users eventually enroll in. The unit of diversion is a user ID since users may visit the site multiple times to explore classes before finally enrolling in one. In the second experiment, Audacity changes the infrastructure serving videos hoping to reduce the video load time. This time, the unit of diversion is an event. Now, which of the following metrics would be good invariants? Number of signed in users, number of cookies, number of events, click-through rate or CTR on the Start Now button. Remember, this button takes the user from the homepage to the course list. Or finally, the time it takes a student to complete a course on average. Check each box if that metric would be a good invariant for that experiment.

## üìñ Lesson 23: 4   Choosing Invariants Solution   Lang En Vs3

In the first case, the number of signed in users is definitely a good population sizing metric. Since the unit of diversion is the user-id, the signed in users are being randomly assigned to the experiment and control groups. So, you should definitely have roughly the same number of users in each group. Now, number of cookies and number of events are a little more tricky. They're not being directly randomized. But, they should still be split evenly between the two groups. Unless users in the experiment tend to clear their cookies more often, or visit fewer pages, or something like that. If that's happening,it's probably not a good thing, and it's definitely something you want to know about. So, these are also good population sizing invariants. The click-through rate on the Start Now button is a good invariant metric. Since users click the Start Now button before they see the course list, the click-through rate on the Start Now button shouldn't be affected by this change. The time taken to complete a course, on the other hand, could be affected by this change. If ordering the courses differently does cause users to enroll in different courses, then that could change how long it takes users to complete the courses they do enroll in. For example, maybe putting easier courses first causes more users to start with easier courses, and then they finish them faster. So, this would not be a good invariant. In the second case, again, the first three metrics will all be good population sizing invariants. Number of events is good since this is the unit of diversion. So, this is being randomly assigned between the experiment and control groups. Signed in users and cookies are both larger than the unit of diversion, in the sense that one user or one cookie could correspond to multiple events. So, since the events are being randomly assigned, the number of signed in users and cookies shouldn't be different between the two groups either. The click-through rate of the Start Now button would be a good invariant metric, since users click Start Now before viewing any videos. There could be a learning effect, but you won't catch learning effects if you're diverting by event anyway. The time to get through a class can't be tracked if you're using event-based diversion. Since by the time the user gets through a course, they could have been assigned to both the experiment and the control group multiple times. Even if you could track this, it wouldn't be a good invariant, since load time could affect how long it takes to complete a class. For example, users could switch tabs if videos load slowly, and end up getting distracted, and taking longer to complete the course.

## üìñ Lesson 24: 5   Choosing Invariants, Part 2   Lang En Vs3

Now consider invariant metrics for another experiment. This time, Audacity changes the location of the sign in button to see if they can get users to sign in sooner. The sign in button currently appears on the course list page, and if a user who isn't signed in tries to enroll in a course, they are prompted to sign in. But in the experiment group, the sign in button is added to every page, including the home page. The unit of diversion is a cookie, since Audacity wants a consistent experience across the sign-in sign-out boundary. Again, which metrics would make good invariants? Number of events, click through rate or CTR on the start now button, the probability of a user enrolls in a course. The sign in rate, that is, the rate at which users who are not signed in, complete the sign in process. Or finally, the average load time of the video. Again, check each metric that would be a good invariant for this experiment.

## üìñ Lesson 25: 6   Choosing Invariants, Part 2 Solution   Lang En Vs3

Number of events would be a good population sizing invariant. Number of cookies and number of signed in users would be good as well, for the same reasons as before. Cookies are being explicitly randomized over. User IDs are typically larger than cookies, in the sense that one user ID can correspond to multiple cookies. So user IDs should be evenly split as well. And it's more likely that the events could end up unevenly split, but it's not something you're expecting. And it would be good to catch that if it does happen. The click-through rate on the Start Now button would not be a good invariant. By adding a sign-in button to the home page, the experiment could affect how many people click the Start Now button. Maybe fewer people would click the Start Now button if they instead signed in and then went directly to a course they had already started. The probability that a user enrolls in a course would not be a good invariant either. Users often enroll in courses after signing in, so changing sign in rates could affect enrollment. The sign-in rate would definitely not be a good invariant. This is the exact metric that Audacity is trying to change in this experiment. The video load time would be a good invariant. No backend changes were made in the experiment, and the user has no control over a load time. So, they can't affect it.

## üìñ Lesson 26: 7   Checking Invariants   Lang En Vs3

All right, now we'll go through an example of how to actually check an invariant and see whether it's reasonably close between the control and experiment groups. Let's say you run an experiment for two weeks, and your unit of diversion is a cookie. So, the first sanity check you want to do is to see whether the number of cookies in the control group is roughly the same as the number in the experiment group. So you get the number of cookies in each group in each day and the results look like this. This column has the number of cookies in the control group for week one. This has the number of cookies in the experiment group. And this table is the same numbers but for week two. Now the first thing I'll do is compute the total number of cookies for each group, and see if the overall division looks even. If so, great. If not, then I'll look at the day by day breakdown. It turns out that the total number of cookies in the control group is 64,454, and the total for the experiment group is 61,818. Now, there are more cookies in the control group than the experiment group. But, the question is, is this within what we expect? Ignoring the day-by-day data for a minute, how would you figure out whether the difference between the total number of cookies in the control and experiment groups is within what you expect? Remember, each cookie is randomly assigned to the control group with probability 0.5, and to the experiment group with probability 0.5. So, the question is whether it's surprising that out of the total number of cookies, 64,454 were assigned to the control group. Take a few minutes to think about how you would solve this problem and write the steps you would take in plain English. Then share how you would solve the problem on the forums. Again, take a minute to read the post left by the person before you and compare their steps to yours. When you're ready to move on, check this box.

## üìñ Lesson 27: 8   Checking Invariants Solution   Lang En Vs3

Thanks for sharing your thoughts. I'll go over how I solved the problem next, but there could also be multiple approaches.

## üìñ Lesson 28: 9   Checking Invariants, Part 2   Lang En Vs4

Here's how I would figure out whether this division of cookies is within what I expected. I have a random event with exactly two outcomes. A cookie gets assigned to either the control or the experiment group. I can think of this as like flipping a fair coin and if the coin comes up heads, I'll put the cookie in the control group. Then I want to know if it's surprising that the coin came up heads 64,454 times. Does that sound familiar? The number of heads will follow the binomial distribution, so I can construct a binomial confidence interval like in lesson one. The total sample size here is about 120,000, which is definitely enough to assume a normal distribution. So here are the steps I'll take. First, I'll compute the standard deviation of a binomial distribution with probability 0.5 of success, which I'll define as being assigned to the control group. Then I'll multiply the standard deviation by the z-score of my confidence level to get the margin of error, then I'll compute a confidence interval around 0.5. If the experiment is setup properly, it's very likely that thee observed fraction of successes or cookies in the control group will fall within this confidence interval. And finally, I'll compute thee observed fraction of successes or cookies in the control group and I'll check whether this value falls with in this confidence interval. This is a little different then thee approach in lesson one, where we computed the confidence interval around the observed click-through probability. In that case, we didn't know the true click-through probability. But here, I know that if the experiment is set up properly, the true probability is 0.5. So, I can compute that confidence interval. The results are the same though. Now, I'll go through these steps for this data. The standard deviation comes to 0.0014. I'll use a 95% confidence level as usual, so the z-score is 1.96. So the margin of error comes to 0.0027. That means that the confidence interval goes from 0.4973 to 0.5027. So 95% of the time, the observed fraction of cookies should fall within this range. In fact, the fraction of cookies in the control group, which I'll p hat is 0.5104. Now this is significantly greater than 0.5027, so something about this setup is incorrect. To get a better idea of what could be going wrong, it's a good idea to look at the day by day data again. One good thing to check is whether any particular day stands out as causing the problem or whether it seems to be an overall pattern. First, I'll look at which specific days had more cookies in the control group. It turns out that in 11 out of 14 days, there were more cookies in the control group than the experiment, which is quite high. I'll also compute the fraction of the cookies that were in the control group each day, there were a few days with 0.53 or higher. And no day obviously stands out as the highest. This points to an overall problem rather than a problem on a specific day. At this point, it's a good idea to talk to the engineers and figure out if something was wrong with the experiment setup. If that doesn't work, you could try slicing. For example, by country, language or platform to see if one particular slice looks like it's causing the problem or you could check the age of the cookies in each group. Does one group tend to have more new cookies while the other group has older cookies or something like that. Now suppose you run another experiment, this time for seven days with event based diversion. This table shows the number of pageviews in the control and experiment groups each day. The total number of events in the control and experiment groups are these numbers. Calculate a 95% confidence interval for the fraction of total pageviews in the control group and put your bounds in these boxes to four decimal places. The center of your confidence interval should be 0.5, then calculate the observed fraction of pageviews in the control group and write the answer in this box. Also, to four decimal places. Does this distribution between the control and experiment group pass your sanity check? Select yes or no.

# üóÇÔ∏è Section 2: B Testing Subtitles

## üìñ Lesson 1: 1   Introduction To Course   Lang En Vs3

Hi, and welcome to A/B testing. In this course, we'll discuss how to run experiments, or A/B tests for businesses who have an online presence via websites or mobile apps, for example. &gt;&gt; A/B tests allow you to determine scientifically how to optimize a website or a mobile app by trying out possible changes and seeing what performs better with your users. Using A/B tests means that you can get data to make decisions rather than relying on intuition or hippos ie, the highest paid person's opinion &gt;&gt; In this course, we'll be going over the process form start to finish. Everything from deciding what change you want to test in the first place, through drawing a conclusion. &gt;&gt; On the other hand, we won't focus on how to implement your using A/B testing framework &gt;&gt; This course is about how to design a task, choose metrics, and analyze the results. &gt;&gt; Sounds good. But, before we jump into that, why don't we take a minute to introduce ourselves? &gt;&gt; Sure. I'm Diane Tang. I have a PhD in computer science and I worked for over 12 years at Google. I worked on just about every aspect of experimentation. Review from the underlying infrastructure to designing metrics and dashboards to running and analyzing experiments to the processing culture, needed to facilitate data driven decisions. I'll be giving you the practical engineering perspective on how to design and analyze experiments for a bus decision making. &gt;&gt; My name is Carrie Grimes Bostock, I have a PhD in statistics, and I've worked at Google for almost 12 years, in all kinds of different areas. I'll be giving you the more statistical perspective on how you draw conclusions in A/B experiments. And I'm Caroline Buckey. I'm, of course, a developer at UDacity and before that, I was a software engineer in Silicon Valley. I'll be going over examples, diving into details and making sure you have everything you need to apply what you're learning on your own

## üìñ Lesson 2: 10   Overview Of Business Example   Lang En Vs3

Okay. Now that we've gone over the history of AB testing, let's start diving into our example. Throughout this course, we'll use an example of an online education company sort of like Udacity. Let's just call it Audacity. Audacity is focused specifically on finance courses, and they're trying to test features that increase student engagement. First, let's talk about what a typical user flow through the Audacity site might look like. You would probably see that the largest number of users visit the homepage. Then a subset of those users might explore the site by looking at a few different pages. An even smaller group might create an account. And the final group might reach some sort of completion. Maybe they make a purchase, complete a class, finish a series of classes, or share the site on their blog, for example. This type of flow is often called the customer funnel, and it's a common idea for websites. The idea is that you have the largest number of events at the top of your funnel, and as you go down, it becomes rarer and rarer that someone would reach that level. The idea is that users are trickling down the funnel, but that's kind of a simplistic idea. Customers don't actually enter, create an account, and then complete a class in a consistent manner. There's a lot of back and forth swirl between the different states, and repeat visitors who skip over intermediate steps. We'll be discussing the funnel and swirl more and how it ties to metric choice in lesson three. For the rest of lesson one, we're going to go over a simple experiment from start to finish, so that you can see all the steps that are necessary for running an experiment end to end. In future lessons, we'll be going through several of the steps in more detail. We'll consider an experimental change to Audacity's homepage. Specifically, we'll consider a change to the Start Now button. If users click this button, they can see a list of Audacity's courses. In this experiment, we'll make a simple change, making the Start Now button pink instead of orange. So a first pass hypothesis for our simple experiment is that changing the Start Now button from orange to pink will increase how many students explore Audacity's courses? That is, move on to the second step of the funnel.

## üìñ Lesson 3: 11   Metric Choice   Lang En Vs3

Now that we know the general change we want to make, we need to choose a metric to measure that change. For the current version of our hypothesis, we didn't really talk about how to measure whether changing the color scheme was an improvement. What Audacity ultimately cares about is how many people actually complete courses, so one possible metric is total number of courses completed. However, given that it can take students weeks or months to finish courses, using this metric would simply take too much time to be practical. An alternative is how many users actually click on the Start Now button. The assumption is that if more people click the button and thus move on to exploring the site, then eventually, some of them will create an account and go on to complete a course. In other words, increasing the rate at which users progress down the funnel at one level will have a positive impact on the end of the funnel as well. Can you think of any problems with counting clicks? What happens if more total users view the page in one version of the experiment? Suppose these dots are the total number of visitors in each group of the experiment, and the yellow dots are the ones who clicked. More total clicks occurred in this version, but there was a greater percentage of clicks in this version. So instead, we could use the fraction of page visitors who click. That is the number of clicks on the view course's button divided by the number of page views to the home page. This metric is commonly called click-through rate, or CTR. There's also a closely related metric which many people also refer to as click-through rate, but I'll call it click-through probability. Click-through probability is defined as the number of unique visitors who click at least once divided by the number of unique visitors who view the page. To see how these two metrics are different, suppose you have a webpage and two users visit the homepage. The first leaves without clicking the Start Now button, which means they clicked 0 times. And the second person clicks 5 times. Maybe the next page loaded slowly, so the user impatiently clicked 5 times. In this case, the click-through rate equals 2.5, since there were 5 total clicks and 2 total page views. But the click-through probability equals 0.5, since half the users who visited the page clicked. Rates and probabilities have different characteristics, which we'll talk about further in lesson three. For the purpose of this lesson, we're going to use click click-through probability as our metric, and not click-through rate. Given this, our updated hypothesis is that we will increase the click-through probability of the button. And we assume that that will ultimately increase the final business metric, which is total courses completed

## üìñ Lesson 4: 12   Estimating Click Through Probability   Lang En Vs3

For the Audacity example, we decided to use click-through-probability instead of click-through-rate. Why, and how do you decide in general? &gt;&gt; So generally speaking, you use a rate when you want to measure the usability of the site, and a probability when you want to measure the total impact, right? So if, for example, you want to measure the usability of a particular button, you use a rate because the users have a variety of different places on the page that they can actually choose to click on. And so the rate will say how often do they actually find that button. Now, if you just want to know how often users went to the second level page on your site, you use a probability, because you don't want to count if users just double-clicked, or did they reload, or all of those types of issues. &gt;&gt; Okay. And in our example, we're interested in whether users are progressing to the second level of the funnel, which is why we picked a probability. &gt;&gt; That's right. &gt;&gt; That makes sense. So, how will we actually compute the probability? &gt;&gt; So, to compute the probability, you're going to have to first just work with the engineers to modify your website. They're going to have to change the website so that on every page view you capture the event, and then whenever a user clicks you also capture that click event. Now, once you have the data captured, to compute our rate, you just sum the page views, you sum the clicks and you divide. Pretty simple. For a probability though, you're going to have to match each page view with all of the child clicks, so that you count, at most, one child click per page view.

## üìñ Lesson 5: 13   Repeating The Experiment   Lang En Vs3

So let's say we go to measure the click-through probability of the start now button. And we see that 1,000 unique users visit the Audacity homepage. And 100 of those visitors clicked the start now button at least once. From this data, the best estimate of the click-through probability would be 100 divided by 1000, or 10%. But, how sure would you be about this estimate? In other words, suppose you repeated the measurement. You had a different 1,000 users visit the site and again you recorded how many of them clicked the start now button. Which results would surprise you? Would you be surprised if you got 100 clicks again? 101? 110? 150? How about 900? Check all the answers that would surprise you. We'll talk later about how to quantify this, but for right now just check the ones that you subjectively would be surprised by.

## üìñ Lesson 6: 14   Repeating The Experiment Solution   Lang En Vs3

I would expect the new measurement to be relatively close to 100. I would be very surprised if the result were 900, and I'd actually also be pretty surprised about 150. 110 or 101 would both be in line with what I expected. Later, we'll talk about the standard error of your sample, which describes precisely how variable the estimate will be if you take different samples with the same size.

## üìñ Lesson 7: 15   Which Distribution?   Lang En Vs3

In the next several videos, we'll be going over the statistics behind whether to be surprised by 150 clicks. If you're already familiar with the binomial distribution, confidence intervals, and hypothesis testing, then you can follow the link in the instructor's notes to skip past this section. So, Carrie, how do you know whether to be surprised by 150 clicks? In other words, how do you know how variable your estimate is likely to be? &gt;&gt; Well, there are certain distributions that commonly arise in statistics that can help give you some good guidelines for how variable your data is likely to be. If you haven't worked with this before, something like the bell curve that comes up in grades is a good example of the normal distribution and we'll talk about that in a minute. But for our data here, we're going to be using a slightly different distribution than binomial, because instead of having continuous data, we have successes and failures. Now, for the binomial, we're in good shape for this example because we can call a click a success and a no click visit to the page a failure. It doesn't really matter though. You could also use this distribution if your data were red and blue, or any two exclusive outcomes. What matters is, we have exactly two possible outcomes, click and no click, and that our metric we're going to compute with this is a probability.

## üìñ Lesson 8: 16   Binomial Distribution   Lang En Vs3

Lets say we have a biased coin and it has a probability three-quarters of coming up heads, which I'll call p. This coin could actually be a visitor to a webpage, who has a three-quarters probability of clicking a button, but I'm going to do this example with coins for simplicity. We we'll define success to be heads and failure to be tails. This doesn't necessarily mean we're upset if we get a tails, it's just the way of specifying what the two outcomes are. Now, let's say we flip the coin ten times. How many times will it come up heads? Of course, it could come up heads zero times, ten times, or anything in between, but some outcomes will be more likely than others. If we mark the probability of each number of heads, we'll get something that looks like this. As you can see, the most likely results are seven or eight heads, but anything between six and nine is reasonably likely. Like Carrie mentioned, this distribution is called a binomial distribution, and we can draw it for any number of flips. To make it easier to do this, I'll relabel this axis as the percentage of flips that came up heads, or the fraction. As you increase n, the binomial distribution starts to look more and more like a normal distribution. Here, you see the binomial distribution with n equals 30, and the normal distribution overlaid in blue. The binomial and the normal distribution that approximate it will have the same mean, which will be equal to p, the probability that the coin comes up heads. And the standard deviation will be the square root of p times 1 minus p over N, where N is the total number of flips. Now in this case, we assumed that we knew the probability that the coin would come up heads. Let's instead consider a case where we don't know that. We flip the coin 20 times, so N equals 20, and it comes up heads 16 times, which I'll call X. Now, let's say we want to estimate the probability the coin will come up heads. That is, we want to guess the bias of the coin. Of course, this could be a fair coin that happens to come up heads more than tails, but the results we saw will be more likely if the coin is biased. To estimate the bias, which I'll call p hat, it's an estimate of the probability. We'll take the number of heads, in this case 16, and divide by the number of flips, in this case, 20. And we get four-fifths. So, our best estimate is that this coin has a probability four-fifths of coming up heads. If you'd like to see more details on what the binomial distribution looks like for different parameters, the page linked in the instructor's notes may be helpful. In order to use the binomial distribution, a few assumptions have to hold. First, there have to be exactly two types of outcomes. We've been calling these success and failure. And second, the events have to be independent. Learning the outcome of one coin flip doesn't tell you anything about whether the next coin came up heads. And finally, the events need to follow an identical distribution. That is, the probability of success needs to be the same for all of them. In practice, identifying cases where your events are independent can be kind of tricky. So let's go over some examples. First, suppose you shuffle a deck of 52 cards and you draw and hold the first 20 cards. Each card is an event and the outcome is whether the card is red or black. Next, suppose you roll a die 50 times. Each roll is an event and you choose to label six as a success and any other result as a failure. Next, suppose a search engine, such as Google, is measuring how often people click on search results after making a search. Each event is one search made on the search engine and the outcome is whether the person clicked any link in the results. Next, supposed an online education company, like Udacity, is measuring how many of the students who enroll in a course complete it within two months. Each event is one student and the outcome is whether the student has completed the course two months later. Finally, suppose an online shopping company, such as Amazon, is measuring how often items that are added to a shopping cart are purchased within a week. Each event is a single item and the outcome is whether the item has been purchased a week later. For each of these situations, check the box if you think it is valid to assume a binomial distribution, and specifically, think about whether the events will be independent. Some of them are a bit subjective, but take your best guess.

## üìñ Lesson 9: 17   Binomial Distribution Solution   Lang En Vs2

The shuffled cards are not independent. Since each card appears in the deck exactly once, as you draw each card, it gives you information about the next card. If the first card is black, the second is slightly less likely to be black, and so on. If your deck were infinitely large, that wouldn't be a problem, but drawing 20 out of 52 cards is a pretty big fraction. The die rolls are independent. The probability of success is one-sixth rather than one-half, but the probability doesn't have to be a half for the binomial distribution to apply. The events just need to be independent. And in this case, each die roll doesn't affect the outcome of the other die rolls. The search engine case is not independent. Often, if someone doesn't find what they want the first time they search, they'll immediately search again with slightly different words. Those are two separate events, but they're not independent. You would expect the results to be related in some way, but you can't necessarily tell that they came from the same user. The education case is probably not exactly independent, but it's a lot closer. Each student corresponds to only one event, not multiple events, and Udacity students are spread out all over the world. You wouldn't expect two different students to be terribly related. Of course, there are exceptions. Maybe some students create two accounts, in which case, we wouldn't know that it's two different students. Or maybe some take courses with their friends, in which case, whether they complete would be correlated, but in practice, it would probably be reasonable to assume a binomial distribution here. The shopping cart case is also not independent. For example, one person could add several items to their cart, then purchase all of them. The outcome for these different events was highly related.

## üìñ Lesson 10: 18   Confidence Intervals   Lang En Vs3

Okay, we expect clickthrough probability to follow a binomial distribution, but how do we actually use this to decide which values should surprise us? &gt;&gt; Well, the benefit of knowing that it should follow binomial distribution is that we can use the formula we have for sample standard error for the binomial to estimate how variable we expect our overall probability of a click to be. What that means is that for a 95% confidence interval, which is commonly what you hear people talk about, if we theoretically repeated the experiment over and over again, we would expect the interval we construct around our sample mean to cover the true value in the population 95% of the time.

## üìñ Lesson 11: 19   Calculating A Confidence Interval   Lang En Vs3

Now, let's look at how to compute the confidence interval for our sample. We've already found the center of the interval, that is a probability of a click. The equation was p hat, or the estimated probability equals X over N, where X was the number of users who clicked and N was the total number of users who visited the page. In this example, p hat is 100 over 1000, or 0.1. So, the center of the confidence interval will be 0.1. Next, I want to calculate the width of the confidence interval, which is also called the margin of error, or m. To do this, I'll need to use the standard error of the binomial distribution. Recall that if the sample is large enough, instead of using the binomial distribution, I can assume the distribution is normal, like this blue line. A good rule of thumb is that if N times p hat is greater than 5, it's safe to assume a normal distribution. You should also check that N times 1 minus p hat, is greater than 5. But, for small click-through probabilities like we usually see, this is the more stringent condition. In our case, N times p hat is 100, so the assumption should be safe here. When we can use the normal approximation, then the width of the confidence interval, that is the margin of error, will be equal to the z-score of the confidence level, times the standard error. And for the binomial distribution, the standard error's the square root of p hat times 1 minus p hat over N. If you look at this formula, you'll notice a few things. The amount of random variation we expect in our sample, the width of the confidence interval, is a function of both the proportion of successes and the size of the sample. This means we need to consider the proportion of successes when we decide how many samples to collect, which we'll talk about again later this lesson. When the success probability is farther from 0.5 then the standard error will be smaller, which means the distribution is tighter, which means the confidence interval will be smaller. Similarly, if the number of samples is larger, the standard error and the confidence interval will also be smaller. Now I'll need to find the z-score for the boundary of a 95% confidence interval. For a normal distribution with a mean of 0 and a standard deviation of 1, which is also called a z distribution. With 95% confidence, the true value would be within 1.96 and negative 1.96 of the estimate we observed. The value 1.96 is called a z-score, and you can look it up in a table like the one linked in the instructor's notes. In this case, since we're doing a two-tailed test, each tail will contain 2.5% of the distribution. So 1.96 is the z-score for 97.5%, or 100 minus 2.5. In our case, the margin of error comes out to about 0.019. So, we'll add this margin of error to the point estimate to get the upper bound of 0.119, and subtract to get the lower bound of 0.081. This means, if you'd run the experiment again with another thousand page views, you'd maybe expect to see between 80 and 120 clicks, but more or less than that would be pretty surprising. Now, let's say you made a similar measurement on a different page, and this time you got 2000 total users and 300 of them clicked. And let's say, you wanted a 99% confidence level. Calculate the upper and lower bounds of the confidence interval, and write the numbers in these boxes to three decimal places. To do this, you'll need a z-score table, and you can find a link to one in the instructor's notes. Again, you should use a two-tailed test.

## üìñ Lesson 12: 2   Course Format   Lang En Vs3

This course will have five lessons. In this first lesson, we'll be giving you an overview of what A/B testing is and what it can and can't be used for. We'll also go over an example A/B test from start to finish so that you can see what the whole process looks like. In that example, we'll first choose a metric for an experiment, then review some necessary statistics, then we'll design our experiment, and finally, we'll analyze the results. Then each subsequent lesson of the course will dive into more detail on a particular part of the process. In lesson two, you'll will be learning about how to protect the participants of your experiment, and what questions you should be asking yourself regarding the ethicality of experiments. In lesson three, we'll go into more detail on choosing and characterizing metrics to use when evaluating your experiment. In lesson four, we'll talk more about designing your experiment, which includes choosing which users go in your control group and your experiment group. And, in lesson five, we'll go over lots of examples of what your results could look like, how to analyze those results, and how to draw valid conclusions. Now, you won't need any prior experience with A/B testing in order to take this course, because we'll be covering it from the ground up. And you won't need any programming experience either, because this course won't cover how to implement an A/B testing framework. It's about how to design and analyze experiments. Introductory knowledge of statistics, on the other hand, will be required. So if you need to brush up, check out the links in the instructors notes. Now throughout the course, I'll be going over lots of examples and giving you opportunities to practice what you're learning. I'll also be having conversations with Dianne and Carrie about their experiences in A/B testing. Let's do that now.

## üìñ Lesson 13: 20   Calculating A Confidence Interval Solution   Lang En Vs3

To calculate the confidence interval, first I'll estimate the probability of a success, which will be the point estimate. That will be 300 divided by 2000, which comes to 0.15. So, 0.15 will be at the center of the confidence interval. Next, I'll calculate the margin of error. To do that, I'll need the z-score, and it turns out that the value is between 2.57 and 2.58. You could really use either and it wouldn't matter much, but I'll use 2.58. And then that will need to be multiplied by the standard error, which is the square root of p hat, times 1 minus p hat over N. And that comes out to about 0.021. Now, to get the upper bound, we add the margin of error to the point estimate, which gives 0.171, and to get the lower bound we subtract, which gives 0.129.

## üìñ Lesson 14: 21   Establishing Statistical Significance   Lang En Vs3

Okay, now we've estimated the click-through probability of the Start Now button, but we haven't actually changed the button yet. Once we do that, and we run the experiment, how will we analyze the results? &gt;&gt; Well, there's a concept in statistics called hypothesis testing, or inference, which is a quantitative way to establish how likely it is that your results occurred by chance. So, the first thing we need is what's called a null hypothesis, or a baseline. In our case, that's the theory that there's no difference in click-through probability between our control, and our experiment. Then we need to think about what's called the alternative hypothesis. Are we interested in whether the click-through rate is different? Or just whether it's higher, or lower? Or are we interested in any kind of difference at all?

## üìñ Lesson 15: 22   Null And Alternative Hypothesis   Lang En Vs3

To establish that our results are statistically significant using hypothesis testing, you need to calculate how likely it is that your results are due to chance. In order to calculate this probability, you need to have a hypothesis about what the results would be if your experiment had no effect. In the Audacity example, we'll collect two samples. A control group with the orange button, and an experimental group with the pink button. We've already assumed that each group follows a binomial distribution, but the probabilities might be different for the two groups. I'll call the probability that someone in the control group clicks Pcont for control, and the probably that someone in the experiment group clicks Pexp for experiment. This potentially gives two different distributions. If changing the button color had no effect, then we would expect the two groups to have equivalent distributions, so they would right on top of each other. In other words, p control and p experiment would be equal. Another way of saying this would be that p experiment minus p control equals 0. This hypothesis of what our results would look like if the experiment had no effect is called the null hypothesis. And it is also represented as H naught. We also need a hypothesis about the results if the experiment does have an effect. This is called the alternative hypothesis or HA. In this case, if changing the button color does have an effect, we expect the P experiment minus P control is not equal to zero. Once we've defined these hypotheses, we can estimate p control and p experiment from the data we've collected. Then we'll calculate the difference between these, and compute the probability that this difference would have arisen by chance, if the null hypothesis were true. Then we want to reject the null hypothesis, and conclude that our experiment has an effect if this probability is small enough. This is the same type of significance threshold as a confidence interval, so it make sense to choose the same cutoff. Thus, we'll choose the same cutoff as before. And reject the null if this probability is less than 0.05. This cutoff is also called alpha. Now, suppose you're running an experiment where you have changed the checkout flow of an online shopping website. You test the effectiveness of your change by running an experiment where some users check out using the old flow, and some using the new flow. For each group, you measure the proportion, or the probability who successfully complete the checkout. What null and alternative hypothesis would you choose for this experiment? There could be multiple valid answers, but choose a formulation that you think would work. Share your response in plain English in the forum thread linked in the instructor's notes. Then read the response of the person above you in the thread, compare their answer to yours, and leave a comment with your thoughts. When you've done this, check this box.

## üìñ Lesson 16: 23   Null And Alternative Hypothesis Solution   Lang En Vs3

I would choose the null hypothesis that both groups have the same probability of completing a checkout. And I would choose an alternative hypothesis that the two groups have a different probability of completing a checkout. We could also say as we did before, that the null hypothesis is that px minus p comp is equal to zero. And then the alternative hypothesis would be that the difference is not equal to zero.

## üìñ Lesson 17: 24   Comparing Two Samples   Lang En Vs3

Now that we've set up a hypothesis test, how will we decide whether to reject the null? &gt;&gt; Well, we're going to need to look at our conference intervals in a slightly different way. So remember in this case, we actually have two samples. We have the control side, which may have a different number of users from the experiment side. So we'll need to compare the proportion of clicks estimated on the control side, with the proportion estimated on the experiment side. Then the quantitative task tells us whether it's likely that the results we got, the difference we observed, could have occurred by chance, or if it would be extremely unlikely to have occurred if the two sides were actually the same.

## üìñ Lesson 18: 25   Pooled Standard Error   Lang En Vs3

Because we have two samples, we'll need to choose a standard error that gives us a good comparison of both. The simplest thing we can do is calculate what is a called a pooled standard error. Recall that we'll measure the users who click in each group, which we'll call x control and x experiment, as well as the total number of users in each group, which we'll call n control and n experiment. Now, the first thing we'll calculate will be what's called the pooled probability of a click. And I'm using a hat here because this is an estimated probability. And the pooled probability is the total probability of a click across groups, that is, the total number of users who clicked divided by the total number of users. Then we'll calculate the pooled standard error, which is given by this formula. Now recall that we're going to estimate the difference between p experiment and p control, and I'll call this difference d hat for difference. Under the null hypothesis d, the true difference is equal to zero. Then we would expect our estimation, d hat, to be distributed normally, with a mean of zero and a standard deviation of the pooled standard error. If the estimated d is greater than 1.96, our z score, times the pooled standard error, or less than the negative of this cutoff, then we can reject the null hypothesis as unlikely and say that our difference represents a statistically significant difference.

## üìñ Lesson 19: 26   Practical, Or Substantive, Significance   Lang En Vs3

&gt;&gt; Okay. We can use hypothesis testing to see whether a difference we observe in our experiment is actually significant. What comes next? &gt;&gt; So what we have to do next is decide from a business perspective, what change in the click-through probability ability is practically significant. In other words, what size change matters to us? &gt;&gt; Well, as a statistician would say, what's substantive in addition to being statistically significant. &gt;&gt; That's the difference between what engineers say and what statisticians say. Substantive and practically significant mean the exact same thing. &gt;&gt; Okay. So why can't we say that every change matters to the business? Can't we launch any experiment that makes any difference at all? &gt;&gt; Well, you could, but you might not want to for a variety of reasons. One, there might be an investment in making the change itself, so it might not be worth it to you or you might just want to wait for the next change, thinking that maybe you could do well here, but you could do better. So, in something like medicine, you always want to be clear that it's worth the change. If you're testing a new drug, maybe patient's respond a little bit better, but you're going to have to invest in training nurses, actually launching the drug. All these other things. So you may want to have a higher level of practical significance to justify changing the treatment protocol. &gt;&gt; Now this is one major difference between the online world and sort of the traditional sciences, right? In medicine, you probably want a 5%, a 10%, a 15% change to really be substantive. Right? At Google, for example, a 1 or a 2% change in click-through probabilities is actually quite large. And so the differences in the magnitudes for what's consider practically significant can be quite different. &gt;&gt; Yeah, so what you really want to observe is repeatability. Statistical significance is about repeatability. &gt;&gt; And you want to make sure when you setup your experiment that you get that guarantee that yes, these results are repeatable so it's statistically significant. But you also want to make sure that if you see a change in your experiment that you're interested in from a business standpoint, so it's practically significant, it's also statistically significant. So you want to size your experiment appropriately, such that the statistical significance bar is actually lower than the practical significance bar. &gt;&gt; That makes sense. So then, so the audacity example, we still need to pick a practical significance boundary. &gt;&gt; That's right. So let's say that from a business perspective, a 2% change in the click-through probability would be practically significant.

## üìñ Lesson 20: 27   Size Vs. Power Trade Off   Lang En Vs3

Okay, we know how to calculate pooled standard error and confidence intervals. Are we ready to run the experiment? &gt;&gt; Not quite. So, we first have to design our experiment. Now, the main question that we have to decide is given that we have control over how many page views go into our control and our experiment, we have to decide how many page views we need in order to get a statistically significant result. This is called statistical power. Now, the key thing is that if we see something interesting, we want to make sure that we have enough power to conclude with high probability that the interesting result is, in fact, statistically significant. Now, the key thing to keep in mind is that power has an inverse trade-off with size. The smaller the change that you want to detect, or the increased confidence that you basically want to have in the result, means that you have to run a larger experiment, so more page views in your control and your experiment.

## üìñ Lesson 21: 28   How Page Views Affect Sensitivity   Lang En Vs3

Like Diane said, before you run your experiment you'll need to decide how many page views you plan to collect before you make your conclusion. First let's look at how the distribution changes when you increase the sample size. This is what the distribution of results would look like if you collected a thousand samples and there was no true difference between the two groups. That's why the mean is zero. You'll reject the null and conclude that there was a difference if you measure a value less than this one roughly, or greater than this one. So you're probability of falsely concluding there was a difference, which is often called alpha, is 0.05. If you increase your number of samples, say to 5,000, then the standard error will decrease, so the distribution of results will look much narrower. To keep alpha the same, that means the cut-offs for rejecting the null will be closer to zero. Now let's say you decided to only collect 1,000 page views so that you could make a conclusion more quickly. Recall that this is the distribution of results if there is no true difference, and as we saw, alpha will be 0.05. Now let's consider the case where there is a difference. Specifically, the difference is equal to the practical significance level of 0.02. You'll fail to reject the null, and conclude there was not a significant difference in the same cases. Now, the probability of failing to reject the null when the null was in fact false, which is often called Beta, is pretty high. Even if there is a true difference, it's pretty likely that you'll be inside this range. So by collecting a small sample, alpha is low, that is, you're unlikely to launch a bad experiment. But beta is high, that is, you're likely to fail to launch an experiment that actually did have a difference you care about. Now beta, or the probability of falsely failing to draw a conclusion, depends on how big your effect really was. For well behaved distributions like the normal distribution, as your true change gets larger and larger, then beta will go down. So you typically consider beta at your practical significance boundary since you don't care about any smaller changes, and any larger changes will have a lower beta, that is, a lower chance of error. People also refer to one minus beta as the sensitivity of the experiment. In general, you want your experiment to have a high level of sensitivity at the practical significance boundary. People often choose 80%. Now again, let's look at what happens if you collect 5,000 samples. Both distributions get tighter. And as before, alpha doesn't change. However, in the case where there is a true difference, you're much less likely to fall within the range of failing to reject the null. That is, you're more likely to reject the null and conclude there was a difference. So beta has gone down and your power has increased.

## üìñ Lesson 22: 29   Calculating Number Of Pages Views Needed   Lang En Vs3

Sensitivity can be a bit tricky to calculate, so people have a few different techniques they might use to determine how many page views they'll need. First, some programming languages have built-in libraries that can calculate this. You might also be able to find a table where you can look up the answer, or you might be able to use an online calculator. For the audacity experiment, we'd like you to use the online calculator linked in the instructors notes. Now on this calculator, there are a few different pieces of information you'll have to enter. The baseline conversion rate in our case means the estimated click through probability before making the change, and the minimum detectable effect is what we've been calling the practical significance level. You'll also need to specify whether your practical significance total is an absolute or a relative change. We've been using an absolute change, so make sure you check this option. We'll talk more about the difference in Lesson three. Finally, you'll need to enter 1 minus beta and alpha. Note that this calculator refers to 1 minus beta as statistical power, rather than sensitivity. Write the number of page views we'll need in each group of the experiment in this box. Recall that before making the change, we collected 1000 unique page views, of which 100 resulted in a click, We decided on a practical significance level of 2%, or 0.02, which I am calling d min, because it's the minimum difference we care about. And you should use alpha equals 0.05, and beta equals 0.2, which is fairly standard.

## üìñ Lesson 23: B Testing   Lang En Vs3

So, Diane and Carrie, to start out, what exactly is A/B testing? &gt;&gt; So, A/B testing is a general methodology used online when you want to test out a new product or a feature. And what you're doing is you're going to take two sets of users and you're going to show one set, a control set, your existing product or a feature, and then another set, your experiment, the new version. And what you're going to do is, you're going to say, how did these users respond differently, in order to determine which version of your feature is better? &gt;&gt; So, should you use A/B testing for every change you're considering, or are there certain types of changes it works better for? &gt;&gt; So I was talking to John Lilly. He's a VC at Greylock now, but he used to be the CEO of Mozilla, and he came up with a great analogy. And his analogy is that A/B testing is really useful for helping you climb to the peak of your current mountain. But if you want figure out whether you want to be on this mountain or another mountain, A/B testing isn't so useful. &gt;&gt; Now that said, you can test a pretty wide variety of things with A/B testing, everything from some new features, additions to your UI, different look for your website, and a lot of companies use it. So for example, when Amazon first started doing personalized recommendations, they wanted to see whether people really bought more stuff. And they discovered that actually had a signfiicant increase in revenue from the personalized recommendations. So, you can use it for fairly complicated changes. Now, Google maybe took it a little too far sometimes, so once we ran 41 different shades of blue in the UI to see how users responded, if they reacted. And on the one hand it was interesting and useful internally, but it might have been going a little too far with those experiments. &gt;&gt; Maybe, but we can also test less user visible changes. So, for example, ranking changes. LinkedIn tested a change where they were trying to figure out whether they should show a news article or an encouragement to basically add new contacts. That's a ranking change. Google does a lot of ranking, between our search lists and our ads, we test all of those changes also via A/B testing. &gt;&gt; You can also test changes that you're not even sure a user would notice. So for example, 100 milliseconds is not a lot of page load time, but, both Amazon and Google have run experiments. Amazon showed in 2007 that, for every 100 milliseconds they added to the page, they actually had a 1% decrease in revenue. And for Google, we have similar results, that you find, even though 100 milliseconds doesn't seem like that much, on average, the number of queries people do actually decreases for every 100 milliseconds of latency that you add. &gt;&gt; Wow, so those are some things you can learn from A/B testing. What are some things you can't do with A/B testing? &gt;&gt; A/B testing isn't as useful testing out new experiences. And you can sort of think about this in the obvious way, right? You know, you're testing a new experience and you've got an existing set of users and they are like, hey, you just changed my experience, what happened, I liked my old way and I'm kind of like grrr, right? That's called change aversion. The other thing is that they can be like, oh, well, this is new, and they test out everything. That's called a novelty effect, right? And so what happens in a new experience is there are sort of like two issues. One is a question of, what is your baseline for comparison? But two is a question about how much time you need In order to actually have your users adapt to the new experience, so that you can actually say what is going to be the plateaued experience so that I can actually make a robust decision. &gt;&gt; Yeah, and time can be a problem for other reasons. So, for example, if you have a website that recommends apartment rentals. People don't look for apartments that often, and what you really want is return business, or maybe you want to grow your business by referrals to other people who like your service. And the reality is, in the scope of an experiment it's going to be really hard to measure whether people actually come back to you from more referrals. And even if they refer their friends, well, honestly is it going to be next week? Is it going to be six months from now? So, there's certain things that you may want to know about your site, that are pretty difficult to get at, through short term A/B testing, and we'll talk in lesson three about some options for coming up with some metrics that are good proxies for that kind of thing. &gt;&gt; So the last example that we'll give for when A/B testing isn't as useful, is that A/B testing can't really tell you if you're missing something. So, let's say you're on a digital camera review site. A/B testing can tell you whether or not you should be showing this camera review above this camera review, but A/B testing can't really tell you if you're missing this entire other camera that you should be reviewing but you aren't reviewing at all. &gt;&gt; Got it.

## üìñ Lesson 24: 30   Calculating Number Of Pages Views Needed Solution   Lang En Vs3

Our baseline conversion rate is 10%. That is 100 divided by 1,000. And the minimum detectable effect, or practical significance boundary, we decided was 2%, and that's an absolute change. Finally, I'll make sure that 1 minus theta is 80 Percent and alpha is 5%. This gives the answer that we'll need at least 3623 page views per branch or group of the experiment. So the correct answer is 3623.

## üìñ Lesson 25: 31   How Does Number Of Page Views Vary?   Lang En Vs3

Now that you know how to calculate the number of page views you'll need for an experiment, I want you to consider the following possible changes to your experiment. And decide whether each will increase or decrease the number of page views you need. First, suppose the click-through-probability in your control group had been higher but still less than 50%. Would you need more page views or fewer to get the same sensitivity? Second, suppose you decided to increase your practical significance level, which we've been calling dmin. That is, you would no longer care about a 2% change but you would, for example, want to see a 3% change in order to launch the experiment. Third, suppose we decided to increase your confidence level which is also called 1 minus alpha. For example, maybe instead of 95% confidence, you would choose a 99% confidence level. Finally, suppose you wanted to get a higher sensitivity which is also called 1 minus beta at the practical significance boundary. For each case, mark whether you think you would need to increase or decrease the number of page views to keep everything else the same. I encourage you to think about what you would expect to happen, then try changing the numbers in the online calculator to see what actually happens. If you can't figure out why a change has the effect it does, watch the solution video for an explanation.

## üìñ Lesson 26: 32   How Does Number Of Page Views Vary? Solution   Lang En Vs3

To see how the click-through-probability would affect the number of page views needed, recall that the standard error depends on the click-through-probability. Specifically, the standard error is proportional to the square root of p times 1 minus p. So, for example, if the probability were 0.5, the standard error would be proportional to the square root of 0.5 times 0.5, which comes out to 0.5. On the other hand, if the probability were either 0.1 or 0.9, then the standard error would be proportional to 0.3. It turns out that as your probability gets closer to 0.5 and further away from extremes like 0.1 or 0.9, then your standard error increases. Thus increasing the click-through-probability but not above 0.5 would increase the standard error. Which means that I'd also need to increase the number of page views in order to reduce the standard error back to its original level. So now on the online calculator, I'll try increasing the baseline conversion rate to 20% and sure enough the sample size went up to about 6,000. Now if you increase your practical significance level, you're saying you no longer care about detecting a 2% change. You would need the change to be larger than 2% before you cared about detecting it. Larger changes are easier to detect, so you shouldn't need as many page views. So here I'll try increasing the practical significance level to 5%, and the sample size needed goes down to about 1,000 page views per branch. If you increase your confidence level, you're saying that you want to be more certain that a change has occurred before you reject the null. In essence, you're being more conservative. You could accomplish that by rejecting the null less often, but then your sensitivity would go down. If you want to keep sensitivity the same, you'll need to increase the number of page views you collect. So I'll try this out and increase the confidence level by decreasing alpha. And, the number of page views needed has gone up, now it's about 1,500 instead of 1,000. Finally, as you've seen before, if you want to increase the sensitivity of your experiment, you'll need to collect more page views to narrow the distribution. So I'll try increasing sensitivity here, and the number of page views needed goes up. Now it's about 2,000 per group.

## üìñ Lesson 27: 33   Calculating Results   Lang En Vs3

Okay, so we've designed and sized the experiment. Now, let's assume that we've run it and analyzed the results of the experiment. Suppose the control group had 10,072 total page views, and the experimental group had 9,886. These numbers aren't exactly the same because of how people were randomly assigned to groups. In the control group 974 of those users clicked the Start Now button, and in the experimental group, 1242 did. So, it certainly looks like more people clicked in the experimental group, but was this due to random variation? To answer that, we'll need to calculate the confidence interval for the difference. I'll start by calculating the pooled probability, that is the total number of clicks in both groups, divided by the total number of users, and that comes to about 0.111. Next, I'll calculate the pool to standard error using the formula you saw before, and this comes to about 0.00445. Now, go ahead and finish the calculation, by calculating the estimated difference, or d hat, and the margin of error, or m. Write your answers in these boxes to four decimal places. Then, put the upper and lower bounds of the confidence interval in these boxes, also to four decimal places. Finally, would you launch the new version of the Start Now button based on these results? Select yes or no. Don't forget to consider whether the change is greater than the practical significance level of 2%, and use the 95% confidence level.

## üìñ Lesson 28: 34   Calculating Results Solution   Lang En Vs3

We defined the estimated difference as the experimental probability minus the control probability. And that comes to 0.0289. The margin of error is equal to the pulled standard error times 1.96, which is our Z score for our confidence level of 95%. And that comes to 0.0087. The lower bound of the confidence interval will be d hat minus m, which comes to 0.0202. And the upper bound will d hat plus m, which comes to 0.0376. So what can we conclude from these results? Well, we can conclude it is highly probable that click-through probability changed by at least 2%. Remember that we want to look for both statistical and practical significance, and here we have both. Because 2% was our practical significance level, and we can be confident that we have at least that big of a change at the 95% level. Based on this, I would definitely launch the new version.

## üìñ Lesson 29: 35   Confidence Interval Case Breakdown   Lang En Vs3

Now let's look at some different cases that could have come up in our results. Here I've drawn our practical significance boundary as these two blue lines, and zero as this black line. Our point estimate, that is d hat, for the example we just went over was greater than the practical significance boundary. And in fact, both ends of the confidence interval were greater than the practical significance boundary. Based on this, it's highly probable that the click-through probability did, in fact, change by more than the practical significance level. Now let's go through some other possible cases of what your results could look like, and practice recommending a decision in each case. Choose whether you would want to launch the change, not launch the change, or try to gather more data, ideally by running an additional test, in each of these cases. The first case is the one you just saw, and as I said, I would launch this change. You should assume that a positive change in click-through probability is desirable, and a negative change is undesirable. Remember that the goal of running A/B tests is to gather data to make a decision. And the reason we did the work of computing the power in the confidence intervals is so that we would understand how likely it is that our results are repeatable and robust. And make the right decision. Given this, choose the decision you think is best for each case.

## üìñ Lesson 30: 36   Confidence Interval Case Breakdown Solution   Lang En Vs3

This second result is often called neutral. There's no statistically significant change from 0 since the confidence interval includes 0 and you're also confident that there's not a practically significant change. Given this it's not worth the effort to launch the change. In the third case, your result is statistically significant. Your confident, that is, that there was a positive change, but it's not practically significant. In fact, you're confident that there was not a practically significant change. In other words, you're confident there was a change, but you don't care about the magnitude of the change. Don't let your team celebrate when they get a result like this. Again, it's not worth the effort to launch this change. If it had been, you would've set your practical significance boundary lower. This fourth case is one of the cases that can be the most difficult to handle as an analyst. Some people call this a neutral change just like change two, but here, the confidence interval bounds are outside of what's practically significant. If you ran an experiment and found that it could be causing your number of users to increase by 10% or it could be causing them to decrease by 10%, would you say that that change is neutral? Instead, it would be better to say that you do not have enough power to draw a strong conclusion. Running an additional test with greater power would be recommended in this situation, if possible. Here's another tricky case. The point estimate is beyond what's practically significant. Meaning your best guess, is that this change is an effect you care about. But the confidence interval overlaps 0, so there might not even be a change at all. Do you launch assuming that it is positive, even though it could potentially have no effect. Repeating this test with greater power would give additional confidence to the results. And finally, this is another common tricky example. Again, your best guess is that there is a practically significant positive change. However, it's also possible your change is not practically significant. How do you make the call? For all of these last three cases, you should run an additional test with greater power if you have the time. But sometimes you have to make a decision even though there's uncertainty about how real your result is or even about the direction of the result. Let's get some perspective from Diane about these tricky cases.

## üìñ Lesson 31: 37   Making Decisions About Uncertain Data   Lang En Vs3

Diane, what should you do if one of the last three cases comes up, but you don't have time to run a new experiment? &gt;&gt; Wouldn't life be easy if we only had the first three cases? The problem is that despite our best intentions and efforts to power our experiments properly, you're going to get situations where the data's uncertain. What you have to do is to communicate to the decision-makers when they're going to have to make a judgement, and take a risk, because the data is uncertain. They're going to have to use other factors, like strategic business issues, or other factors besides the data.

## üìñ Lesson 32: 38   Conclusion   Lang En Vs3

Congratulations on completing the lesson. You learned about the history of A/B testing beyond the use in technology, and when A/B testing is and is not useful. We then introduced the business example we'll be using throughout the course and used a simple scenario to set up an A/B test end to end. In this proposed experiment, you learned how to state and evaluate a testable hypothesis. This involves choosing a metric for evaluating the change and establishing what change in that metric would be practically significant. We discussed the difference between practical and statistical significance and how to compute a binomial conflict interval. You put all of that together to be able to compute the statistical power of an experiment and understand the tradeoffs between power and size. Finally, you apply those concepts to analyzing the results of an experiment and understanding what you can and cannot conclude. In the next lesson, we'll go over what you need to keep in mind to protect participants in your experiments. See you then.

## üìñ Lesson 33: B Tests   Lang En Vs3

For the next few videos, we're going to go over some more examples of when you can and can't use A/B tests. As well as some other techniques you can use instead of A/B testing, and a brief history of A/B testing. After that, I'll introduce the example A/B test that we'll be using for the rest of this lesson. So if you'd rather skip to this point, you can do so by following the link in the Instructor's Notes. Right now, I'm going to go over a list of possible changes you might consider using A/B testing for, and ask you to decide when A/B testing would be appropriate. First, suppose an online shopping company, like Amazon, wants to know whether their site is complete, or whether there are products users want to buy, that they don't have. Check this box if you think they could answer this question using an A/B test. Second, suppose the company already has a free service. Maybe they have a to-do list app and they want to provide a premium service that offers additional functionality. To access the premium service, users would need to upgrade, create a log in, and explore the new functionality. Check this box if you think the company should test out this change, using A/B testing. Third, consider a site that provides a list of Movie recommendations for users, such as Netflix. They create a new algorithm to rank the possible recommendations. Could this be tested using AB testing? Finally, suppose you wanted to change your backend infrastructure, which could impact how quickly the page loads, what results users see, and other things. Would an A/B test be appropriate to test this change?

## üìñ Lesson 34: B Tests Solution   Lang En Vs3

In the first case, you can't figure out whether you're carrying all the products users need by running an A/B test. You could try carrying a specific additional product, but if users don't buy it, you still won't know if there's a different product you're missing. Instead, you might want to try asking users whether there's anything they feel is missing. In the second case, you wouldn't be able to fully test whether the premium service is a good business decision using A/B testing. Users have to opt into a premium service, so randomly assigning people to one group or another wouldn't really work. You wouldn't have a control to compare against. You can still get valuable information from an A/B test. For example, you could see how many users will read about the additional features or how many will choose to upgrade if you make the choice available. But you can't fully test out the change. The third case is a great place to use A/B testing. There are clear control and experiment groups, the old algorithm and the new algorithm. There are also probably some clear metrics you could use to evaluate the change, such as the probability of a user clicking on a recommendation or actually watching the movie. The fourth case may be tricky to test, depending on whether you have the computing ability to run both versions of the backend at once. If you do though, you can definitely A/B test the results of the change. Once again, you have clear control and experiment groups since the groups are very comparable. So, as long as you have the computing power to run both simultaneously, you're good.

## üìñ Lesson 35: B Testing Possibilities   Lang En Vs3

To give you a bit more variety in the examples you've seen, I'm going to go over three more possible changes. Again, for each one, check the box if you think it would be useful to run an A/B test on the change. First, suppose you have a website selling cars, and you're considering a change, and you want to know wether that change will make customers more likely to return or refer their friends. Second, suppose a company wants to update their brand, including the main logo for their site. Would you A/B test this change? Third, suppose you want to test some changes to the initial page of your mobile application. You want to change things like what information is included, what wording you use, and what colors you use. Check each case that you think is a good candidate for A/B testing.

## üìñ Lesson 36: B Testing Possibilities Solution   Lang En Vs3

In the first case, you won't be able to test this using A/B testing. People buy cars very rarely, so it would take too long to see if you get repeat customers. And you don't necessarily even have data about whether customers are recommending the site to their friends. The second case seems like it would be easy to A/B test, and you certainly could collect data about user behavior with the new and old logo. Changing the branding of your company can be surprisingly emotional for your users though. They might need some time to get used to the new logo, so you wouldn't want to make a decision based on a short time window of data collected in an A/B test. The third case is a great thing to use A/B testing for. Once again, there is a clear control and experiment, and choosing good metrics to evaluate this type of change will be much easier than for the first two cases. You'll probably be able to choose something like the probability that the user progresses past the initial page.

## üìñ Lesson 37: 8   Other Techniques   Lang En Vs3

Okay, we've just seen some cases where AB testing isn't very helpful. What should we do in those cases? Are there any other techniques you can use? &gt;&gt; Well, there are a lot of different ways to gather data about users. And, some of them can be complimentary to running an AB test eventually, as well. So, for example, often you'll have logs of what users did on your website. You can analyze them retrospectively, or observationally, to see if a hypothesis can be developed about what's causing changes in their behavior. And, that's something where you may want to go forward, and actually design, and randomize, and experiment, do a perspective analysis. And, you can use the two data sources as, to kind of compliment each other. Where, you may develop a hypothesis from looking at the logs. But, you actually want to test out whether you can make this happen in an experiment. And so then, you'll want to run an AB test as well to compare the results, and, and see if your theory is valid. &gt;&gt; There's a whole host of other techniques as well, ranging from user experience research, to focus groups, and surveys, and human evaluation. Now, what happens is that AB testing can give you a lot of broad quantitative data. But, these other techniques give you very deep and qualitative data that are really complementary to AB testing. So, for example, you know, these other techniques can do a better job of telling you which mountain you should be on. So, we'll be going over some of these other techniques in lesson three. &gt;&gt; So, there are definitely some things that are very difficult to test with AB testing. So, for example, if you completely redo your site, or change the way that users interact with it. &gt;&gt; That's something that can be really hard to run a controlled experiment with, and we'll talk about why later in the course.

## üìñ Lesson 38: B Testing   Lang En Vs3

&gt;&gt; So when did people first start to use A/B testing? &gt;&gt; A/B testing has been around a long time, maybe we didn't call it A/B testing, but it typically came from field such as agriculture, where people would actually divide up their land into sections and then test what would work better for a particular crop or how it grew. And so there have been a lot of different fields that use what's effectively A/B testing for a long time. &gt;&gt; In the sciences more generally, hypotheses testing is a key way that they really determine innovation. In medicine, for example, their version of A/B testing is called clinical trials and that's how they determine whether a new drug is effective or not. &gt;&gt; The key thing that you want to see in A/B testing is that you have a consistent response from your control and your experiment group, so that you can actually really determine and structure the experiment, so you can determine whether there's a significant behavior change in your experiment group as opposed to in your control group. &gt;&gt; Okay. So what's the difference between running these types of experiments and running online A/B tests? &gt;&gt; Well, in the online world we often have a lot more data but kind of lower resolution. So in a traditional medical trial or in a user experience research study, you might have ten, twenty, fifty participants. So normally is the analysis different, because you have to be very careful, but you know a lot about each participant in your trial, so you may know their age, their weight. You know its a single person, you have their drivers license, all this stuff. You may have met them, whereas in an online study, you may have millions of users, hundreds of thousands of clicks, respondents. But you don't really know that much about who's on the other end of that data. So, it may be something where you have trouble distinguishing whether this is a single person, whether there's multiple people. Is it an internet cafe computer? And those are issues that really come up in online user data. &gt;&gt; The key thing to remember is that in the online world, when you're doing A/B testing, the goal really is to determine whether or not this new product or this new feature is something that users will like. And so the goal in A/B testing is to design an experiment, that's going to be robust and give you repeatable results, so that you can actually make a good decision about whether or not to actually launch that product or feature. &gt;&gt; Sounds good. I know Udacity uses A/B testing. How about other tech companies? &gt;&gt; It's everywhere right now. Google, Microsoft, eBay, PayPal, Netflix, like all of the major companies are definitely using A/B testing. There are even companies that focus on providing those services to smaller companies, like optimizing their sites better.

# üóÇÔ∏è Section 3: Choosing And Characterizing Metrics Subtitles

## üìñ Lesson 1: 1   Lesson Introduction   Lang En Vs3

Recall in lesson one, we used the example of changing the Start Now button on our online education website, Audacity, and we use click-through probability as the metric. In this lesson, we'll be focusing on metrics for experiments. We'll first cover how to define a metric, which involves brainstorming and establishing a suite of metrics. Then taking things from a high level concept of a metric to a practical definition that works given your data capture. Then we'll explain how to build intuition about your metrics and understanding your metrics sensitivity and robustness. We'll conclude with how to characterize the variability of your metric.

## üìñ Lesson 2: 10   Difficult Metrics Solution   Lang En Vs3

In the first case, Audacity definitely has access to this data. But students could take a long time between completing one course and signing up for another. So this metric probably takes too long to measure during most A/B tests. In the other two cases, the companies don't have direct access to the data they want. The shopping company doesn't know how happy its users are, and the search engine doesn't know whether users found the information they wanted. So these two ideal metrics can't be directly calculated at all. So the answer is that all of these metrics would be difficult to use in an experiment. Lets talk to Diane and Carrie about some other techniques you can use in the type of situation to get a proxy for what you want.

## üìñ Lesson 3: 11   Defining Metrics: Other Techniques   Lang En Vs3

&gt;&gt; We've just seen some examples of metrics that are difficult to measure directly or even impossible. You said, there are some other techniques we can use here, right? &gt;&gt; Right. And these techniques are actually useful for generating new ideas for metrics, as well. And so they're useful for both the new ideas, as well as digging deeper into a user experience than what your existing metrics might be able to get you. &gt;&gt; Great. So what are these techniques? &gt;&gt; There's a lot of techniques. This ranges from surveys to retrospective analyses, to focus groups. These techniques can be used for both brainstorming new metrics, as well as validating possible metrics. Now we briefly mentioned several of these techniques back in lesson one, but here we're going to be focusing on how to use these techniques to generate ideas for new metrics. Now we're only going to discuss them very briefly, there are details for all of what we're going to be talking about in a PDF linked off of the Instructor's Notes. &gt;&gt; So let's talk first about external data. So when you're looking for external data that's going to help you to define and validate metrics, the first thing that you can come across is companies that actually collect fairly granular data of that market share vertical such as how many people are interested in the travel industry and even for specific websites. They may have visitor data that the put together. So these are companies that just, comScore, Nielsen, there's a few others and they all use different techniques. So the second type of company that'll also have data you might find useful are those that actually run surveys of user. So these would be companies, such a Pew or Forrester, they may ask users how many devices do you have? How much time do you spend on them in a given day? So depending on what you're looking for, you may be able to either get these types of data from a source your company already has that they may have marketing or for other reasons or sometimes you can even get the results published on the web. The third category is what I'll call academic research, although it's a bit broader than that. And these are papers that actually try to establish metrics. They may run correlation studies. They may have very similar results to, you know, how much attention people are paying. So something like eye tracking where you may actually be able to find a study where they had a bunch of people in a lab and they look at how people read a page and they actually collect, you know, how much reading are they doing versus how much time do their eyes spend on the page versus how long did it take them to actually click-through to the next page. So you may be able to find data you need or you may be able to find something very similar. So normally, you can use these in multiple ways as Diane said. First of all, they're great for brainstorming. They help you think of new metric ideas. The second is they're good for validating metrics, where you can actually say, you know, on my site, I think I get this many visitors per day. But when I look at my whole industry, it looks completely out of whack with what I'm seeing. So these can help you establish that your own tracking for users or other sort of aggregate metrics is, is actually working. And then the third thing is they can actually help you develop validation techniques. So, if you see that study about eye tracking you can look at how they analyze their data and maybe you have other data that you could use and you could analyze it in the same way to validate your metrics. So they've a lot of different uses.

## üìñ Lesson 4: 12   Other Techniques, Part 2   Lang En Vs3

Now Carrie just spent a lot of time talking about how can you use external data? But there's also a question about how we can use our own data, or basically do it ourselves. Right? So there's sort of two classes of methods. Now one class of method basically uses all of our existing data. If you're running experiments, you probably have existing logs or data capture that you're using to measure metrics, and sort of track your business. And so it's going to be a set of techniques that are going to use that set of existing data, both retrospective analyses, as well as running experiments. We'll be going over those next. Now the other class of methods are when you want to gather new data. Now these are going to be methods that you'll use when you want to answer questions that you can't really answer from your existing data. The main techniques here are going to be things like, user experience research, surveys, and focus groups. And Caroline, I think you're going to going over those in more detail in the next video, right? &gt;&gt; Yeah. &gt;&gt; So let's talk a minute about what you can do with your own external logging data, or data capture, from your site. So first you can do what's called retrospective, or observational analysis, where you look at how a metric you're interested in, or just measurements you take from your site, change in response to changes you've made in the past, experiments you've run, big spikes in your business. So that's good to get a baseline, and it can also help you develop theories. You can also use this data in conjunction with other methods, such as surveys or user experience research, to help you develop ideas of what you want to look at. So for example, if you had a bunch of students in a lab doing a survey, or a focus group, and you could see that they were all getting stuck on a particular quiz, that might make you suspicious. And you won't actually use your logs to analyze, hey you know, how does this quiz behave, compared to other quizzes? How long are people taking, so on and so forth. The problem with these studies is that they're really, show you correlations, not necessarily causation. Because you're not running a parallel A versus B, or A versus B versus C versus D experiment. It's really hard to establish that the change you made actually caused what happened, and it's not some funny temporal effect. So that's an important difference between doing this type of experiment, or doing this type of analysis, and doing a real structured experiment. &gt;&gt; As Carrie said, using retrospective analysis is really best for establishing a correlation and not a causation. So one method that we can use to validate metrics is actually running experiments. Now this may seem horribly circular. We're going to run an experiment to validate whether or not we can use a metric for evaluating experiments? The thing is that, what we want to do is we want to measure that the metric is actually going to move as we make changes. So using Carrie's example, if we're using the length of time it takes for a student to complete a quiz as a metric for whether or not that quiz is actually going to be a good measure of students understand that material, then we have to actually test out and say, okay, well I'm going to try this quiz, and this other quiz, and I'm going to try a completely different formulation. And compare all of those different quizzes and say, okay, well, which ones of them actually move my proposed metric? Now we're going to talk about how we use this method more when we talk about sensitivity and robustness, later on in this lesson. &gt;&gt; Now, the other thing you shouldn't forget is to talk to your colleagues, because they have great ideas, or may have worked in other companies that use different metrics, or worked in research. And so you should talk to them about what ideas they think make sense for metrics. And then, the other thing is, of course, you want to think about your company's culture. Some companies don't want, necessarily, the people developing the website to think about how much revenue they're bringing in, they want to see innovative features. And then other companies are very focused on obtaining market share and adding users, where as, you know, another, a different company might want to just make their existing users happier. So it's really important to take into account your corporate culture as you define your business metrics. &gt;&gt; That makes sense. Do you usually only use one of these methods at a time? &gt;&gt; You can, but it's usually more robust to triangulate between different methods. So for example, let's say that you wanted to see if students are really happy with your website. So in our audacity experiment, we want to say well, are we, are we seeing signs of happiness? So first you might look through your logs and say, given that somebody took a second course, which we'll say is being happy, what did they do? How long did they spend? How many months were they active on the site? You want to get some baseline for that. And then given that they took a second course, then you might also want to trigger surveys that happened within your site, or do a focus group, where people who've done a single course, actually get a survey that says, you know, are you considering taking a second course? Were you happy with the first course? And that can give you, maybe not a lot of data, but some idea of whether they were happy, and how that relates to taking a second course. So it's usually a good idea to try to lay out the boundaries around your metric. You won't ever get exactly the same results from a couple of different techniques, but they can really give you a good sense, in conjunction, of what you might be missing, and what the right things to measure are.

## üìñ Lesson 5: 13   Techniques To Gather Additional Data   Lang En Vs3

There are three common techniques you can use to gather additional data about your users. They vary along two major axes. Some give more in-depth customized data. But others will be possible to run on a greater total number of participants. The first is called user experience research, or UER. Here, you can go really deep with just a few users, often by observing them doing tasks of interest, like taking a lesson of a course as well as asking them questions. You get a lot of detailed and in-depth information that is useful primarily for brainstorming ideas, anything from coming up with ideas to changes to tests, to identifying problems with your user experience in a way that you can translate into a possible metric that you could use to evaluate your A/B test. You can also use special equipment in a UER to get information that you couldn't get from your site's data. For example, you could use a special camera that tracks eye movement to see what users are looking at, even if they don't click or otherwise interact with the page. However, you'll want to make sure that you validate the results of a UER with something like the retrospective analyses Carrie talked about. The second method is called focus groups. With focus groups, you bring a bunch of users or potential users together for a group discussion. You can talk to more total users than with a UER study, but you can't go as deep with each person. Once you bring the users together, you could show them screen shots or images, you could walk them through a demo, and then you can ask them questions to elicit feedback. You can also often ask hypothetical questions, but even if you have a skilled facilitator, you run the risk of group think and convergence on fewer opinions. Have you ever been in a room and there's been a few loud voices that dominate? That same dynamic can occur in focus groups. Third, surveys are when you recruit a population and ask them a bunch of questions, either online, or in person, or via telephone. Surveys are pretty cheap to run on a whole bunch of users, and the data you get is much more quantitative, but it's not very deep or individually customized. Surveys are useful for gathering metrics that you cannot directly measure yourself, such as the question about how many students who take audacity classes get jobs, how productive they are at their job, and whether the classes contributed to their success. That said, be careful with survey results. Users don't have to tell the truth, and their answers can be dependent on how the questions are phrased. Because of this, you can almost never directly compare survey results to results computed using more observational methods. There are more details about these methods, as well as some other techniques, in the PDF linked in the instructor's notes.

## üìñ Lesson 6: 14   Other Techniques: Example   Lang En Vs3

Now let's discuss how discuss how Audacity could apply these other techniques, to either validate metrics from the customer funnel we talked about previously, or brain storm new metrics that are not covered by the funnel. At the top of the funnel, audacity could compare the count of how many users visit their website to externally available metrics, from companies like Com Score or Hit Wise. Audacity could also look at the completion rate of classes and compare that to externally available data. If Audacity sees that a particular lesson is getting a very low completion rate, they might do a UER study to investigate that. Watching the students try to complete the lesson can help you figure out, do the students understand where to click, can they find everything on the screen, are they progressing in order, how do they interact with the coach? You might see users twiddling their thumbs, waiting for a video to load and that might give you an idea that you should be tracking the latency. If you observe that people aren't seeing the instructors notes below the video, then you might want to use percentage of people who click on the link as the metric, or if latency looks like an issue then that's a metric you could track. For any metric that you come up with during a UER session, you might want to validate that metric with a retrospective analysis to see how that metric has varied over time. Or you might want to run some new experiments and see how that metric varies as you make changes. And finally, the bottom level of the funnel, whether students get jobs and, if they do, do the audacity classes help is an example of an unmeasurable metric. In this case surveys might help, maybe via email or on repeat students. For example, you could ask students if the material covered in class was touched on in any interview questions there were asked. Like I mentioned before, surveys are often useful for capturing metrics like these, that it's not possible to measure directly. The main issue, though, is that you can't directly compare the numbers from a survey to what you get from any other measurements. The biggest issue here is that the populations for your internal metrics and your surveys might not be comparable. You might be reaching a biased population with your survey relative to the population taking the class. Let's also go over the cases you saw before of metrics that were tricky to measure, and go over some other techniques you could use. I had mentioned before that if Audacity wants to measure the rate of students returning to take a second course, they could definitely track this metric long term, but they might not be able to use it for individual experiments. So, Audacity might follow up with a survey, to see what causes users to return. Then, if they can find something measurable that predicts returning well they could use that as a proxy in their experiments. For the shopping site trying to measure the average happiness of shoppers, this is the metric that they probably can't track even long term, but again, they can try to find signals that correlate with what they really want. They could instrument their website to pop us a survey at the end of every purchase, or they could run a small UER and use this data to brainstorm some metrics they could use. For the search engine that wants to measure whether users are finding the information they're looking for, there are a lot of possible proxies. They might be able to use, for example, the length of time spent on the search page. Whether the user clicked on any results that were shown or whether there were any follow-up queries trying to get at the right information in a different way. You might be able to identify which of these proxies are more promising by looking at external data about Information finding research or by running a UER study. Now we didn't discuss this but another very common technique in this situation, probably the most common, is human evaluation where you pay human raters to evaluate your site. This technique is covered in more detail in the PDF linked in the instructor's notes. Next, for each of the following scenarios, choose which techniques you would use to generate metrics. First, suppose Audacity wants one or more metrics to measure user engagement in their classes. They could look at course completion. But that's a long term metric. So it would be nice to come up with more detailed metrics. What techniques would you use to either brainstorm or evaluate potential metrics? Second, suppose a shopping site sells local food products and they want to decide whether to extend their inventory. Maybe they should add new flavors of coffee? Or maybe they should start a whole new line, such as cookbooks How could the site measure interest and potential new problems? Finally, suppose you wanted to test the placement of an advertisement on your site, and so you wanted to know which ads are getting the most views. It's hard to tell what users are looking at, so what techniques could you use to come up with a proxy? Choose from these techniques, which we just discussed. External data, a user experience research study, a focus group, a survey, retrospective analysis, or running a new experiment. It's probably possible to use all of these techniques in some way or another. But for each scenario, pick the two techniques you think are the most promising and write the corresponding number in these boxes.

## üìñ Lesson 7: 15   Other Techniques: Example Solution   Lang En Vs3

For measuring user engagement, one option would be to survey students in the course, asking how engaged they are, and use the survey responses as a proxy. This is option number 4. Another option would be to run a user experience study, and observe how students interact with the courses. If you also asked the study participants how engaged they were, you could figure out whether their engagement correlates with something easier to measure, such as the length of time spent on the page or clicking more links to extra material. So, that's option number 2. Finally, since you think that course completion probably predicts engagement well, but is too hard to measure, you could also do a retrospective analysis of users who have completed courses and see what behaviors they have in common. This is option number 5. Any two of these three would have been a good answer. You may also have thought of a good way to use one of the other techniques, which is great as well. The shopping site scenario is tricky. One option would be to add some products and see how often the new pages were accessed, but that could be a significant investment for products the users might not even be interested in. Instead, you might want to use a focus group to get ideas from users about what products they would like to see, or option 3. Alternatively, you could check if there is external data available from other shopping sites, indicating what users have wanted on those sites, or option 1. For the ad viewing scenario, you might also consider external data, or option 1. Specifically, you could look for studies that show whether there's something you can measure, like time spent on the page or mouse hover events, that you could use as a proxy for whether the ad was viewed. Or you could run a user experience study, option 2, where you observe users, see what ads they're paying attention to with an eye-tracking camera, and then try to find a metric that correlates with that. You could also assume that clicks correlate with views and use your historical data to estimate that the relative number of clicks among ad slots is a proxy for the relative views. Or you could look at the lowest position where anyone has clicked and assume that people have read at least that far. Then, you could do a retrospective analysis of your data to see how well these metrics would work. Again, any two of these three would have been a good answer, or maybe you thought of a way to use another technique.

## üìñ Lesson 8: 16   Metric Definition And Data Capture   Lang En Vs3

All right. We've gone over some ways of coming up with high level concepts for our metrics. What issues do you run into when you go to actually compute that? &gt;&gt; So, what we have so far is really a high level concept for a metric. To move from that to a fully realized definition, there are two main steps. The first step is to fully define exactly what data we're going to look at to actually compute the metric. This can be something as simple as which events do I count in my numerator and my denominator. It can also be questions like am I going to filter the data at all to remove robots or spam or things along those lines. And the second step is given those events, how do I summarize my metric? Am I computing an average, am I computing a median? We'll look at some very simple summer statistics plus also some much more complicated ones. Now the reality is that as you're going through these steps what you're really doing is you're building intuition about your metric, and about your data and about your systems. The mark of a really good analyst is whether or not you understand what changes in your data and in your metrics, you're system can and can't produce, right? So, as an example, I know that if I see a 10% change in click-through rate, that is wholly unrealistic. And realistically I probably have a bug in my experiment as opposed to a real true change that led to a 10% change in click through. That's just utterly unrealistic. &gt;&gt; Okay, so how would you actually go about doing this? Say, for example, click through probability from lesson one. How would we compute that? &gt;&gt; Well, we have to first decide, given the events that we observe, which ones were actually going to count for those metrics and how we're going to combine them. So, as an example, even for click-through probability, there's probably a few different ways to compute it. One is to count total number of clicks and total of number of page views and just divide total clicks divided by the total page views. Now, we can do a more nuanced version of that as well. So there's something called a cookie which is an anonymous identifier for a user. It's not really a user but we can, we like to pretend. And what we can do instead is say, did a cookie visit the site, and then, given that a cookie visited, did they click or not? Now, these different metrics, you know, one's a rate and one's a probability, they have different semantic meanings, but they also impact the act, the actual implementation of metric as well as things like the variability, which we'll talk about a little bit later. &gt;&gt; So you also have to worry about a bunch of other detailed things that come up. So, for example, if you have a page load, and then no click, but then a day later the same cookie comes back, loads the page, waits 15 minutes, and then clicks. Is 15 minutes too long? Is the day too long? Do you consider them all being associated with the same record? There's a lot of these little things that you have to look at. And depending how much data you have, you may want to plot your data over the course of a day, look at evening, you know weekday effects. Look at the week, maybe even look at the minute or the hour, where things are happening. So, you have to ask questions such as, what happens if someone arrives at your website at 11:50 p.m. And then clicks at 12:01 a.m. Now do you count that as being part of the first day's data? Do you completely separate the two events? Or do you count it as part of the second day's data? And although those events in aggregate can be pretty small, if you actually plot your data over the course of a day, you will see this big dip around midnight. &gt;&gt; I've totally seen that before. Now, what's interesting is that you have to also consider the technology that's used when you're actually capturing the events. So, as an example, JavaScript pings, which send a 204 request, are one of the most common ways of actually capturing clicks. Now, the issue is that certain browsers, for example, very old browsers, don't, don't implement JavaScript at all. And then other browsers, you know, so, for example IE versus Safari, will have different failure rates on that JavaScript ping back. And so what can happen is that as you look at different browsers or different platforms, you'll actually get very different click-through rates. And it's not because the actual click-through rates are different, but it's because the technology that you use in order to gather the clicks is actually different. And so in those cases you have to really work with your engineering team to understand all of those nuances, understand when you have a real difference, versus the difference through the underlying technology. &gt;&gt; Wow, who have guessed it's so much trouble to count clicks correctly.

## üìñ Lesson 9: 17   Metric Definition: Example   Lang En Vs3

Now, I'll step through an example of turning a high-level metric into a well-defined metric. In lesson one, we have chosen the high-level metric click-through probability. And we defined this as the number of unique users who clicked the button divided by the number of unique users who visited the home page. Now, this is actually not a completely specified metric yet. And there are also some other possible definitions. First, to use this definition, we need some way of determining whether two events are from the same user. Let's say we use cookies. Next, if the same user or cookie visits the page once and then comes back a week or two later, do we really only want to count that once? Usually, you'll want to count those visits separately, which means that you'll also need to choose a time period. Do you only count one page view per user each minute, hour, day, or what? So one fully specified definition would be that for each minute, you take the number of cookies that clicked during that minute divided by the number of cookies that interacted with the page at all during that minute. Now, this definition actually leads to a whole family of definitions since you could choose any time interval here. Changing this time interval could give you a different answer. For example, suppose your site only has one user who visits the site and clicks. Then 5 minutes later, they reload the page but don't click. 30 seconds after that, they reload again and do click this time. And then 12 hours later, they reload the page one last time and don't click. Assume that this user keeps the same cookie throughout this process. Now, if you group cookies by minute, there are three separate groups where these two events are in the same group since they happened within a minute of each other. Since two of these groups resulted in a click, and the third did not, the per-minute click-through probability is two-thirds. Similarly, if you group by hour, there are two groups, one of which had a click, so the probability is one-half. And if you group by day, everything goes in the same group and the probability is one. An alternative definition would be to remove the idea of a unique user and instead to create a unique ID for each page view. And then when a user clicks, record the idea of the corresponding parent page view. Then you could define the click-through probability as the number of page views that eventually result in a click divided by the number of page views. This data capture is usually easier than recording cookies and then later grouping by cookie. Now, this definition also needs a time interval. How long do you want to wait after each page view to see if it resulted in a click? Once you pick a time interval, you would count how many page views had a click within the specified time. One way these two definitions could give different results is if the user refreshes the page within the given time period. For example, suppose the user visits the page without clicking. Then 30 seconds later, they refreshed the page which generates another page view. Finally, 1 second after the second page view, they click. Then, using definition one with a time interval of one minute so that all these events are grouped together, there would be 1 cookie that clicked divided by 1 total cookie for a probability of 1. But for definition two with the same time interval, only 1 page view resulted in a click over 2 total page views, so the probability would be one-half. Finally, an even simpler definition would be to count the total number of clicks and divide by the total number of page views. As you know, this would really be a click-through rate, rather than a click-through probability. Which definition you use will depend on your product. Often, definitions one and two will be almost indistinguishable if you choose the same relatively short time interval. So you might want to go with definition two, since it's easier to compute. Now, for each of the three definitions we just discussed, the cookie-based probability, the page view-based probability, and the rate, consider whether it would be affected by the following problems. You can find the complete definitions for these metrics in the instructor's notes. First, suppose you want your metric to be unaffected by the user double clicking. That is, you want your metric to have the same value of the user double clicks as if they single click. Which of these metric definitions would work for this purpose? And second, when the user clicks the Back button, the browser may have cached the page, so another page view may or may not be generated. Which if these metrics would have the same value regardless of whether a second page view is generated when the user presses the Back button? Third, suppose you were worried that JavaScript may not be tracking clicks correctly. For example, it might send two clicks instead of one, or it might fail to record a click at all. Which metrics would have the same value regardless of whether problems such as this occurred? In each case, check the box for any metric that would give the same value regardless of whether the problem occurred. In each case, assume that all the relevant events occurs within the time interval for the probability metrics. So for example, if the time interval for the probabilities were five minutes, assume that a double click happens within five minutes.

## üìñ Lesson 10: 18   Metric Definition: Example Solution   Lang En Vs4

In the first case, both probabilities would give the same result, regardless of whether the user double clicked or single clicked. For example, suppose a user loads the page, which generates a page view. Then one minute later, they click twice, half a second apart. Assuming that the time interval is longer than a minute, say five minutes, then the cookie definition and the page view definition of probabilities will both count the single unique click and give a result of one. The click-through rate, on the other hand, would be two, since both clicks would be counted separately, which is a different answer than if the user had single clicked. In the second case, only the cookie definition would give the same answer either way. Compare these two timelines. In the first timeline, the user loads the page, which generates a pageview, then navigate away. Then, one minute later, they use the back button to return, and it generates another pageview. And then they click a minute after that. The second timeline is the same, but in this case, the browser cached the page, so the second pageview is missing. Thus, by looking at your data, all you can really see is that there was a pageview, and then two minutes later, there was a click. The cookie probability will calculate one unique click and one user in both cases. The pageview probability will also calculate one unique click, but in the first case there will be two pageviews in the denominator. And in the second, there will only be one, giving a different result. Finally, the rate will also calculate a different number of pageviews in the denominator, two versus one. Giving a different answer. In the last case, if two clicks are recorded instead of one, this is the same as double clicking. So the rate will be affected, and the probabilities will not. However, suppose, instead, that a click is completely missed. So, instead of recording one pageview with a click a minute later, you just record a page view with no click. All three definitions will give one in the case where there was a click, and zero in the case where there was not. So all the definitions will be affected. Perhaps it's not terribly surprising that it's hard to choose a metric that won't be affected by a problem this drastic.

## üìñ Lesson 11: 19   Filtering And Segmenting   Lang En Vs3

So, we've seen how important it is to precisely define what data you need to compute your metric. What other issues come up? &gt;&gt; Well, for one thing, you often see, sort of abuse on your site, such as spam or fraud, and you want to try to filter that out. For example, if you have a competitor, who's looking for your site clicking on absolutely everything, that's data you may not want to use in your experiment or you may even have someone malicious trying to mess up your metrics. Now, for a big change, if you're a big company too, if you have a change in an experiment, you might actually get blog coverage for that, and so you could potentially get a lot of traffic that's just coming to look at the experiment and you'd like to be able to at least identify that. Ideally, you'd be able to filter it out, but you want to at least flag the issue when it comes up. &gt;&gt; So there's a bunch of external reasons as to why you might want to filter your traffic. There's a bunch of internal reasons as well. So, one example is, what happens if your change only impacts a subset of your traffic? For example, maybe you didn't want to internationalize your change and so it only, only impacts English traffic. Or, maybe it only impacts the mobile app version and not the web version, right? And those are situations where filtering your traffic would actually make the most sense because you don't want to dilute your results, right? If you actually filter just the affected traffic, then you can actually increase the power and sensitivity of your experiment. &gt;&gt; Now, the goal of all this filtering is to de-bias your data, or dull-bias your data. But and often if you're filtering out spam and fraud, that's exactly what you accomplish. You do want to be careful you don't introduce bias into your data by doing the filtering. So for example, if you have a metric that can only be measured on logged-in users, you might actually be biased in your data because there's a bunch of sort of noncommittal or newer users trying to use the site who maybe haven't created an account yet. So that's a good example. The most common thing that comes up that we see is people who are filtering out especially long or weird sessions of user behavior. Now, that may be a totally legitimate thing to do, but before you do that, you want to check and make sure it's not actually your website, your metric, or even your logging that's causing these sessions to come up. &gt;&gt; So, how would you check? If you got some filter that you're considering, how do you decide whether to use it? &gt;&gt; Realistically in most cases, you're computing a baseline value for your metric. One way to actually figure out whether or not you're biasing or de-biasing your your data by applying these filters, is to actually slice your data. And what this basically means is that you're computing your metric on a bunch of disjoint sets. So for example, by country, or by language, or by platform. What you're going to do in that situation is you're going to complete your metric on all of these different slices. And then when you look at the filter, what you want to see is whether or not you're moving traffic disproportionately from one of these places or not. Now, if it is, and it makes sense because let's say all of your spam coming from, you know, Berzerkistan or some other country, then that might be a good thing. But if you're actually moving disproportionately from one of these places, it may be an indication that you're actually biasing your results further. &gt;&gt; Now, along those same lines, sometimes looking at week over week or day over day data or slicing down to an individual user group is a great way to identify spam or fraud, because often you'll get requests from a single IP address, or they'll come all at once. And so looking at week over week or day over day traffic pattern changes is a good way to spot something that looks at least a little unusual. &gt;&gt; The key thing to remember, when you're doing all of this type of analysis about whether or not you're going to filter your traffic or not, what you're really doing is, you're building intuition. You have to know what changes are going to be expected versus unexpected, so that you can determine, when you actually see the data for your real experiment, do I have a problem? Do I believe this? What's really going on?

## üìñ Lesson 12: 2   Metric Definition Overview   Lang En Vs3

So the first thing we need to do is actually define a metric, or maybe multiple metrics, for our experiment. In other words, we need to decide how we're going to measure whether the experiment group is better than the control group or not. Carrie, how do you get started with this? &gt;&gt; Well, the first thing you want to do is think about what you're going to use the metrics for, before you decide how you're going to define them. &gt;&gt; Right, and so, if you're thinking about how you're going to use the metric, there are really two main use cases. The first is what we call invariant checking, and these are the metrics that shouldn't change across your experiment and your control. For example, if you're running an experiment and a control, one major term of comparison is, are the populations the same? So you might check one, do you have the same number of users across the two? The other thing that you might check is, is the distribution the same? So, do you have comparable numbers of users across countries, or by language? All of these things are sanity checked to make sure that your experiment is actually run properly. The second use case is going to be evaluation, which is basically what Carrie talked about in the first place. &gt;&gt; And even for evaluation, you're going to have a couple of different things to think about. First, you have what you think of as high level business metrics, so that might be how much revenue you make, what your market share is, how many users you have. And then you're going to want more detailed metrics that focus on, say, the user experience with actually using your product, how long they stay on your page, things like that. &gt;&gt; Okay. Can you give some examples of when you'd need these more detailed metrics? &gt;&gt; Sure. So, let's say for example that you have a particular user experience. One example is maybe users aren't finishing a class, on audacity. Well, we don't know why, but we, what we want to do is we want to dig in to the user experience for that class. Maybe the videos are taking too long to load, and we should look at latency. Maybe some of the quizzes are particularly difficult, and the students are having trouble with that. Right? And so what we want is we want a set of techniques to help us really dig into that user experience. And so we'll talk about a bunch of these methods. One of them is going to be user experience research, which we can use in order to brainstorm metrics. &gt;&gt; And sometimes your business metrics may just not work out in the context of any experiment. So for you may not have the information you need, or the time that you're running the experiment may be too short to measure what you want. If we think about our online courses, maybe what we're really interested in is whether students get more jobs after taking the class, or something more nebulous like did they have improved skills. Now the problem with the jobs one is we just don't have complete information. We could try serving students, but we'll never really know if they got a job as a result of this class except maybe over a very long period of time. And then we also don't have time, right? We're running an experiment right now, and even if we could prove that someone got a job as a result of this, we wouldn't necessarily get that information within a month, they might get a job six months later. And then there's the category of things which are more nebulous, like improved skills. How do we really measure that in an experiment?

## üìñ Lesson 13: 20   Filtering And Segmenting: Example   Lang En Vs3

Looking at different segments of your data can also be useful for evaluating metric definitions, since you can look at how the different definitions vary by segment. This exploration is useful when building intuition about your data and your system. For example, let's look at total active cookies over time, since that's an important high-level business metric. So this plot shows the number of active cookies per day for the past four weeks. And there's this weird spike that showed up sometime last week. You can also see some weekly variation. The number of visitors looks higher over the weekend. One way I can verify whether the spike is odd is by looking at what's called a week-over-week plot. That is, I'll divide each data point by the corresponding data point from a week ago. As you can see, that tends to smooth out the weekly variation. So if I had wondered whether one of these spikes was higher than usual, I can see looking at this plot that it would stand out if one of these were abnormally high given what day of the week it was. Since we see that this same spike is still here, that makes it clear that this spike is not due to weekly variation. By the way, this corresponding drop, a week later, happens because we divided this data point by the spike that occurred a week earlier. Another good thing to look at is year-over-year data. This is the same as week over week, but dividing by the numbers from a year ago. That way, if there's an annual conference or something causing the spike, it will disappear. But, as you can see, the same spike is still here, meaning it's probably not due to a yearly variation. You can also see that the weekly variation is back, since the day of week is not quite matched up to the day of week from a year ago. Now the question is, if we can pin down what's causing this spike, since it doesn't seem to be caused by either weekly or yearly variation. One way we could figure this out is by looking at this metric across different segments of our population to see if one segment is causing the spike. So let's trying looking at how this metric varies by country. What's interesting here is that we don't see the spike in most countries, but we do see it in Berzerkistan, so that one country was causing the entire spike. At this point, it's a good idea to talk to the engineering team, and maybe they'll be able to figure out if this spike is in fact caused by only a small number of rogue IP addresses. And this is pretty likely to be spam, or a row grow bot, or some competitor trying to get information about classes, or something of that nature. Now suppose that you suspect there is an issue with JavaScript click tracking. Specifically, you're worried that JavaScript is counting each click event twice on mobile but not on desktop. Which of the following graphs would confirm this problem if it existed? First, would a graph of click-through-rate over time help you find the problem? This would be a line graph showing click-through-rate over time. What about click through probability over time? You can assume the page view definition with a time interval of five minutes. This graph would look pretty similar to the first, but showing a probability instead of a rate. Or how about showing both click through rate and probability on the same graph? So there would be two lines of different colors. Would it help to see click-through-rate over time broken down by platform? That is, there would be one color of line for mobile and another for desktop. Or similarly, would it help to see click-through-probability by platform? Finally, what about a graph showing both click through rate and probability by platform? That is, show both of the previous two graphs side by side so you can compare them. In each case, decide whether this visualization by itself could help you decide whether or not there was a problem, without referencing any of the other graphs. Thus, click-through-rate, by itself, couldn't tell you whether there was a problem, since you don't expect the click tracking issue to vary over time. And click-through-probability, by itself, wouldn't tell you much either. For each of the other four cases, check the box if you think this comparison would tell you whether or not there was a problem.

## üìñ Lesson 14: 21   Filtering And Segmenting: Example Solution   Lang En Vs3

The graph of click through rate over time might look something like this. Now, there are some interesting patterns in this graph. You can see both weekly and daily variation as people are more likely to click through either in the evening or on the weekend. But this graph can't help us answer our original question, which is, is there a problem with the click tracking? Like I mentioned, you can't really tell if this is how high the rate actually is or if it's being artificially inflated by double clicks being recorded. The click-through-probability over time might look something like this. It looks pretty similar to the rate. And again, it's hard to tell by this graph alone whether there's a problem. Now lets take a look at both the rate and the probability on the same graph. You can see that the rate is consistently higher than the probability. But that's to be expected whether there's a problem or not and it's hard to know how much higher you would expect the rate to be. The difference is also consistent over time, so it's hard to see whether there is a problem. Now let's take a look at click-through-rate by platform. Now this graph is pretty suspicious, because it shows the click-through-rate being higher on mobile than on desktop. But users will have different behavior on desktop than on mobile so, it's still not really clear that this is a problem with the JavaScript tracking and not just a difference in user behavior. You might think the direction of this is the opposite of what you'd expect. Like maybe you'd expect that the click-through-rate would be higher on desktop than mobile. But it can be pretty hard to predict things like that. So right now, this graph doesn't really give us enough information to be certain that there's a problem, although it is suspicious. Now if you plot, click-through-probability by platform, you won't see anything suspicious since, if JavaScript does send a duplicate ping. The click-through probability will eliminate that, collapsing it into one. So here we see that the probability is fairly similar between desktop and mobile. Which is maybe what you would have expected. So again, we can't tell whether there was a problem. Finally, if plot both rate and probability by platform, you can clearly see the problem. The click-through-probability is actually slightly lower on mobile, but the click-through-rate is significantly higher. You won't necessarily be able to narrow it down to duplicate JavaScript pings at this point, but this point's pretty clearly to some sort of instrumentation issue. This is a good time to talk to your engineers about what could be going wrong. So to summarize, only the final graph really made it crystal clear that there was a problem. The click-through-rate by platform looked a little suspicious, but it was hard to draw a conclusion from just that graph on its own.

## üìñ Lesson 15: 22   Summary Metrics   Lang En Vs3

Now we've gone over some techniques for getting to a high level concept for a metric, and then translating that into a specific data measurement, and also evaluating possible filters. And a while ago you also mentioned summary metrics. What are those? &gt;&gt; Well so what we have so far are direct data measurements. This can be a page view, a click measure of latency for a particular event. And what we want to do, at this point is, to summarize all of these individual events into a single summary metric. Now in some cases, the summary metric is really obvious. For example, if you're counting how many cookies are visiting the homepage, it's a count, or a sum of just the total count, right? You can also summarize that further into something like, you know, the average number of visitors per week. Now for a rate or a probability, if you actually look at the specific computation, it's actually computing an average over the clicks per page view for every single individual effect. Now in all of those cases, the summarization is actually part of the metric definition. But there's a whole bunch of other cases where you actually have a choice of summary metric. &gt;&gt; Okay. What are some examples of that? &gt;&gt; Well the primary situation that that occurs, is when your per event measurement is itself a number. And this is something like the load time of a video, or how many terms are in a query, or what the position of the first click on the page is. Now when you have a number like that, you have a whole set of metrics to choose from. For example, the mean, the median, the 25th percentile, the 70th percentile, the 90th percentile. You can choose any of those types of metrics to actually summarize your individual events. &gt;&gt; So how would you choose between these different options? &gt;&gt; Well so what you're going to do is, you're going to establish a few characteristics for your metric. The first one is going to be the sensitivity and robustness. You want your metric to be sensitive enough, in order to actually detect a change when you, when you're testing your possible future options, right? The second that you're going to want to characterize is what the distribution of your metric looks like, and that's going to help you choose. &gt;&gt; Okay. Can you elaborate on that? What kind of things do I need to do to characterize the distribution? &gt;&gt; The most ideal way of doing this is to do a retrospective analysis, and to compute a histogram. Now what a histogram is, is that on the x axis you have all of the different values for your metric. So for example, you're going to have all the different values for load time on the x axis. The y axis is going to be the frequency, so how often individual events have that particular load time. When you actually plot that histogram, you get a shape, that's going to be your distribution. And what you're looking at is what that shape is. If it's a very normal shape, then a mean or median's going to make a lot of sense. As it becomes more one sided, or lopsided, you might want to go more for a 25th, or a 75th, or a 90th percentile. That's going to also depend a lot on what change it is that you actually want to test. Now I think you're going to be going over these, some of the simple ones, in much more detail, in the next video. And in the Instructor's Note we have a lot more details about some of the other distributions that can occur. &gt;&gt; Okay. And do you have any general categories of metrics that you like to keep in mind? &gt;&gt; There's four categories that I like to keep in mind. The first one are the sums and counts. This is, like, how many cookies visit the homepage. The second one is going to be all of the distributional metrics: the means, the medians, the 25th, the 75th and 90th percentiles. The third category is going to be probabilities and rates. We've talked about those a lot in these past couple of lessons. And the fourth one is ratios. Ratios are very, very useful, because they can compute a whole range of different business models, and various different things that you may care about, but they can be very difficult to characterize. &gt;&gt; Okay. Let's go over an example before we move on to sensitivity and robustness.

## üìñ Lesson 16: 23   Summary Metrics: Example   Lang En Vs6

Like Diane just mentioned, there are four broad categories of summary metrics. First, there are sums and counts. An example of this type of metric might be the number of users who visited some page. Second, there are means, medians and percentiles. For example, the mean age of all users who have completed a certain course or the median latency of a page load. Third, there are probabilities and rates. Like you saw, a probability has 0 or 1 possible outcome in each case. For example, did the user click zero times, or one time? Whereas the rate could have zero or more, for example, the user clicks two times or four times. Fourth, there are ratios. These are more general than rates, which are two counts divided by each other. A ratio is any two numbers divided by each other. So it could have any value. Business metrics often make sense as ratios. For example, you might want to measure the probability that a user clicks a link that generates revenue, versus the probability that a user clicks any link on the page. Now, let's focus in on the second category and see how we can choose between these different options by looking at the distribution of our data. To do that, let's look at an example of some simulated user data about how long users spend on a particular page of the Audacity site. Here, I've shown a histogram of the time spent on the page where each bar is the frequency. So out of 10,000 data points, a little less than 3,000 fall in this first bucket. The mean of this data is about 1.3. That is, the average user spends about 1.3 seconds on this page. The median, on the other hand, is about 0.9. That means 50% of all users spend less than a second on the page. The reason for this difference is all these points off to the right. They increase the mean proportional to their size, so if one user stays for say six seconds, then that outweighs a few users who don't stay long at all. This is an example of an exponential distribution. If you're thinking about something like, how many users really get information from this page, you might want to use something besides the median or the mean. Maybe the 75th percentile or the 90th percentile. In addition to these possible metrics, you could take this one step further. Let's say you find out, in some UER studies that it would take the average person at least three seconds to read most of the content on the page. Based on this, you could use as your metric the percent of users who spend at least three seconds. In this case, that would be about 11% of the data points that stay about three seconds or longer. This metric does have its own problems, though. What if you make the page really confusing? That could artificially inflate this metric.

## üìñ Lesson 17: 24   Sensitivity And Robustness   Lang En Vs3

You mentioned that to choose a summary metric, we need to think about the sensitivity and the robustness of the metric. Can you explain that more? &gt;&gt; Yeah, so in lesson one, we talked about the sensitivity of a test to a change that you care about. The sensitivity of a metric is kind of the same thing. The idea is that you want to choose a metric that picks up changes you care about but, this is the idea of robustness, is robust against changes that you don't care about. So it doesn't move a lot when nothing interesting has happened. &gt;&gt; I think that makes sense, but could you give me some examples? &gt;&gt; Sure. So on our latency example where we're looking at the load time of a video, that's a great example of kind of classical statistics. Do you use the mean or do you use the median? Or, in our case, you may use neither. And the idea is that the mean is sensitive to outliers. So, if in your data you see a lot of cases of really long load times, maybe due to something going on in the users machine, or a bad network connection, then you want to maybe not choose the mean, because the mean is going to be pretty heavily influenced by those types of observations. And so that's called not being robust. Now, on the other hand, you could choose the median, which tends to be much more robust to that type of behavior, but if you only affect a fraction of your users, even if it's a fairly large fraction, like 20% with a change, you might not see the median move at all. So the median is robust. But in this case, you might want to actually consider using some other statistics, such as the 90th or the 99th percentile, and see how those change as well. So they might be a better reflection of what you're actually trying to measure. &gt;&gt; Okay. So how would you actually measure the sensitivity and robustness? &gt;&gt; Well, there are two main ways and ideally, you could do both. The first has to do with running experiments or using experiments you already have. So, for example, in our latency example, we could run a few simple experiments where we say, increase the quality of the video which should, in theory, increase the load time for users. And we could see if the metrics were interested in, actually respond to that. Now the date is going to get a little fuzzier so they may not respond exactly that way we think, but we should be able to tell if they're actually moving in a way that intuitively makes sense. And we can also use what were called a versus a experiments to determine if they're too sensitive. That's an experiment where you don't change anything. You just compare people who saw the same thing to each other. And you see if your metrics pick up any spurious differences between the two. And that's a really critical element to make sure that you're not going to be calling things significant that maybe don't really mean anything. And then you can also look back at experiments that were run by your company earlier. If you have experiments that have happened before and you also now have the benefit of hindsight, you may have user data about how well people liked it and how they responded. You can certainly look and see if those experiments move the metric you're interested in. Now, the second main category of things you can look at is sort of a retrospective analysis of your logs. If you don't have experiment data or if you can't run new experiments, then you can look back at changes you know you made to your site, and see if the metrics you're interested in actually moved in conjunction with those changes. Or you can just look at the history of the the metric and see if you can find a cause for any major changes that you see. That can help you get some good intuition about what's going on.

## üìñ Lesson 18: 25   Sensitivity And Robustness: Example   Lang En Vs2

Now lets talk about how to measure the sensitivity and robustness of some different metrics. Specifically, we'll try to choose a summary metric for the latency of a video, that is how long it takes the video to load. Like before, there are various different summary metrics we could use, median, 90th percentile, 99th percentile. So, we'll want to look at the sensitivity and robustness for each of those summary metrics. Like Carrie mentioned, we could start out by either looking at the results of a bunch of experiments or by doing a retrospective analysis. Let's do the retrospective analysis first. What we might do is de-segment the data by different videos. In other words, look at the distribution of load times per video. If we wanted to look at the distribution of a single video, we could plot it as a histogram like this, but this can get hard to see when we have multiple different videos to compare. Instead, we could draw something called a density line over the histogram that roughly approximates the shape of the histogram. Then we could plot only this density line for several different videos to compare. If I do that, I get something like this. Suppose here that I've picked five roughly comparable videos of the same size, so I get a roughly similar distribution of load times for the different videos. You can see two peaks here, a fairly long load time, and then more people with a shorter load time. This could happen if you had people with different types of Internet access, a slower Internet access and a faster one. Now, in order to characterize the sensitivity and robustness of different summary metrics, I can see how they vary across videos. So, here I've plotted a few different summary metrics by video. The median, 80th, 85th, 90th, and 99th percentile. In theory, since these videos are all comparable, there should not be too much difference between the different videos for a good metric. Here, you can see that the median, the 80th and the 85th percentile don't move around too much. They're pretty good. But the 90th and the 99th percentile are zigzagging around a bit. This is a good indication that the 90th and 99th percentile are not robust enough as summary metrics, since they're moving around quite a bit, even for videos that are pretty comparable. Of course, you have to be careful. Maybe these metrics are moving around for some other reason because the videos aren't actually comparable. For example, maybe the videos are at different resolutions or have a different encoding scheme. In this case, let's say that we're pretty sure that these videos are comparable and that we've checked for those things, but in general, if you think your metric might be too sensitive or equivalently not robust enough, then it's a good idea to dig and ensure that there's not some underlying factor that you haven't taken into account. Now, the other technique Carrie mentioned is to look at actual experiments, preferably experiments you've already run to save yourself some effort, but you can also run new experiments. So, for this example, for the load time, it would be great if we had experiments that changed the resolution. That should impact the latency, and if it doesn't, then our metric isn't sensitive enough. So let's take a look at data from five different experimental groups that have a range of resolutions. Here, video one has the highest resolution, which means that it should have the highest load time. And in fact, you do see that video one is off to the right a bit more. You can also see that the people who already have the slow Internet connection are a lot more affected by the resolution than the people with the faster connection type. Now let's also look at the same summary metrics for these experimental videos. What we should see is the latency going down as we increase the video number, that is, we have a lower resolution. And in fact, for some of these metrics we do see that, but for the median and the 80th percentile, they don't really seem to be moving. This is a good indication that the median and the 80th percentile are not sensitive enough. They're not showing a change when we do make a change that we care about. So in this case, the 85th percentile might be a good choice of a metric that's both robust and sensitive.

## üìñ Lesson 19: 26   Absolute Or Relative Difference?   Lang En Vs3

Are we ready to move on to variability? &gt;&gt; Not quite. First, we have to actually decide how we're going to compute the comparison. What we have is, we have a value for your experiment, and you have a value for your control. But, we have to actually decide, how are you going to compute the comparison between the experiment and the control? &gt;&gt; Couldn't you take the difference like we did in lesson 1? &gt;&gt; The simplest way is just to take the difference. And, if you're just getting started with experiments, or you're building up your knowledge of a whole bunch of different metrics, that's probably the way to go. &gt;&gt; True. But if you're running lots of experiments, you may want to consider computing the relative change, as opposed to the absolute change. In other words, the percent change. Now, the main advantage of computing the percent change is that you only have to choose one practical significance boundary, to get stability over time. Now, the main situations that I really see this being applicable are basically with regards to seasonality, and as your system is changing over time. So, lets say you have a shopping site, right? And in June, most people are on vacation, they're not shopping a lot. So, you have fewer users, you probably have a lower click-through rate. Whereas, in December, you've got loads of users, and a much higher click-through rate. You, if you have the same practical significance boundary, and across the same times, you can basically have the same comparison. Now, the other situation is that if you're actually running lots of experiments, and your system is actually changing over time, your metrics are probably changing over time as well. Again, if you're using the relative difference, you can stick with one practical significance boundary as opposed to having to change it as your system changes. &gt;&gt; Right. The main disadvantage is really variability. Ratios, such as relative difference, are not always as well behaved as absolute differences. So, if you're just starting out with this, or if you have some metrics you don't understand that well, it's often good to start with the absolute difference, and then work your way up.

## üìñ Lesson 20: 27   Variability   Lang En Vs3

Okay, what's the next thing we need to characterize about our metrics in order to analyze the experiment? &gt;&gt; Well, the next thing we need to talk about is variability. Up until now, we've talked sort of about developing intuition for the metric, about sensitivity, and robustness. But now, we're going to need a really more rigorous statistical definition of variability, so that we can use it in lesson four to look at sizing the experiment, and in lesson five to actually analyze the confidence intervals, and draw conclusions. We also want to check that the practical significance level we're interested in, is really realistic for our metric. If we have a metric that varies a lot under normal circumstances, that may not really work for us in practice, because the practical significance level we're interested in, just may not be feasible with this metric. &gt;&gt; Okay, so, how would we figure that out? &gt;&gt; Well, in lesson one we did a simple example of our click-through probability, where we looked at user data which was whether the user clicked on a specific link or not. And our summary statistic was the overall click-through probability. In that case, we were able to do an analytic, or a theoretical computation of the variance that we expected from our overall probability. Now, for other types of metrics, the same thing works. For example, if you have nice normal data, like demographic data, you have counts or probabilities, then usually, you can do the, the confidence interval, theoretically. But in some other cases, you may actually have to do this another way. So, if you're using something like a count or a probability, then you're only really dealing with the variability of a single measurement, or of a constrained one, in the case of probability. If you move on to using ratios or percentiles, like the 90th percentile, or if your data, like our latency data, is pretty lumpy, then you probably want to actually compute the variability empirically, which we'll talk about a bit more in, in a moment.

## üìñ Lesson 21: 28   Variability: Example   Lang En Vs2

Okay. We've looked at a lot of the distributions you might see in your data and used that information to choose a specific summary metric or to understand the sensitivity and robustness of your summary metric. But now, it's time to add a level of rigor. In order to calculate a confidence interval for your metric, you're going to need two things. You'll need to know the variance or equivalently the standard deviation of your metric and you'll also need to know its distribution. In lesson one, we calculated the confidence interval for a probability metric, which we assumed followed a binomial distribution. Then we calculated the standard error, which was our estimate of the standard deviation using the formula the square root of p hat times one minus p hat over n. Then we calculated the width of the confidence interval or the margin of error, as the z score of our confidence level times the standard error. So we definitely needed to know the standard error and we also used the fact that this was a binomial distribution in two ways. First, we use the fact that this was a binomial distribution to get this formula for the standard error. And second, this formula for the margin of error depends on the assumption that this is a normal distribution. And remember, the binomial approaches a normal distribution as N gets larger. So to summarize, if your type of metric is a probability metric, you can usually assume a binomial distribution, which approximates a normal for a large enough sample size and the estimated variance will be the standard deviation squared or p hat times 1 minus p hat over N. Now let's talk about some other types of metrics you might see. If your metric is a mean, then by the central limit theorem, your metric will follow a normal distribution if the sample size is large enough. And the variants of the metric will be given by the variance of the sample, which I'm representing as sigma hat squared divided by the size of the sample. Notice the distinction between the variance of the sample, that is take each of your data points and then collect the variance of them versus the variance of the actual metric, which means that if you were to collect a new sample, how would you expect this metric to vary? We'll be going over an example of this in a minute. Now this estimated variance is an analytic result. For other types of summary metrics though, estimating the variance analytically may not be so easy. Take the median, for example, if the underlying data is normal and the sample is large, then the median will be approximately normal. But if the underlying data is not normal, then the median might not be normal either. Suppose, for example, you're considering latency and your latency is a bimodal distribution since some people have a slower internet connection and some people have a faster type. Then for a large enough sample, the mean would be normally distributed. Where this blue line is the distribution of the underlined data and the pink line is the distribution of the mean. The median on the other hand, might not be normally distributed. In this example, you can see it looks pretty crazy. Other percentiles besides the median might also not be normally distributed. To estimate the variance of the median, you'd need to make an assumption about how the underlying data was distributed. And depending on your assumption, it might not be easy to estimate the variance analytically. So for a median or other percentile, the distribution of the metric depends on the distribution of the underlying data and your estimate for the variance will depend on this assumption as well. Another type of metric that's often easier to analyze is a count or rather, you're usually looking at the difference between two counts, say the experiment group minus the control group. This difference will always be normally distributed, but it often is especially for things like demographic data. The estimated variance of the difference is the sum of the variances of the individual variables. Other types of metrics, such as rates have more unusual distributions, but they can have analytic solutions. Rates tend to follow a poisson distribution and the variance of the poisson distribution is actually equal to the mean. So you could estimate it by taking the mean of your sample, which I'll call x bar. For your experiment though, you would be interested in the difference between two rates. That is you would need to estimate the variance of the difference between two poisson distributions. Or alternatively, you could test that the ratio of the means is close to one and compute the variance of that. Unlike for the normally distributed data, these difference in rates aren't likely to be either poisson or normal in distribution. You can do this analytically and there are several options. See the Instructor's Notes. But most people probably don't want to and it can be a big investment and hard to generalize. We'll talk in a minute about other options. Finally, businesses often want to use general ratios. For example, you might want to use the ratio of click-through probabilities in your experiment and control group instead of the difference. Like for the median, the distribution and estimated variance of a ratio will depend on the distribution for the numerator and the denominator. Often, you won't know those distributions and for anything more complicated than two normals, you often won't have an analytic result for the variance. You've already calculated a confidence interval for a probability in less than one using the binomial distribution. Now, I'd like you to calculate a confidence interval for a mean, which will follow a normal distribution. Suppose that audacity wants to measure the mean number of students that visit their home page each week. So they record home page visits for seven weeks and get seven measurements, one for each week. From this they can computer a mean, which I'll represent as N bar by summing the data points and dividing by 7. Now what is the confidence interval for this measurement? To answer this, you'll need to calculate the standard deviation of the seven counts audacity collected, which I'll call sigma. You can do this any way you want. One way would be to enter the numbers into a Google spreadsheet or you can copy the spreadsheet linked in the Instructor's Notes. To compute the standard deviation, you would type equals and use the standard deviation function. Then you would select the appropriate range, add a close paren and press Enter. Most programming languages also have built in functions to compute the standard deviation or you could write your own function. You'll also need to calculate the standard error for the mean, which is equal to the square root of the variance. That is sigma over the square root of seven, since seven is the number of data points in our sample. Use this to compute a 95% confidence interval, around audacity's measurement of the mean number of users per week. Enter the upper and lower bounds of your confidence interval in these boxes and round each answer to the nearest integer.

## üìñ Lesson 22: 29   Variability: Example Solution   Lang En Vs2

To calculate the confidence interval, I'll need the average of the seven measurements, the standard deviation and the standard error. I can compute the average using Google spreadsheets average function, and this comes out to about 91,000. Then I can compute the standard deviation, using the same strategy but with the standard deviation function. And this comes out to about 17,000. Finally the standard error for the sample will be the standard deviation divided by the square root of 7, which comes to about 6,000. Rounded to the nearest integer, I get 91,762, 17,015 and 6,430. Now, to calculate the confidence interval, the estimated mean, or n bar, will be the center of the interval. Then I'll need to calculate the margin of error. Because this metric is normally distributed, the margin of error is the z score times the standard error, and since we're using a 95% confidence interval, our Z score is 1.96, then the margin of error comes to about 12,605. The upper bound of the confidence interval will be the center plus the margin of error, which is 104,367. And the lower bound is 79,158. So if Audacity repeated the measurement for another seven weeks, they might expect to get anywhere from 79,000 to a 104,000 as the mean homepage visits per week

## üìñ Lesson 23: 3   Metric Definition Overview, Part 2   Lang En Vs4

Okay, so we've got a few different types of metrics we're interested in. There's the sanity checking metrics and then there's the evaluation metrics, which breaks down into these overall business metrics and the more detailed metrics, but for all of these, how do we actually go about making a definition? &gt;&gt; Well, there's really a bunch of different steps. The first is you want to come up with a high level concept for a metric. This is going to be your one sentence summary that everyone is going to be able to understand, something like "active users" or "click-through probability". Everyone sort of understands that. But then, from there, the second step is to really figure out all of the nitty gritty details. These are going to be things like, okay, you want to measure active users. How do you define what active is? Is it a seven day active, is it a 28 day active? Which events count towards activity? Right? An automatic notification may not count towards it being an active user. The third step is that you are taking all of these individual data measurements, and now you need to summarize them into a single metric. Maybe, maybe this is something like a sum or a count, could be an average, a median. We'll go through bunch of these possibilities later. Once you have that summary, now you have an actual complete metric definition, and now that you go into all of the other steps that Kerry talked about in the introduction. Okay. But before we get into that, I have one more question. It sounds like for sanity checking, you might need multiple metrics. But how about for evaluation? Should you stick to just one metric there, or do you also use multiple? &gt;&gt; It may depend culturally on the company and where you are, and how comfortable people are with the data. You can certainly pick one metric and some companies do, but at the same time, you know, your leaders may be more comfortable with a whole suite of metrics where they can see how things move. Now for PR purposes, external reporting, you may really have to settle on a single overall objective that you have for the company. And certainly if you work at a large company, you have the problem that you don't want to have one team working towards one goal and another team working towards another one. And so you may, in that case, also want to settle on a single objective. &gt;&gt; Now, one thing that you can do is if you have multiple metrics, you can create a composite metric. This can be something called an objective function or an OEC, which is, stands for an overall evaluation criterion, which is a term that Microsoft uses for when they come up with a weighted function that combines all of these different metrics. So when you do that, you now have a single combined metric that summarizes all your individual metrics. &gt;&gt; Now personally, I tend to shy away from composite metrics, because of course, first you have to agree on a definition. And getting everyone to agree to a combination of revenue and users and all this other stuff can be very tricky. I mean, it's hard to define. The second is you can run into problems if you over-optimize, looking at one thing, and you don't actually look at how other things move, particularly if you, if you make a radical change to your site. And then, the third is issue is just that the moment this metric starts to move, everyone who askz you to have a single metric typically comes and says well, why is it moving? And you end up looking at all these different things, anyway. &gt;&gt; Yeah, you basically go right back looking at all the individual metrics. I agree with Carrie. Now, the last situation that you'll sort of think about when you're trying to choose a metric is how generally applicable the metric is. If you're running a whole suite of AB tests, then ideally you'd have one or more metrics that you can use across the entire suite. It's much better to use a metric that's less optimal for your test, if you can use it across the suites that you can do the comparison, than it is to come up with the perfect metric for your test. Whenever you do something custom, it just introduces more risk. &gt;&gt; That makes sense. Now let's get started by coming up with some high-level concepts for metrics.

## üìñ Lesson 24: 30   Nonparametric Answers   Lang En Vs3

Carrie, what can you do if you want to use one of these more difficult metrics like a median or ratio or anything where you can't calculate the variants analytically? &gt;&gt; Well, we're going to talk in a minute about specifically computing the variants empirically, but there's actually a broader class of methods here called non-parametric methods. That means that you have a way to analyze the data without making an assumption about what the distribution Is. Now, these methods can be noisier, and they can be more computationally intensive, but they can also be very useful. &gt;&gt; Interesting. So, what's an example of that? &gt;&gt; Well, a great simple example is called the sign test. Now, let's imagine that we ran our AB experiment for 20 days, and on 15 of those days, the experiment side had a higher measurement than the control side. Now, that seems unlikely to have arrived by chance, and what you can do is use what we learned in Lesson One about the binomial to actually calculate how likely it is that that occurred, if there was really no difference between the two sides. Now, the downside of doing this is that it doesn't help you estimate the size of the effect. That is, you can't say, you know, I'm confident this is at least 2% change in my metric. The upside is, it's pretty easy to do, and you can do it under a lot of different circumstances. &gt;&gt; Okay, so if you wanted to launch any positive change in your experiment, then you could figure out whether there was one using a sign test. &gt;&gt; Right, and we'll talk a little further about that in Lesson Five. &gt;&gt; But, like you mentioned in Lesson One, sometimes you won't want to launch unless you meet some threshold, your practical significance level. So, what then? &gt;&gt; Right. Well, later in the lesson we're going to talk about how to actually compute the variance empirically, from the sample data. Once you've done that, you have two choices. First, if you look at your summary statistic distribution, and it's pretty nice and normal, you can do what we've done for more normally-distributed metrics and use a normal conference interval just with the variants you estimated empirically. If your data's a little funnier than that, or if you want to be really robust, you can actually go ahead and compute a non-parametric conference interval. It's pretty easy to do, and go ahead in that way.

## üìñ Lesson 25: 31   Empirical Variability   Lang En Vs3

So for more complicated metrics, you might need to estimate the variance empirically instead of computing it analytically. Are there any other reasons to use the empirical method? &gt;&gt; So, fundamentally what you're doing when you're computing the variance of a metric, is you're making an assumption about the underlying distribution of the data. For a relatively simple metric, making that assumption makes total sense and you should absolutely do that, but as you get to more complicated metrics, the distribution can be very weird and that's why you might want to shift to an empirical estimate. Now, at Google, what we found was that even for some of the simple metrics, the analytical estimate of the variance ended up being an underestimate. And we were actually very surprised by that, and we'll actually explain some of the reasoning as to why it turns out that it's an underestimate more in lesson four. But because of that, we shifted to using A versus A experiments across the board to estimate the empirical variability of all of our metrics. &gt;&gt; What are A versus A experiments again? &gt;&gt; So this class is about A/B testing, where you have control, your A, versus your experiment, the B. And any changes that you measure between A and B are due to the changes in the system that you've done. Now in an A versus A test, what you have is a control, A against another control A, and so there's actually no change in what the users are seeing. What that means that any differences that you measure are due to the underlying variability, maybe of your system, of the user population, what users are doing, all of those types of things. Now I think Carrie mentioned how we might use A versus A experiments to actually test for the sensitivity and robustness of metrics. For example, if you see a lot of variability in a metric in an A versus A test, it's probably too sensitive to be useful in, in evaluating a real experiment. &gt;&gt; Okay. So you can kind of pin down the variability with these A/A tests. How many do you usually need to get a good sense? &gt;&gt; Well, at Google we started with ten, then we moved to twenty. Now, we literally run hundreds of A versus A tests at a whole bunch of different sizes. Now, clearly there's a diminishing return as you run more A versus A tests. The key rule of thumb to keep in mind is that the standard deviation is going to be proportional to the square root of the number of sample. &gt;&gt; That's great if you have enough traffic and you run your own experiments so that you can run that many, but what if you can't for some reason? &gt;&gt; Well, another option is to run one really big A versus A experiment. And then there's a method in statistics called the bootstrap, where what you do is you take that big sample, and you randomly divvy it up into a bunch of small samples and you do the comparison within those random subsets. I think you're going to basically be explained that technique more in the next section. &gt;&gt; Yeah, so why wouldn't you always use the bootstrap method so that you wouldn't need to run that many A/A tests? &gt;&gt; It's definitely an option. One advantage of running the several, many, or just lots of different A versus A tests is because if your experiment system is itself complicated, it's actually a very good test of your system. So, for example, is your randomization function truly random? Do you have any other issues with regards to bias or weird population effects? Or anything else along those lines, you can actually truly test that with the A versus A tests. Now, in reality, what we have is a whole gradation of different methods. If you're starting out and you're running your first experiment using a relatively simple metric, do the analytical estimate of your ants. As you're starting to push towards more complicated metrics or you're running more and more features through, at that point, you might want to consider at least doing the bootstrap. Now if your bootstrap estimate is agreeing with your analytical estimate, you can probably move on and you don't have to worry about it. But if your bootstrap estimate isn't agreeing with your analytical, at that point you may want to consider running a lot of A versus A tests and really digging into understanding what's going on.

## üìñ Lesson 26: 32   Empirical Variability: Sanity Checking   Lang En Vs3

Okay. Let's look at the results of some A/A tests on click-through-probability. Since we've already done the analytics calculation for click-through-probability, we'll be able to compare the empirical results to the analytic results. But you can also use A/A tests in cases where you weren't able to do an analytic calculation. Now, there are three main things you can do, using A/A testing, and I'm going to go through an example of each of them. First, if you already have an analytic calculation of your confidence interval, you can check your A/A test results to see if you're getting what you expect. This functions as a kind of sanity check. If you're getting results that are not what you expect, this indicates that something is wrong with your calculations. Maybe you made an invalid assumption about the distribution of your data. Second, if you are willing to make an assumption about the distribution of your metric, but you weren't able to estimate the variance analytically, you can estimate the variance empirically, and then use your assumption about the distribution to calculate the confidence interval the same way we did before. And third, if you don't want to make any assumptions about your data, you can directly estimate a confidence interval from the results of the A/A tests. Let's start with this first use case. Let's say we run 20 experiments, each running on 0.5% of our traffic. Then, we run another 20, each on 1% of our traffic and 10 more, each on 5%. All of these are A/A tests, with no difference between the two groups. For the sake of simplicity, I'm going to say that each of these 20 experiments had 50 users in each group. In reality, you might see around 50 users due to the randomization, but I'm going to say that each group had exactly 50, just to make the calculations a little easier. Similarly, each of these experiments had 100 users per group. And each of these experiments had 500. Now let's say we analyzed each of these experiments using the same methods as in lesson one. How many of them would you expect to see a statistically significant difference between the two groups at the 95% confidence level? Even though there was no difference between the two groups, remember that in a 95% confidence interval, the true value, in this case zero, will only be captured 95% of the time. That means that out of 20 experiments, we expect to see one significant difference on average. Of course, for an individual group of 20 experiments, it wouldn't be that surprising to see zero or two, or some other number of significant results. Now, let's say that this Google spreadsheet shows the actual results of the experiment. This column shows the click-through-probability measured for group one and this column shows what was measured for group two. If I scroll down, I can also see the results for the 1% experiments here and the 5% experiments at the bottom. Now, if I analyze these numbers using the methods from lesson one, then I find that one experiment in the smallest group is significant and zero in each of the other two. That's not terribly surprising. If I had seen, say, five positives in one of the groups, then that would be a sign that something was wrong with the set-up of the experiment, or in the assumptions that I made. I'm not going to step through that calculation, but there's a sheet linked in the instructor's note that has the calculation carried out. Another thing we can check about the A/A tests is whether the differences follow the distribution we expect. To do that, I'll insert a third column which contains the difference between the two groups for each experiment. One thing to check is whether the differences are following a normal distribution as we expect. For the smallest experiments, the distribution looks fairly normal, but for the other two it doesn't. However, I'd say that this is probably due to the fact that we didn't run that many experiments. For 20 data points, this could be the result of a normal distribution and remember this one had only ten data points. Another thing that these plots show is that the distribution is getting tighter as the experiment size increases. It is hard to see that from the actual width of the distributions, since these plots are scaled to the range of the values. However, if you look at the actual range of values on the x axis, you can see that the smallest experiment, the differences ranged from negative 0.15, to positive 0.15. While for the medium experiments, the range was from negative 0.09 to positive 0.09, and for the large, it was from negative .03 to positive .03. So these distributions are definitely getting tighter as the sample size increases, which is in line with what we expected.

## üìñ Lesson 27: 33   Empirical Confidence Intervals   Lang En Vs3

Now, let's move on to the second use I mentioned for A/A tests, where we can estimate the variance empirically if we weren't able to calculate it analytically. I'll actually compute the standard deviation instead, since that's the direct analog of standard error. And, I do that by taking the standard deviation of each of the twenty differences from the smallest experiments. And, for that size of experiment, I get that the standard deviation of the difference is 0.059. I can also do the same thing for each of the other experiment sizes. I won't show that here, but, again, you can find the results in the sheet linked in the instructor's notes. Now, since we expect that our metric follows roughly a normal distribution, we can compute the margin of error as the standard deviation we just calculated times the Z-square of our confidence level. This is the same equation you saw for the margin of error in lesson one, but with the empirical standard deviation instead of the analytic standard error. If you didn't know beforehand whether to expect your metric to follow a normal distribution, then you might look at histograms like the ones we just looked at to see whether the metric looked like it followed a normal distribution. If we do that for the small experiment we just looked at with a 95% confidence level, then the margin of error comes out to 0.116. Remember, if we had done this analytically, the standard error depends on the pooled probability, which will be different for each experiment. That means we actually would have gotten a slightly different margin of error for each experiment. Whereas, empirically, we calculated one margin of error across all the experiments. If I do calculate the pooled probability and the pooled standard error for each experiment, I see that the pooled standard error varies, but it is always close to 0.059. So, we're getting roughly the same results either way. But if your metric doesn't seem to follow a normal distribution, then what can you do? Well, you can also directly estimate a confidence interval from the results of your A/A tests. This is the third use case I mentioned earlier. The way to do this is take all your differences and put them in order, in our case ranging from negative 0.15 to positive 0.15. Then if you want a 95% confidence interval, select a box that includes only 95% of the values. That is discard 2.5% of your values on each side. Then, the range of your remaining data points will give you a 95% confidence level. Since we have 20 data points in our smallest experiment group, dropping the highest and the lowest difference give us a 90% confidence interval. For our data, that gives us a lower bound of negative 0.1, and an upper bound of 0.06. Recall that the empirical standard deviation I calculated a minute ago was 0.059. Now, if I multiply that by 1.65 which is the z-score for a 90% confidence level, then that comes to about 0.097. So, if the true difference were 0 that would give us a confidence interval of negative 0.097 to positive 0.097. So, these two methods give a sort of close answer, but it's not that close. The main reason for this is that we only have 20 data points. You'd probably want to run more A/A tests to actually trust this confidence interval, unless each test was run for a pretty long time. You also can't differentiate between a 90% confidence level and a 95% with only 20 tests, since the best you can do is drop the lowest point and the highest point, which already takes you down to a 90% confidence level.

## üìñ Lesson 28: 34   Empirical Variability: Bootstrapping   Lang En Vs3

Like Diane mentioned, if you don't want to run a lot of AA experiments, you can run one experiment and then use it to estimate the variability of your metric using something called the Bootstrap Method. The idea in bootstrapping is that you run one experiment. And even though in our spreadsheet we only showed one number from each group, for each experiment, the click-through probability, those numbers were actually calculated from a lot of individual data points, about a bunch of individual page views and clicks. Then you take a random sample of those data points from each side of the experiment, and calculate the click-through probability based on that random sample as if it were a full experimental group. Then you record the difference in click through probabilities and use that as a simulated experiment. Then you repeat this process over and over, recording the results. And you can use the results as if you had run multiple experiments, even though you really only ran one big experiment. So the numbers in this spreadsheet, we've been assuming that they came from multiple A/A tests. But they actually could have come from one big experiment, from which we drew many bootstrap samples. Now it's your turn to calculate confidence intervals empirically. Follow the link in the instructor's notes to get to this spreadsheet with values of click through probability across 40 experiments. These could be 40 different A/A tests, or they could be 40 groups sampled using bootstrapping. The analysis is the same. Note that the sample size was 100 users in each group of the experiment. For each experiment, calculate the difference in the click-through-probability between the two groups. Then calculate a 95% confidence interval in two different ways. First, calculate the standard deviation of the differences, and then assume that the metric is normally distributed to calculate the width of the confidence interval. Use the average of the 40 differences as the point estimate, or the center of your confidence interval. Second, calculate an empirical confidence interval Using just the differences, not making any assumption about the distribution of the metric. You can use Google Spreadsheets or any other method you want to carry out the calculations. Write the upper and lower bounds of your two confidence levels in these boxes to four decimal places.

## üìñ Lesson 29: 35   Empirical Variability: Bootstrapping Solution   Lang En Vs3

I'll show you using Google Spreadsheets to perform the calculations. First, I'll make a new column with the differences. So I'll take equals, then first group minus the second group and I'll drag that down. Next I'll calculate the standard deviation of the differences, which comes to about 0.0364. I'll also calculate the average which comes to about 0.0043. So the center of the first confidence interval is 0.0043. And the margin of error is 0.0364 times 1.96. That is the standard deviation I just calculated, times the z score for a 95% confidence level. That comes to 0.0713. Then the upper bound of the confidence interval will be 0.0756 and the lower bound will be negative 0.0670. To get the empirical confidence interval, I'll sort the rows by difference, which is in column D. Then the bounds of the confidence interval will go from the second smallest value to the second largest. That means the balance of the empirical confidence interval are negative 0.06, and 0.08. The two confidence intervals are in the same ballpark but they're not super close. That's because for the empirical confidence interval in particular, it's best to have more than 40 experiments to be really confident in your answer.

## üìñ Lesson 30: 36   Variability Summary   Lang En Vs3

Can you summarize what we've learned about variability? &gt;&gt; So what we've learned so far is that different metrics have different variability. And in fact, the variability may be so high for a metric, that's actually not practical to use in volume in your, your experiment, even if the metric makes a lot of business or product sense. Now, what we've also learned is that to really be able to compute the variability, we need to understand the distribution of the underlying data. And we looked at both analytical techniques as well as empirical techniques for computing that variability. Now, hopefully what you've learned overall is that computing the variability of a metric is actually really tricky. And so you want to be taking a lot of care, and you want to be ensuring that not just any random engineer or product manager is going through and computing the variability, especially of complicated metrics

## üìñ Lesson 31: 37   Lessons Learned   Lang En Vs3

Diane and Carrie, how much time do you usually spend choosing and validating metrics, like we talked about in this lesson? As opposed to, say, running the experiment, or analyzing the results? &gt;&gt; So, what I would say is that, for a lot of the analysts that I've worked with at Google, we spend the majority of our time actually coming up with, validating, and choosing metrics to actually use in evaluation. As opposed to evaluating the experiments themselves. &gt;&gt; Especially validating metrics. A lot of people like to throw out product metrics that all seem perfectly reasonable. For example, the number of times a particular search term shows up when you're running a marketing campaign for a particular area like travel. Now this seems like a perfectly reasonably thing to track. But you really don't know how your site is actually going to respond. And so sometimes you end up with spurious results if you haven't used the metric before. Oh, it went up 100, right? Or you end up with no results at all. It didn't move at all, and you have no idea if that's normal or if that's something you should be concerned about. &gt;&gt; Okay. Do you have any stories about things that have gone wrong. &gt;&gt; Sure. Where would you like to start? &gt;&gt; How about data capture and metric definition? &gt;&gt; So one story I like to tell is about click-through rate. So back in 2006, you know, we were running lots of different experiments and everyone was using click-through rate to measure the user experience of their test. Now, you're like, how hard can it be to calculate click-through rate? I mean, really. It's clicks divided by impressions or page views. Well this is the problem. Talking about impressions or page views? The first page of the search results or all the next pages? Are you doing it in the US only or globally? Are you removing spam or are you not removing spam? I mean, you name it, we had to decide every single one of those questions. And so whenever someone was looking at experiment the first question was not oh, well how does this result compare to this other experiment? The first question was how did you compute six through eight again? Which, which, which definition did you use? Just being able to standardize the definition was really important towards just being able to start the conversation at a whole different level. &gt;&gt; Latency, which we‚Äôve been talking about a lot, or how long it takes a page to load is another good example. Now when you think about it you don‚Äôt really sit next to the user and measure this. You‚Äôre looking in a whole bunch of events and pings back that happen and hand offs. So for example, when does the first byte load? How did you measure that? When you say how long does it take the page to load, are you talking about when the first byte loads or when the last byte loads? And so, it really becomes a discussion about agreeing on what you're actually going to technically measure in this metric. And not a discussion about, hey, did we improve our latency here or not? &gt;&gt; Wow, so how about sensitivity and robustness? Any stories about those? &gt;&gt; Well, latency is another good example of those, because latency tends to be really lumpy. And you look at the mean, and it doesn't move at all. And part of the reason is that you have users who have very different connection speeds. So you have a bunch of people who have super fast speeds. You have people who have slow speeds. You have people who have some kind of problem on their computer, maybe they have an old browser. And so these signals that you're getting cause these sort of lumpiness in the distribution. So you really want to move away from using something like the mean and start looking at well, do I have to use a higher percentile? Because I can't get the mean to move at all. One change effects the people who have the fast connections, and one change effects the people who have the slow connections, and I can't get any sort of central measure. So we spent a lot of time with latency, looking for the right higher percentile metric, that we could actually get to move, when we knew we'd done something that was positive for the latency experience. &gt;&gt; Another example is from search. Now in search, we love the metric tasks per user per day. Now, the problem with task per user per day is not that it's a bad metric, it's that it's a very stable metric. And what that basically means is that, for just about any experiment you weren't changing that metric. And in fact, if you did actually measure a change in task per user per day, it was probably a sign that you screwed up your experiment as opposed to actually changing the metric. Now, it's worth noting that even as a business metric, task per user per day has a bunch of definition issues, like what time period actually makes the most sense. Do you care about per day, or per week? I mean, just think about your own search habits. You probably search a couple of times per week, or even a two week period, as opposed to on an every day basis. Now, one of the things to, sort of, think about as you're, sort of, defining metrics is, do your metrics have a big weekly variability? Businesses tend to love, like, these 30 day metrics, but if your if your site has a bunch of weekly variability, 28 days might make way more sense than 30 days. &gt;&gt; Yeah. I mean, there's a certain amount of judgment that you have to apply at the beginning of this process, but you also really have to try these things out. Because you actually won't think of all those types of effects. And when you try it, you'll suddenly see something really strange. &gt;&gt; I mean, as we talked about, the key thing is that you're actually building intuition. You have to understand your data, your system. You have to work with your engineers to understand the nuances about how data is actually being captured. &gt;&gt; Wow! Interesting.

## üìñ Lesson 32: 38   Lessons Learned, Part 2   Lang En Vs3

How about variability? First, we said, here's how to compute the variability analytically. But then, we said, sometimes it's maybe better to estimate it empirically. Which should I be doing? &gt;&gt; Well, it's usually good to start out with some kind of analytical characterization of your variability. Sometimes, that's even sufficient. But, if nothing else, it means that you have to look at the distribution, and start to get a feel for your data. Which is really important as part of this process. In some cases, like where you're using counts, or probabilities, or averages, your data is fairly nice, it may be sufficient. Or, it may give you a good sense of how to size your experiment. So, at least, you can tell if you're in the ballpark. &gt;&gt; I agree. I mean, you should always start with at least a basic analytical estimate of your variability. Now, where we sort of encounter the problem is that with a metric like revenue per query, it's actually very difficult to compute the variability of that analytically. It turns out that for some metrics, it was actually easier to compute it empirically as opposed to analytically. And, once we're just computing that empirically, then we were like, well, we may as well try it for all the other metrics. Which is when we discovered some of the discrepancies between the analytical and the empirical calculation, even for something like a probability. We'll be discussing the reason for that more in lesson 4 &gt;&gt; Okay, and just overall, what are some of the main lessons you've learned about metrics? &gt;&gt; You know, the one that I keep going back to over and over again is the necessity of invariance, or sanity checking, right? I went back to the very first experiment that Google ran, which is back in the year 2000, where we tested out should we showing 10 results, 20 results, or 30 results. It seemed a very straightforward experiment, but the results were actually really weird. And, what we actually found was that it was because the latency was changing as we ran those experiments. That was actually causing the change in the user response, as opposed to the number of resolves. And so, in that situation, using the latency as an invariant, as opposed to evaluation metric,was the right answer. &gt;&gt; Now, there's another story where, for a variety of reasons, we accidentally put an IFrame, an invisible IFrame, over the ads. And so, at least in some browsers, you actually couldn't click on the ad. Being able to know and say, oh, well when click-through rate changes by, that large an amount, that's just simply not possible. That is also, like, a really big thing to understand, and know, and have intuition about your system. &gt;&gt; You also have to just sort of see how these things behave in practice. When we first started looking at what made a bad query, a query where a user didn't find the results they were hoping for in their search. The thing that everybody said is, well, a bad query is the query where people click on a bunch of results, like two or more results, or they hit the next page button. Because, they obviously haven't found what they wanted on the first page. Now, it turned out very quickly that, that wasn't a great way to pick out bad queries, because you've heard the song from Avenue Q, The Internet is for Porn? Well, we get a lot of porn queries, and for various reasons, we won't talk about here. It appears that they all basically look like that. And so, we were really just picking out a lot of sort of browsing porn queries in the process. The second thing that came up here, which is more sort of time critical, is when we first started using these metrics, people were really using search engines for navigational results. They were looking for a particular store, looking for a particular home page. And so, you really got this behavior where people did a query, and then they clicked on the first result, and they were happy. They found what they wanted. But today, the amount of content on the internet is more diverse, plus people have gotten used to search engines being smart, so they really do more complicated tasks. And, even if you're looking for a person, you might end up clicking on their Twitter feed, you click on their Facebook page, you click on their university home page. So, the behavior has really shifted. And, this is a good example of a metric that seemed like a good idea, worked at a particular point in time, or didn't work, and then, sort of, stopped working as the use case evolved for the website.

## üìñ Lesson 33: 39   Lesson Conclusion   Lang En Vs3

In this lesson, we have gone over all the necessary steps to choose metrics for evaluating your experiment. First, we talked about establishing the high level concept for your metrics. Next, we needed to go into some nitty-gritty details in order to actually compute the metrics. Then we went over how to build intuition about your metrics and how appropriate they'll be for different experiments. Finally, you learned how to characterize the variability of your metrics. In the next lesson, we'll dig into how to design an A/B test using what you've learned.

## üìñ Lesson 34: 4   High Level Metrics: Customer Funnel   Lang En Vs3

Like Diane and Carrie said, we first need to come up with the high level concepts for our metrics. A good place to start is with the overall business objective. And what Audacity ultimately cares about is helping their students get jobs in finance. Of course, they also need their business to be financially sustainable. That is, they need to make money. We can break these objectives down into steps using a customer funnel like you saw in lesson one. At that point, we put four steps into the funnel. How many users visit the homepage, how many users explore the site further, how many create an account, and how many complete a course. The idea is that each of these is a step in the process of either getting a job or helping Audacity make money. In this case, because they might use coaching at some point. This funnel is pretty coarsely grained though. Can you think of more stages a student might go through? Maybe in between some of these steps or before or after them. Try to expand the funnel until it has at least ten total steps, and put the steps that you've thought of in this box. Since Audacity is a hypothetical company, you can use Udacity or another online education company to generate ideas if you'd like.

## üìñ Lesson 35: 5   High Level Metrics: Customer Funnel Solution   Lang En Vs2

Thanks for sharing your thoughts. Next, I'll go over what steps we put into our version of the Audacity funnel. Your steps might look a little different than ours, since Audacity is a hypothetical company.

## üìñ Lesson 36: 6   Refining The Customer Funnel   Lang En Vs3

Let's expand on this list of steps that makes up our current funnel. First, we could break up the step exploring the site. Exploring the site could mean that the student viewed a page listing all the classes that Audacity offers. Or, it could mean that, from that list, the student clicked on an individual course to view more details about it. Next, there are a lot of steps between creating an account and completing a class. First you could look at how many students start taking a class or enroll in that class. And since classes have several lessons, you could keep track of progress through the class. How many users finish lesson one, lesson two, et cetera. Audacity also provides coaching, so during the lessons students can ask for help. This is a paid service, so this is also how the business becomes financially sustainable. There are different levels of coaching based on how much help the student needs, so you could also track how many students use each level. After a student completes a course, you could go on to track whether that student enrolls in a second course. And ultimately you want to know how many students get jobs with the help of Audacity classes, although this could be hard to measure. Once we've made a list of potential steps, we can pick the most important ones, and use them to define a funnel. In this case I've chosen the number of visits to the homepage, the number of visits to the course list, the number of visits to each individual course page, the number of accounts created. The number of enrollments and courses, the number of students who used coaching, the number of students who complete a course, and the number of students who go on to get a job as the steps that will make up our primary funnel. The reason this is called a funnel is that typically you have fewer and fewer users that get to each stage of the funnel. If you look at the actual timeline for a particular user, though, it's more like a swirl. For example, in this funnel, users might complete courses without accessing coaching. And students who haven't completed a course yet, might return to enroll in a new course. It's also important to consider how this funnel would look across different platforms. Sometimes iPhone and Android apps have a slightly different flow both from each other and from your desktop site. Although, in this case, these stages should be supported on all platforms. You also need to be able to track progress through the funnel across platforms. For example, you might see that people start the class on their phone, then continue it on their laptop. Each stage in the funnel right now is a metric that you could use for an experiment, the number of users who reach that point in the funnel. This type of metric is called a count. However, sometimes instead of a count, you'll want to use a rate or a probability like you saw in lesson one. Often, you'll want to keep the counts at a few key points in the funnel. For example, the number of people who visit the homepage and the number of people who enroll in courses. For other points in the funnel, you might want to switch to a rate by dividing the number of users at that level of the funnel by the number at the previous level. For example, instead of measuring the number of users who visit the course list, you might measure the rate at which people progress from viewing the overview page to the list of courses. Since people move between these two levels of the funnel using the Start Now button, this is the same as the click-through rate of the Start Now button that we discussed in lesson one. The reason to use a rate is because you want to first figure out how many people enter the funnel. And then increase the rate at which people progress down the funnel. For each rate in the funnel, you might also care about the probability that a unique user progresses down the funnel. For viewing the list of courses, you probably care about how easy it is to do this from the Course Overview page, which is all right. But you probably also care about whether a student reached the course list at all, which is a binary probability. Now for all of these metrics, they're not yet metrics you could actually use. We start with these high level concepts. What business objective you're tracking through the funnel, whether you care about the absolute number or the count, the usability, or the rate, or the progression, the probability. But, we still have to transform these into full definitions. Before we tackle that though, we're first going to go into some other techniques for coming up with these high level concept metrics.

## üìñ Lesson 37: 7   Choosing Metrics   Lang En Vs3

Now that you've seen how to take a first pass at choosing high-level metrics, it's time to get some practice. I'm going to describe some of the experiments Audacity is considering, and for each one, I want you to choose the metric you think would make the best primary metric to evaluate that experiment. The first experiment that Audacity runs changes the course list page. Each course has a short description on this page, and Audacity tests updating one of these descriptions. The second experiment is to increase the size of the Start Now button on Audacity's homepage. And the third experiment is to add additional text to the site, explaining what the coaches do, and what other benefits come with the paid service. Now, for each of these experiments, I'm going to give you some metrics to choose form. Your first choice is the click-through-rate on the Start Now button on the homepage. Since the Start Now button is the only way of progressing from the homepage to the course list, this metric is also the rate at which users progress from the top level of our funnel we talked about previously, to the second level. The second metric you can choose is the click-through-probability of the Start Now button. That is, the probability of progressing from the homepage to the course list. The third metric you can choose is the probability that a student progresses from the course list to a specific course page. That's related to the probability that a user progresses from the second to the third level of our funnel. But note that it's not quite the same because this third level is the visit to any course page, and depending on the experiment, you might want to choose this to be a specific course page. The fourth metric you can choose is the probability of progressing from a specific course to enrolling in that course. Again, this is related to, but not the same as, the probability of progressing from this level of the funnel to this level, since you might want to compute this metric for a specific course, rather than across all courses. And the fifth metric you can track is the probability that an enrolled student starts using or paying for coaching, the paid service. Now, like Diane and Carrie mentioned, for most experiments, you'll actually want to track more than one metric. For this quiz, choose one of these metrics for each experiment as the primary metric for that experiment, and write the number of your your choice in these boxes. Please also discuss on the forums, what additional metrics you would want to track for each experiment. You might want to track more than one metric from the list, or you might think of other metrics you're interested in tracking.

## üìñ Lesson 38: 8   Choosing Metrics Solution   Lang En Vs3

And for the updated course description, the primary metric I would want to track is the probability progressing from the course list page to the course overview page for that course. That's metric 3 on our list over here. It would also be good to track continued progression down the funnel. If the updated description was accidentally misleading, you might see an increased click through rate to the course overview page, but then a drop off when students realized the course wasn't right for them. So it might be good to track the probability of enrolling in the course, and if you have time, maybe even how students are progressing through the course. Now by increasing the size of the Start Now button, you're making the button more prominent, presumably with the intention of making it easier to find. So this is a good time to use the click-through-rate of the button, which is metric 1 on our list. Click-through-probability could also be a good choice here, but like Diane mentioned in lesson one, rates are usually better for assessing how easy it is to find a button. For the experiment making the benefits of the paid service more clear, the best metric to use is the probability that the enrolled students start paying for coaching, or metric 5. It would also be good to measure the user retention over various time periods. That is, how long do the students continue paying for coaching? Or you might also look at some usage metrics. That is, among the students who are using the coaching, do they use the coaching more, now that it's more clear what the coaches can do?

## üìñ Lesson 39: 9   Difficult Metrics   Lang En Vs3

Often in AB testing, you'll find that a metric you want is difficult to measure. This can happen because you don't have access to the data or because the metric would simply take too long to collect. In the audacity example, the last level of the funnel was how many students get jobs. And both of these problems apply to this metric. Audacity probably doesn't know when one of its students gets a job. And even if it did, getting a job can be a long process. So, it might not be possible to use this as a metric for most experiments. Now I'm going to go over some additional metrics, and I want you to choose which ones you think would be hard to measure. First, suppose Audacity wanted to measure the rate of students returning to take a second course. That is, the number of users who complete one class and then start a second divided by the total number of users who complete at least one class. Would this be a difficult metric to use? Or what if an online shopping site like Amazon wanted to measure the average happiness of shoppers who complete a purchase? And finally, suppose a search engine like Google wanted to measure the probability that a user finds the information they were looking for within their search results. And for each metric check the box, if you think this metric would be difficult to measure, for either of these two reasons.

# üóÇÔ∏è Section 4: Designing An Experiment Subtitles

## üìñ Lesson 1: 1   Introduction   Lang En Vs3

Okay. So we've talked about how to choose and characterize metrics to evaluate for your test. And presumably you know what change or changes you want to test. Now it's time to really apply what you've learned in working through the decisions you need to make in actually designing your experiment. First, we'll need to decide how you define what you use as a subject in your experiment and in your control. In other words, what are the units in the population that you're going to be running the test on and comparing? We call this the unit of diversion. Next, we'll need to choose the population. You'll need to decide which subjects are eligible. Everyone? Only subjects in the U.S.? When you're testing how to change and computing the evaluation metrics, you need to ensure that you're doing the test and computing the metric on equivalent populations. Then we'll use those decisions and what we learned in lesson three to properly size your experiment, before concluding with a few other decisions you need to finalize your experiment design, such as the duration of the experiment.

## üìñ Lesson 2: 10   Ethical Considerations: Example Solution   Lang En Vs3

In the first case, there is probably no new information being collected, since the newsletter sign-ups were already being stored by user ID. As long as the original data collection was approved, this should be fine. The second case though, depends on exactly how the email addresses are being stored. The most natural implementation would be to store them by cookie, since the diversion is by cookie. That would make those cookies non-anonymous, which potentially impacts any other data that is being collected. For example, if another experiment is storing data by cookie that you wouldn't want re-identified, then that data has now become linked to an email address. Thus it would be good to seek out additional review for this experiment. This is a pretty subtle point, but in general, you to watch out for what whether you are accidentally identifying data that would otherwise have been anonymous. The third case does not require an additional review. Storing clicks by cookies is not a problem and is probably already being done else where on the site.

## üìñ Lesson 3: 11   Unit Of Analysis Vs. Unit Of Diversion   Lang En Vs3

Okay, we've talked about user consistency and ethical considerations. What's the third thing to keep in mind? &gt;&gt; The third thing to keep in mind is variability. Now, you might recall back in lesson three, we talked about how to compute the variability of a metric empirically. And that the reason why was because sometimes the empirically computed variability was much higher than the analytically computed variability. Now the reasoning for this is because that's what happens when your unit of analysis is different than your unit of diversion. Now, what's the unit of analysis? The unit of analysis is basically whatever the denominator of your metric is. So for example, if you're doing click through rate, and you have clicks divided by page views then page view would be your unit of analysis. Now, when your unit of diversion is also a page view, so as would be the case in an event base diversion, then the analytically computed variability is likely to be very close to the empirically computed variability. If, however, your unit of diversion is a cookie or a user id then the variability of your same metric click through rate is actually going to be much higher. Sometimes by a factor of four, five, maybe even more. And in those cases you really want to move to an empirically computed variability given your unit of diversion. &gt;&gt; Wow, why does that make such a big difference? &gt;&gt; So what happens is that when you're actually computing your variability analytically, you're fundamentally making an assumption about the distribution of the data. But you're not just making an assumption about the distribution of the data. You're also making an assumption about what's considered to be independent. You're basically doing these random draws and whether they're independent or not. When you're doing event-based diversion every single event is a different random draw, and so your independence assumption is actually valid. Now when you're doing cookie or user ID based diversion, that independence assumption is no longer valid because you're actually diverting groups of events. And so they're actually correlated together. And that will increase your variability greatly.

## üìñ Lesson 4: 12   Unit Of Analysis Vs. Diversion: Example   Lang En Vs3

Like Diane said, changing the unit of diversion can change the variability of a metric, sometimes pretty dramatically. I Googled Diane and some of her colleagues tested this by measuring the variability of a metric for two different units of diversion, query and cookie. Diverting by query is a type of event based diversion, since for a search engine, a query really is an event. The metric they measured was called coverage, which is defined as the percentage of queries for which an ad is shown. You can calculate this by taking the total number of queries with an ad and dividing by the total number of queries. Since the number of queries is the denominator, that means that the unit of analysis in this case is a query. So when the unit of diversion was a query, then the unit of diversion and the unit of analysis were the same. This plot shows the results Diane saw, with the standard error on the Y-axis and 1 over the square root of the sample size on the X-axis. The red line shows the standard error for cookie-based diversion and the black line shows the standard error for query-based diversion. To see why the X-axis is 1 over the square root of N, instead of N directly, recall that the standard error for a binomial is proportional to 1 over the square root of N. That's why both lines here are straight. Notice that when the unit of diversion is a cookie, which is not the same as the unit of analysis, the variability is much higher than when the two units are the same. The variability might be higher by as much as four times, depending on the sample size. The standard error for the query based diversion was also much closer to the analytical standard error. This lines up with what Diane mentioned earlier, that when your unit of analysis and your unit of diversion are the same, the variability tends to be lower and closer to the analytical estimate than when they're different. If you'd like to see more details about these results, see the paper linked in the instructor's notes. Now for each of the following cases, would you expect the analytic variance to match the empirical variance? First, suppose your metric is click-through-rate, defined as the number of clicks divided by the number of page views, and suppose your unit of diversion is a cookie. Second, suppose your metric is the number of cookies that view your homepage, which you're computing on a daily basis. You're using event based diversion, and in this case, an event is a page view. Third, suppose your metric is the percent of users that sign up for coaching, that is the total number of users who sign up for coaching, divided by the total number of users who are enrolled in any course, all within a one-week period. And you're diverting based on user ID. In each case, check the box if you think the analytical and the empirical variance would match.

## üìñ Lesson 5: 13   Unit Of Analysis Vs. Diversion: Example Solution   Lang En Vs3

In the first case, the unit of analysis is a pageview. This doesn't match the unit of diversion, which usually means the analytical estimate will be an underestimate of the variance. So, I wouldn't expect the analytic and empirical variance to match. In the second case, the unit of analysis is a cookie. Or really a daily slice of the cookie's activity, since I said that this metric was being computed daily. There isn't really a denominator to this metric, but since the number of cookies is what's being computed, the cookie is the unit of analysis. The unit of analysis is larger than the unit of diversion in the sense that one cookie could generate multiple pageviews. This is a problem, given that the unit of diversion is a pageview, because it means the same cookie could have events in both the experiment group, and the control group. That means this metric is actually not well defined for this experiment design. In general, you need your unit of diversion to be at least as big as your unit of analysis. In this case, cookie would work as the unit of diversion, and user-id would work, since one user-id can correspond to multiple cookies. But usually not vice versa. But, event level diversion is too fine grained here. In the third case, the unit of analysis is a user-id, which matches the unit of diversion. You'll never get analytic and empirical estimates that agree exactly, but they'll probably be a lot closer in this case.

## üìñ Lesson 6: 14   Inter  Vs. Intra User Experiments   Lang En Vs3

We've discussed the three main considerations when you're choosing a unit of diversion. Are there any other questions to keep in mind when you're choosing a population? &gt;&gt; Yeah, so first you want to think about the fact that, in anything but event diversion, if you do cookie diversion, if you do device diversion, you're really looking at proxies for users. And that means you're going to have one group of users on the A side of your experiment and one group on the B side. Now if you do event based diversion, you can end up with the mix of the same people on both sides. So you have to be pretty careful in this case to make sure you haven't inadvertently mismatched your users. There are some other options. There's what's called an intra-user experiment. That typically means that you expose the same user to this feature being on and of over time, and you actually analyze how they behave in different time windows. This has some pitfalls, for example, you have to be really careful that you choose a comparable time window. You don't want to do this in the two weeks before Christmas and then have them behave very differently in the second part. The other issues with a lot of features, you have a frustration or a learning problem, where people learn to use the particular feature in the first two weeks and then when you turn it off they're like, why did my website change? So you can get some different behaviors as a result. For certain other types of applications like search ranking, preferences or, you know, other things where you actually have a ranked order list, you have an option of running what's called an interleaved experiment, where you actually expose the same user to the A and the B side at the same time. And typically this only works in cases where you're looking at reordering a list. There's more information in the instructor's notes if you want to follow up on that. But what we mainly talk about in A/B testing is what's called inter-user experiments. That means you've got different people on the A side and on the B side. We'll talk in a minute about a refinement of that called a cohort. In a cohort, you try to match up your entering class so at least you have roughly the same parameters in your two user groups. In medical and drug treatment trials and things like that, they'll take this even a step further, and they'll actually match up patients in the trial based on demographic information, age, medical history, and they'll actually do a paired analysis. But because of what we don't know about people on the internet, we're, we don't even know if these are real people. You basically never see that in internet experiments.

## üìñ Lesson 7: 15   Target Population   Lang En Vs3

Okay, and assuming that we're doing an inter-user experiment. That is, there are different users in the different groups. Is there anything else we need to decide about our population? &gt;&gt; Well, you have to decide who you're targeting in your users. So, for example, there's some easy divisions of your user space. For example, what browser they're on what geo location they come from, what country, what language they're using. And depending on your website you may have more information. If they're logged in with a user ID, you may know how long they've been using your website. And then finally you may even have, depending on what you're doing, even demographic information, such as their age, that you could use to target a very specific population of, of your user space. &gt;&gt; Okay and would you typically make that decision in advance? &gt;&gt; Well there are a couple of reasons why you might make that decision in advance. So you may want to restrict who sees your experiment for a variety of different reasons. So for example, if you're running a feature and you're not sure if you're going to release it and it's a pretty high profile launch, you might want to restrict how many of your users have actually seen it. So you don't get any, you know, press coverage or blog coverage. You might also not want to go through the trouble of testing your feature if you have a new UI. If you want to release it internationally, you might have to go through much more of a testing process with, you know, is this language right? Is this you know, the way we want to say it in Japanese or something like that? So you might want to restrict by language. Or even you might not be sure that your feature works on old browsers, and you might want to just restrict it to say modern browsers. So, there's a variety of different reasons. And the last one might be that, if you're running a couple of different experiments at your company at the same time, you might not want to overlap, right? You might want to have, you know, oh, I'm just going to take this section of traffic, and you guys can run that other experiment in Korean, and it'll be fine. So, there's a bunch of reasons. And then the last reason is sort of more numeric, which is you may not want to dilute the effect of your experiment across a global population. So if you're analyzing an experiment for the first time, and it only affects English, you may want to actually do your analysis specific on English, and be able to ignore the rest of the population. &gt;&gt; That makes sense, and would you ever not want to target the experiment in advance? &gt;&gt; Well, sometimes you can't necessarily ID who a particular feature is going to affect. Like if you look for all search queries that bring up images. You may not really know exactly which space that's going to be. You may also be in a situation where you want to test the effect across your global population, because you're not sure if it really works. You know, you may not be sure your targeting is exact, the way you want. And then finally you may just not care that much because it could be a feature that effects 90% of your traffic. And it's just not worth the trouble to try to target the experiment. &gt;&gt; So what's involved in this? Is this the same as in lesson three, just compute my metrics on the segments I care about? &gt;&gt; Well, you probably want to talk to your engineering team first, or whoever implemented the feature. So that you can really tell are we sure that this is not going to trigger for this particular browser. It's, you know, is our targeting exactly right? And are we actually concerned about potential interactions so we might want to run a global experiment. The second thing is, you know, you always want to make sure that you have the same filters on the targeted and untargeted parts of your experiment. So you don't want to do accidentally include only logged-in users on the targeted bit. And then when you go to compare it to your global population you realize that there's something completely wrong. So you want to make sure that everything's lined up. And finally, before you launch a big change, you may actually want to go back and run a global experiment and make sure that you don't have any unintentional effects on the traffic you weren't targeting. because that can be a real issue.

## üìñ Lesson 8: 16   Target Population: Example   Lang En Vs3

Like Kerry said, if you think you can identify what population will be affected by your experiment, you might want to target your experiment to that traffic. That is, only run your experiment on the affected traffic. Let's take a look at how this can affect the variability of your metric. Just like you saw that changing the unit of diverging can change the empirical estimate of variability, filtering your traffic can change the variability as well. For example, suppose you want to control the change that will only affect users in New Zealand to see whether it increases click-through probability. You run the experiment and looking only at the New Zealand data, you find that there were 6021 unique visitors in the control group, 300 of whom clicked. There were also 5979 users in the experiment group, 374 of whom clicked. Based on this and using the equations you saw in lesson one, the estimated probability of a click in the control group or the baseline probability is about 5.1%. The estimated probability in the experiment group is 6.3%. And the pooled standard error is 0.0042. Now suppose you had analyzed the global data instead of just the New Zealand data. Since the change doesn't affect other traffic, let's say for the sake of simplicity that in all the non-New Zealand traffic, there were 50,000 users in each group and 2,500 of them clicked in each group. Now, for the global data, which includes both the other data and the New Zealand data added together, what would be the pooled standard error? Is this higher or lower than the pooled standard error for just the New Zealand data? Write your answer in this box to four decimal places. Also, is there a statistically significant difference at alpha equals 0.05 in New Zealand? How about globally? Select yes or no in each case.

## üìñ Lesson 9: 17   Target Population: Example Solution   Lang En Vs3

To calculate the global pool to standard error, I'll first need to calculate the number of users into the number of clicks in the control and experiment groups globally by adding up the New Zealand data with the other data which gives these results. Then I can calculate the pooled probability and the pooled standard error which come out to 0.051 and 0.0013. Recall that for New Zealand, the pooled error was 0.0042 which is about four times as large. In this case the variability of the global data as measured by the pooled standard error is lower than the filtered data. Mostly because there is so much more data globally. This will often be the case in practice. But it's also good to keep in mind that, in practice, your data will actually be a mix of different populations almost every time. When you filter, you're going to get a smaller but also more uniform population. Which means that for the same number of data points, the variability of the filtered data is likely to be lower. In order to find whether the difference was statistically significant we first need to calculate the difference which globally was 0.0013. Then I'll construct a confidence interval and see if it includes 0. The margin of error will be the pooled standard error times the z score of 1.96 which comes to 0.0025. Since the margin of error is wider than the estimated difference, the confidence interval will include 0. Which means that this difference is not significant. Carrying out the same calculations for the New Zealand data, the difference observed is 0.012. And the margin of error is 0.0082. In this case, the margin of error is smaller than the observed difference. So the confidence interval will not include 0. And the results are significant. Note that even though the New Zealand only data had a higher variability, and thus a wider margin of error, or a wider confidence interval, the New Zealand results were significant whereas the global results were not, because the observed difference was so much higher in New Zealand, 0.012 versus 0.0013. Adding all of the unaffected traffic that was outside of New Zealand diluted the difference in the global data, causing the result not to be significant. To summarize, the global pooled standard error was 0.0013. The New Zealand results were significant. And the global results were not.

## üìñ Lesson 10: 18   Population Vs. Cohort   Lang En Vs3

You keep talking about the population, but you mentioned cohorts earlier too. What's the difference? &gt;&gt; Well, in the population you have a whole group of users. But within that population you can define what's called a cohort. And typically, this means people who enter the experiment at the same time. So for example, if you just divert by cookie or a user ID, and you look at this particular country. Now, you may have all kinds of problems during the span of your experiment where people come who come in classified as that country, either drop out of the experiment, or potentially even for mechanical reasons, like their IP gets reclassified into a different geographic region. So you can lose users, and gain users, and have users who've been exposed to the experiment for different period of time. So cohort usually means that you define an entering class and you only look at users who entered your experiment on both sides around the same time, and you go forward from there. You could also use different information, so you don't look at people who join later, right? You start with your initial group, and you can also use other information. Like, if you want to know that oh, these are users who've used your site consistently for the last two months. Or who have both a mobile and a desktop associated with their user ID, you can use all of that to define a cohort. &gt;&gt; Okay, and when would you want to use populations versus cohorts? &gt;&gt; Well, cohorts are harder to analyze, and they're going to take more data because you'll lose users. So typically, you only want to use them when you're looking for user stability. So say, you have a learning effect or you want to measure something like increased usage of the site or increased usage of a mobile device. Those are cases where you really want to see if your change had a real effect on their behavior relative to their history. And then you want a cohort. If you don't need those types of metrics, then you can probably stick with the population. &gt;&gt; Got it.

## üìñ Lesson 11: 19   Population Vs. Cohort: Example   Lang En Vs3

Like Carrey said, you might want to use a cohort, depending on the type of problem you are looking at. A cohort makes more sense than looking at the entire population, if you're looking for learning effects, if you're examining user retention, if you want to increase user activity, or anything else that requires the user to be established for some reason. Going back to the Audacity example, here's a situation where they might need to use a cohort. Suppose they have an existing course that's already up and running. Some students have completed the course, other students are midway through, and there are students who have not yet started. They want to try changing the structure of one of the lessons to see if it improves the completion rate of the entire course. Now, because they want to see, what happens throughout the course where students can pause or unpause the lessons, switch devices, etc., the unit of diversion will need to be a user-id. That said, it doesn't make sense to just run the experiment on all the users in the course. To see that, suppose that this blue line shows the time that students start the lessons that Audacity is changing with later times to the right. Each purple dot represents a user or student. So, at this time four students started the lesson at the same time. Now, suppose that Audacity starts running the experiment at this time, for students who started the lesson a while ago, they may actual have finished the lesson already. So, they're already past that lesson, and they're not even going to see the change. So, taking this whole population of user-ids, and running the experiment on them, isn't what Audacity wants. Instead, it would make sense to use a cohort, and only include users who started the lesson after the experiment was started in the experiment. That is, it's a subset of the population, who have the shared experience of receiving the new lesson, and not seeing the old lesson. Now, for the control, Audacity needs to create a comparable cohort. They can not just use these users, who are not included in the experiment as the control because there may have been other system changes in that time that affected the new users. So, instead, Audacity will need to split this cohort into an experiment cohort and a control cohort, so that they all have the same timing of when they started the lesson. Now, in this example, we chose a cohort based on time, but you could also use other factors to create a cohort. Just like running your experiment only on New Zealand, using a cohort is limiting your experiment to a subset of your population. Thus, in the same way, using a cohort can affect the variability of your metric.

## üìñ Lesson 12: 2   Unit Of Diversion  Overview   Lang En Vs3

Diane, can you tell me more about how to choose the subject of your experiment? First, what does that mean? &gt;&gt; So, what you need to do is you need to decide how to assign events to either the control or to the experiment. Now what you could do is you could randomly assign every event. So for example, every page view to either the control or the experiment, but then if you have a user visible change, the user's basically going to be like, oh, I see the change. They reload. I don't see the change. What's kind of going on? So typically what you want to do for a user visible change is that you want to basically assign people as opposed to events. Now the problem with online testing is, what's a person? Like, how do you actually determine who a person is? One obvious example is to use a user ID like a log in, but the problem there is, each person could have multiple log ins. You know, for example, I have both a consumer and a corporate account. Right? And so, you know, if you're a student, you could have multiple different accounts, if you wanted to try the quiz in different ways or for whatever reason. So each person doesn't have to have a single account. Now another option is to basically use an anonymous identifier like a cookie. This is basically something that gets assigned to a particular browser in a device. Now, again the same issue happens here, right? Because this is tied to a single browser and single device if you change from your laptop to your mobile phone, if you change from Safari to Chrome, in each of those cases, you're going to be changing the cookie. And, so in any case, if you're trying to basically assign a person, you're going to be using some imperfect proxy. Now in each of these cases, whether you're doing an event based diversion, a cookie based, or a user ID, these are all what we call our unit of diversion. So I think before we sort of discuss the pros and cons of these, I think you're going to be going through the details of what each of these really mean. &gt;&gt; Yeah.

## üìñ Lesson 13: 20   Experiment Design And Sizing: Overview   Lang En Vs3

We have talked about how to choose both our unit of diversion and our population. What's next? &gt;&gt; Next, we have to actually size our experiment and control. Now, the key thing to keep in mind is that this is an iterative process. What we're going to do is we are going to try out some decisions for our unit of diversion and our population, see what the implication is on both the size as well as the duration of our experiment. And then if we don't really like those results, we'll need to revisit our decisions and iterate. &gt;&gt; Sounds good. Where do we start? &gt;&gt; Well, it makes sense to start with the sizing process, and we'll look back at what we learned in lesson one, and then revisit it based on what we've talked about since then. &gt;&gt; From there we'll discuss some of the other design considerations, what the duration of the experiment is, what the risk factors are and all of those types of decisions.

## üìñ Lesson 14: 21   Sizing: Overview   Lang En Vs3

So Carrie, in lesson one, we talked about how the size in experiment based on your practical significance, your statistical significance and the sensitivity you want. What do we need to update about that based on what we've covered since then? &gt;&gt; Well, we have a couple of things that we've just looked at. Your choice of metric, your choice of the unit of diversion, and your choice of population. And all those things can affect the variability of your metric. So you want to take all the stuff into account and then start to determine the size based on the process that we talked about before. And then you're going to have to figure out if what you've planned to do is really realistic, given how long you have to run the experiment and the variability of your metrics. &gt;&gt; Okay. Can you give me an example? &gt;&gt; Well let's think about what we talked about before. The page load time, the 90th percentile latency. Now originally you could measure that in an event-based diversion because you just measure each page load time. But let's say we now say, okay, we also want to look at a user ID diversion where we look at whether that user uses our site more or less based on the latency that they're experiencing. Now that's a little more challenging as a metric, because you're going to need a fair amount of user data to make that work. And if you're originally planning to run this globally, you may realize looking at the variance of your metrics, that that's just not really realistic. It's going to take a very long time to get a lot of data, it's a big investment. So you may want to go back and say, you know what? I'm really affecting the 90th percentile here, that's what I'm targeting. So, let's look at people with slow connections. And then maybe, because I need to get enough data, I want to look at a cohort of users who've used my site fairly regularly over the past two months. And that way, I can get more data about them more quickly. And so, while this restriction may give you a smaller scope to your project, it can really give you a better sense of whether you're going to get a signal out of this experiment at all before you invest the time and the user time in actually running a much larger experiment.

## üìñ Lesson 15: 22   Sizing: Example   Lang En Vs3

Suppose that Audacity sometimes includes promotions for their coaching next to the videos in their free courses. The promotion might say something like, do you need some help with this material? Contact one of our coaches. They run an experiment changing the wording of this message and they use click-through-rate as their metric. Now since the unit of analysis is a pageview Audacity also might want to make the unit of diversion a pageview. However, they might also want to consider using a cookie to get a more consistent experience. Let's see how that would affect the size of the experiment. If you calculate the variability analytically, it won't change between the two units of diversion but for the cookie-based diversion, the analytic estimate is likely to be an under-estimate. So Audacity does an empirical estimate of the variability with 5,000 page views in each group. First they randomly sample by page view to simulate the unit of diversion being a pageview and they get a standard deviation of 0.00515. Then they randomly sample by cookie to get a different set of pageviews and the standard deviation is 0.0119. Now that's quite a difference but how will it actually affect the size of the experiment? In order to calculate the size, we can assume that the standard error for the experiment is proportional to one over the square root of the sample size. That means that as you vary the sample size the standard error would look like this for the cookie and the pageview units of diversion. As in the graph before, the x axis is 1 over the square route of N so the standard error follows a straight line. Using this information about how the standard error varies with the sample size, you can calculate the size that would be needed. If you want to see how to do this, you can look at the r code linked in the instructor's notes which will do the calculation. In this case, lets say that the practical significance boundary, or d min is 0.02. Then if Audacity used pageview as the unit of diversion, they would need about 2,600 page views to get enough power. But if they diverted by cookie, they would need about 13,900 pageviews, which is a huge difference. In cases like these, you probably need to actually plug the numbers in to get the exact size, but it's also important to have an intuition for how this type of change will affect sizing. That way you can quickly decide which changes are worth considering and when you look at your data you know whether its plausible or not, and the way to build intuition is by getting lots of practice.

## üìñ Lesson 16: 23   How To Decrease Experiment Size   Lang En Vs3

Now let's say Audacity does another experiment, this time changing the order courses appear on their course list page. The metric they use is the overall click-through-rate to individual course pages. That is, the total number of times the user clicks on any course, divided by the number of page views. To give a consistent user experience of the course list, while still including non-logged in traffic in the experiment, Audacity chooses cookie as the unit-of-diversion. And they use their standard values of 0.05 and 0.2 for alpha and beta. Their practical significance boundary, or dmin, is 0.01, and they empirically estimate that their standard error to be 0.0628 for 1,000 page views, sampled using cookie based diversion. The result is that Audacity would need at least 300,000 page views in each group in order to have enough power, which would correspond to all of their traffic for a month. Audacity isn't willing to spend that long getting results on this one experiment, especially if it means they can't run any other experiments in the meantime. Which of the following strategies could Audacity try, in order to reduce the total number of page views needed to get a result? First, you already saw in lesson one, that they could increase the practical significance boundary, that is, not try to detect a smaller change. Or, alpha or beta, that is, accept a higher probability of a false positive or a false negative. So, this would work. But here are some other things they could consider changing. First, would it help to change the unit of diversion to a page view rather than a cookie? That is, if each page view was randomly assigned to the control or experiment group, even if two page views came from the same cookie, would this reduce the total number of page views needed? Second, would targeting the experiment to specific traffic help? That is, suppose that Audacity has classes in many languages, and this experiment only reorders the English classes. So then, would it help to restrict this experiment to only the English traffic? And finally, would it help to change the metric to a cookie-based click-through-probability rather than a click-through rate? Check any of these three changes that would reduce the total number of page views needed.

## üìñ Lesson 17: 24   How To Decrease Experiment Size Solution   Lang En Vs3

The first change would almost certainly reduce the number of pageviews needed. By changing the unit of diversion to be the same as the unit of analysis, the variability of the metric will probably decrease and be closer to the analytical estimate. By decreasing the variability of the metric, you decrease the number of pageviews you need to be confident in your results. The main question here is whether the less consistent experience will be acceptable. In this case, if audacity recalculated the empirical estimate of the standard error using the pageview as the unit of diversion, they might find that the new standard error was 0.0209 for the same sample size. Then only 34,000 pageviews per group would be necessary, which is quite an improvement. Targeting the experiment to English traffic will also reduce the total number of pageviews needed. Since the non-English traffic is not effected, including it will dilute the results of the experiment, which would increase the number of pageviews needed. Of course, there are fewer non-English page views available than total page views. So this might not reduce the time frame of the experiment, but other experiments could be run on the non-English traffic in the meantime. So this could still be worth doing. Filtering the traffic could also impact your choice of practical significance boundary. First, since you're only looking at a subset of your traffic, you might need a bigger change before it matters to the business. Or since your variability is probably lower, you might want to take advantage of that and detect smaller changes rather than decreasing the size of the experiment. Because the practical significance boundary could move in either direction, your size could really move in either direction. But it's likely that the variance will go down and the practical significance boundary will increase, so it's likely that the size will be smaller. In this case, suppose that audacity keeps pageviews as the unit of diversion and then targeting the experiment to English only traffic further reduces the standard error to 0.0188. And they also decide to increase their practical significance boundary to 0.015 for the English traffic only. At this point, they would only need 12,000 pageviews per group. The third change, changing the metric, depends on the specific definition. But it will often not make a significant difference to the variability, especially if you're using a short time window for the probability. If there is a difference, the variability will probably go down. Since the unit of analysis would be the same as the unit of diversion in this case. So this could reduce the number of pageviews needed, but it also might not help much. Again, I did these two sizing calculations with code linked in the Instructor's Notes, if you're curious.

## üìñ Lesson 18: 25   Sizing  Triggering   Lang En Vs3

We just saw that targeting your experiment to the appropriate traffic, for example, to just the English speaking traffic, can reduce the size of your experiment. But you mentioned earlier that you might not always know beforehand what will be affected. Right? &gt;&gt; Yeah. So the first reason you don't know is that you just don't know. So, for example, if you design an improvement to your web page and you don't know which browsers are really going to benefit the most or the least from it, that's something that happens fairly frequently. Or, you could have a feature on your site like language detection, where it classifies it as a language, if say, the number of search results in that language is higher than the number in some other language. These are things which can be fairly hard to predict, exactly, how many of your users are going to be affected. You may be able to ballpark it, but it can be harder to actually target that experiment directly. Now the other thing you want to think about is, you know, how are you actually going to detect impact? If you have a feature that triggers in a certain way, if it's a really obvious UI feature, then you know how many people were actually exposed to it. But if it's something that happens on the back end, you know, do you trigger this particular feature in generating recommendations? You actually want to keep track of what your metric is for, hey, they should have been exposed to my feature, and they actually were exposed to my feature. &gt;&gt; Okay. And then what effect does that have on the size of the experiment? &gt;&gt; Well if you really don't know what fraction of your population is going to be affected, you're going to have to be pretty conservative when you plan how much time and how many users have to see your experiment. Now that said, what I like to do is either run a pilot where you turn on the experiment for a little while and see who's affected, or you can even just use the first day or the first week of data to try to get a better guess at what fraction of your population you're really looking at.

## üìñ Lesson 19: 26   Duration Vs. Exposure   Lang En Vs3

Okay, we've talked about how to size our experiment, what comes next? &gt;&gt; Well we have to translate our ideal size into a set of practical decisions. First of all, what's the duration of the experiment that I want to run? Secondly, when do I want to run the experiment? Is, you know, back to school a good time to run it? What about holidays? Is it going to overlap something that's important and then third you have to think about what fraction of your traffic you're going to send through the experiment. Those are all interrelated as they get you to the ideal size but you need to think about them a little bit separately. &gt;&gt; The fist two make sense. Could you go over the last one a little bit more? &gt;&gt; Sure. So what's that your unit of diversion is a cookie. And so what we're really asking is on any given day what proportion or what percentage of the cookies are you sending to your experiment and your control? Now, let's say we're and we need 1 million cookies in our experiment and our control combined. Now, if you only get a 100,000 cookies visiting your site on any given day. That means that if you want to run 50% of your traffic through the experiment and 50% through the control, you need to run your experiment control for ten days. Now, another choice is to run your experiment at 25% each, say, it's because you want to run another test, then you'd have to run your experiment for 20 days as opposed to 10. And that's how, the duration of your experiment, is related to the proportion of traffic that you're sending through your experiment. Okay, why wouldn't you always run on all of your traffic so you can get results quicker? &gt;&gt; Well, there's a couple of different reasons. The first, which we've touched on before, is safety. So basically, you may have a new UI feature, and you're not sure either how well it functions in all browsers, or I mean, hopefully you know. But you may want to test it. Or how your users are going to react. So they might get frustrated with it. It might not work that well. So you might want to actually keep the site mostly the same, and only expose a few people to it until you feel more comfortable with it. The other reason would be something like press. Let's say it's a new feature. You're not really sure you're going to keep it. Do you really want a lot of people seeing this and potentially blogging about if you're not sure it's even going to be the way you go with the site? &gt;&gt; So, another reason is that right now we're randomizing across the unit of diversion, but the question is what other things are actually impacting the variability of your results? Carrie mentioned something at the beginning of this video which is about you know, what if it ran on a holiday, right? Well, you know, if you're running a 50-50 experiment, then you can gather all data on a single day, would you actually want to make a decision based on a single day if it was a holiday? Well, a more common scenario is that you have to have very different behavior on weekdays and weekends. And so you might actually prefer to run at a smaller percentage across multiple days to get a sense for how the differences are by week day and weekend, across holidays, by different times of day, all of those different types of things that you are actually accounting for those other sources of variability. &gt;&gt; And the other option is, you may be running multiple tasks at your company or you may be running multiple tasks of the same feature with you know, different levels of a parameter or different types of the same feature? And if you really want those things to be directly comparable, the easiest thing to do is to run them at the same time on smaller percentages of traffic. And then you know that if one of the tests was affected by a holiday or a strange shift in your traffic, they all were, and you should be able to compare the relative ordering of the tests against each other. &gt;&gt; Got it.

## üìñ Lesson 20: 27   Duration Vs. Exposure: Example   Lang En Vs3

Okay. Suppose you're considering another experiment and you've computed the size needed to be 1 million pageviews, split across control and experiment groups. If your average traffic per day is 500,000 pageviews, then even if you split all of your traffic evenly between the control and the experiment groups, you'll need to run the experiment for two days. However, is your traffic really the same everyday? It's more likely that you have some weekly variation such as in this graph, where the traffic is lower on the weekends and higher on the weekdays. And if you look at your metric of interests, you might see a variation in that based on what day of the week it is also. In such a case, you should run on a mix of weekend and weekday days. That is you wouldn't want to just run the experiment on two weekdays. And then you might need to run for three days rather than two to get enough traffic, since the weekends don't have as much traffic. And if the change is risky enough that you don't want to expose such a large percentage of your traffic to it, you might run for longer say, seven days with a lower percentage of your traffic diverted.

## üìñ Lesson 21: 28   When To Limit Exposure   Lang En Vs3

Which of the following experiments are risky enough that Audacity might want to limit the number of users who are exposed? That is, for which experiments do you think it would make sense to run the experiment for longer, but expose fewer total users to the change? First, suppose Audacity changes the database they're using in their back end. Second, consider the experiment from lesson one, where Audacity changed the color of the start now button. How risky was this experiment? Next, suppose Audacity tests changing the account creation. Before, each user had to create an account with a new password, but now Audacity tests allowing Facebook login. Finally, what if Audacity tests changing the order of the courses on the course list page? For each change, check the box if you think this is a high enough risk change to limit the number of users who are exposed.

## üìñ Lesson 22: 29   When To Limit Exposure Solution   Lang En Vs3

If everything goes as planned, this first change won't have any user visible changes. So, it wouldn't be risky at all. In practice though, if this kind of change goes wrong, the effects could be huge. Your site might go down, or not work at all. Of course, you should always be testing changes like this, and all changes in the controlled development environment before exposing it to users. But, sometimes new bugs appear when the change is exposed to real traffic. Because of this, it's often a good idea to roll out this kind of a change to a small percentage of users, and make sure nothing goes wrong, before rolling it out to everyone. The second case is low risk. Changing the color a button is innocent enough that even if all your users saw the change, it would probably be fine. But, of course, as always, you should still test the change before rolling it out to users. The third case is higher risk, particularly if you end up not rolling out the experiment. How are you going to deal with all these Facebook logins that you're not supporting? Keeping the affected users to a small number, so that you won't have very many of these to deal with, would be a good idea. Finally, assuming that you've run similar experiments in the past, this last case is low risk also, since most users won't notice ranking changes. If this is the first time you've tested a ranking change, though, then this might be risky for the same reason as the database change. If there's a bug, the courses might not appear at all, for example.

## üìñ Lesson 23: 3   Unit Of Diversion: Example   Lang En Vs3

Like Diane said a unit of diversion is how we define what an individual subject is in the experiment. There are three commonly used categories of unit of diversion. The first two a user identifier and an anonymous identifier are different approximations for an actual user or person. And the last is just the single event. Let's dive into each of these. A user identifier is what most people think of first for running an experiment on. This would be something like the login that people create on websites or apps. For example, your email address if you log into Facebook or Amazon, or your username, if create a username instead. While a person could have more than one login, typically a login is a pretty good proxy for a user and it's stable and unchanging. If you use a user id as your unit of diversion, what that means is that all the events correspond to the same used id are either in the control group or the experiment group, but they are not mixed between the two groups. Whether the user is using an app on their phone, visiting the website on their phone, or visiting the website on their desktop computer, it's a consistent experience. However, it's worth noting that a user id is considered personally identifiable. Some sites use your email address as your login and for sites where you create a username many people use some variation on their name. And either way the site will typically associate some other information with the account, such as the user's email address or phone number, to help with account recovery. An anonymous id is usually something like a cookie. On most websites, whenever a user visits the website, it will write a cookie, which is usually an anonymous random identifier to a file on that device. The cookie is specific to a browser and a device though. If the users switches from Chrome to Firefox, or if they switch from their laptop to their phone, they'll get a different cookie. Users can also choose to clear their cookies, in which case the next time they visit the website they'll get assigned a new one. It's also possible to set your preferences such that every time you close your browser, all your cookies are cleared automatically. In other words, it's much easier for a person to change their cookie than it is for a person to change or clear an account, ie a user identifier. Apps on mobile devices can also use a similar cookie mechanism. Although it can be a bit more cumbersome to clear a cookie on a mobile device or for an app than it is in a browser. Finally, event-based diversion means that on every single event, you redecide whether that event is in the experiment or in the control. This means that a user may not get a consistent experience at all, so this is only appropriate in situations where the changes are not user visible. For example, if you have a ranked list, changes to the order of the list would fall in this category. Most users can't tell or won't notice. There are also a couple of other less commonly used options for unit of diversion. On mobile devices only, there's an option called a device id that's in between a cookie and a user ID. The device device id is typically something that's tied to a specific device, and it's unchangeable by the user. It's also considered identifiable because it's immutable. But it doesn't have the cross device or cross platform consistency that the user identifier might have. Another option is to divert based on IP address, so that any event with the same IP address will be put in the same group. If the user changes location, then they often get a new IP address. To make sure all this is clear, I'll go through an example. Imagine you have a user who does the following sequence of events. They're on desktop, not signed in, and they visit the homepage. They've also cleared their cookies recently. So they're not automatically signed in. So, they sign in again, then they visit a class, and then they start watching a video. After that video, we don't see the user again for a while, but then they pop up on their mobile phone where they're automatically signed in because the login is saved. Then they continue watching the video from where they stopped last. Now suppose you're running an experiment that would affect each of these different pages. For example, maybe you changed something about the navigation bar and it shows up on every page. For each of the different units of diversion we've talked about, user-id, cookie, event, device id and IP address, when would the user be assigned to the same group as before and when could they potentially be switched to the other group? That is, the experimental group if they were in the control and vice-versa. For each case, check the box at the point or points, where the user could be switched from the experiment to control or vice-versa, including the first time that they are assigned to a group. For the IP address diversion, you don't haven't enough information to answer. Clearly, they'll be assigned to a group when they first visit, but after that their IP Address could change at any point and if it does, they could be reassigned.

## üìñ Lesson 24: 30   Learning Effects   Lang En Vs3

Diane, earlier this lesson you mentioned learning effects. Can we talk about those some more? First remind me what they are? &gt;&gt; Sure. So learning effects is basically when you want to measure user learning. Or effectively whether a user is adapting to a change or not. Now we had briefly mentioned these back in lesson one where we talked about two different types of learning effects. Change aversion, where when users first see a change they're like, what is this? I don't like anything. Or a novelty effect which is the exact opposite. Oh, this is a new thing. What's going on? Let me try everything around. Now, in both situations, what happens is that when a user first sees a change, they're going to tend to react in one of these two ways. But over time they're going to probably plateau to a very different behavior. Now, the key issue with trying to measure a learning effect is time. It takes time for you just to actually adapt to a change and often times you don't have the luxury of taking that much time to make a decision. &gt;&gt; Okay, but if you do have that much time to spend, is there anything to keep in mind? &gt;&gt; Sure, there's a bunch of things to keep in mind. Now one of them I mentioned earlier in this lesson which is choosing the unit of diversion correctly. If you want to measure user learning, you need a stateful unit of diversion like a cookie or a user ID. The second issue that we want to take into account is that because a lot of the learning is based on not just a slight time but how often they see the change so we call that a dosage. Then you probably want to be using a cohort as opposed to just a population. And so you would choose a cohort in both the experiment and the control based on either how long they've been exposed to the change or how many times they've seen it. Now the last issue is a question about both the risk and duration. Right? From a duration perspective, because you want to measure a learning effect, this is going to take some amount of time to basically see what's going to be happening. Now, the other thing though, is that it's going to take a long, a long period of time. You don't want to be putting a lot of your users through a change that you're testing over a long period of time because maybe you end up testing other changes. Now, the other thing is one about risk. If you're actually wanting to measure a user learning effect, that means that you're probably a little uncertain about what the effect is going to be, which means that it's probably a higher risk change. Now, both of those mean that you're probably going to want to run it through a small proportion of your users for a longer period of time. &gt;&gt; That makes sense, and is there anything else you would do? &gt;&gt; Well, we've done a lot at Google, because we've done a lot of looking at sort of user learning. Is that we've developed methodology where we use both pre-periods and post-periods. Now, both pre-periods and post-periods are, those uniformity trials. There's a versus a test that we discussed back in lesson three. But what we're doing is instead of using it across the entire system, we're using it in a way that's specific to your experiment and your control. And so what happens is that before you run your A/B test on your experimental control, and you have those populations, you're on a pre-period on the exact same populations but they're receiving the exact same frequence. It's an A versus A test on the same set of users. And what happens in the pre-period is that if you measure any difference between your experiment and your control populations that difference is due to something else. Maybe system variability, user variability, things like that. Now a pre-period I would note, is useful not just for when you want to test user learning, but sort of across the board. So that you know that any difference that you measure in your experiment and control is due to the experiment, and not due to any preexisting and inherent differences in your population. Now, that's what a pre-period is, and that basically says, okay, I don't have any differences in my populations. A post-period is saying, after I run my experiment, my control, I'm going to run another A versus A test. And then, what, what we can say is that if there are any differences in the experiment and the control populations after I've run my experiment, then I can attribute those differences to user learning that happened in the experiment period. And so, that's what we basically do. Now, the key thing that I sort of note is that these are pretty advanced techniques. If you're really trying to measure user learning, hopefully you've run tens, if not, hundreds of experiments already. If not, I'd probably stick to some of the simpler techniques.

## üìñ Lesson 25: 31   Lessons Learned   Lang En Vs3

Carrie and Diane, what's the main advice that you would give to analysts as they're designing their experiments? &gt;&gt; I don't mean to sound trite, but just do it. Right? The work that you're going to do in choosing your diversion, in terms of setting what you need to be consistent? Or choosing your population to say who's actually going to be affected? Or the sizing. All of that work is going to ensure that the work that you do in actually running the experiment, not to mention the work that you're putting your users through of actually seeing how they're going to react to those changes, all of that isn't wasted. So, really, just go through the work of designing your experiment properly. &gt;&gt; Yeah, like we talked about in lesson three, even if your analytic calculations don't turn out to be exactly right, they really give you a good framework for starting to understand how many users you're going to need, what f, type of variability you're looking at, whether what you're looking at is even realistic for your user population. And that's a great starting point, and it forces you to think through a lot of these issues. &gt;&gt; Okay, and is there anything in particular you'd like to call out? &gt;&gt; So, what I'd call out is really the interaction between your choice of metrics, that we discussed in lesson three, with your unit of diversion. Some of your metrics may not be valid to compute given your unit of diversion. And th-, and what can also happen is that the variability of your metric can really change depending on what you choose as your unit of diversion. The other thing to keep in mind is the, is always going to be an iterative process, even if, or especially if, you're setting up a series of experiments or you want to set this up for your company in general. You're going to go through the process of trying to figure out what metrics will work for you, run a few experiments, see if you like the results. You know, can you really see something? And you may have to go back through this process and really settle in something that works for you in terms of your business. &gt;&gt; Now, we've talked a lot about building intuition, and what I would say is that the entire point of building the intuition is that as you're going through this iterative process, you're able to use your intuition to better make guesses about what's going to work. So you basically speed up this process. &gt;&gt; That makes sense. And is there anything else? &gt;&gt; So the last thing that I try to point out is that your choice of unit of diversion and your choice of population makes for really good choices about your invariant choice. We talked about those a little bit in lesson three where you have these metrics that you use as invariance, or basically sanity checks, to make sure that your experiment and your control are run properly. Well, if you choose a unit of diversion of cookie and a population of English, then what you can do is saying, well, I'm going to count how many cookies are in English for both my experiment and my control. And if those are the same, then I have some idea that my experiment has actually been run properly. &gt;&gt; And this is a really great time to keep an eye out for surprises. One of the most common things we see is people design a feature thinking it will only trigger, you know, 5% of the time on their site. And then when they do their first day of an experiment they find that the feature is on 80% of all their traffic. And it's okay to get surprises, but do keep your eye on the data as it starts to come in, because that's a great time to figure out that something's gone wrong.

## üìñ Lesson 26: 32   Conclusion   Lang En Vs3

In this lesson, we first learned about what the different possible choices for a unit of diversion is and why we might have a preference. We also discussed how to choose a population. We then walked through the irrative process of sizing your experiment, and how to choose how long your experiment will run for. When you put these decisions together with what we discussed in lesson one about statistical power and practical significance, you have gone through all the decisions necessary to design your experiment. In the next lesson, we'll be finishing up our course by going through how to evaluate the results from our AB test.

## üìñ Lesson 27: 4   Unit Of Diversion: Example Solution   Lang En Vs3

If you're doing diversion based on the user ID, the user would be assigned to a group when they first logged in. And that assignment would not change. Note that you can't assign the user to a group based on their user ID before they sign in. That means you wouldn't be able to run the experiment on this event, so it wouldn't be in the experiment group or the control group. If you're doing cookie-based diversion, you'd make a decision when the user first visits the home page, and again when they start the mobile app, since they will have a different cookie on their mobile device. If the user never clears their cookies, then this is the answer. But, they could clear their cookies at any point, meaning that they could be reassigned at any other point also. If you did event based diversion, then on every single event, you'd re-decide whether that event was in the experiment group or the control group. And each event could have a different decision. If you are doing device id based diversion, then you'd assign the group at the start of the mobile experience. Since you don't typically have device id's for non-mobile devices, you wouldn't be able to run the experiment on the events before the user switched to their mobile device.

## üìñ Lesson 28: 5   Consistency Of Diversion   Lang En Vs3

&gt;&gt; Now that we've covered some different way you can divert traffic, how would you actually choose between them? &gt;&gt; So there are really three main considerations. The first was user consistency. If you're using a user ID, then the user gets a consistent experiences as they change devices as long as they're signed in. And so for a certain set of changes, so for example, if you're testing how courses are being displayed, then the user will get a consistent experience across devices. Now, on the other hand, if you're testing a change that crosses the sign in, sign out border, then a user ID doesn't work as well. So for example, if you're changing the layout of the page or the location of the sign in bar. In that case, you may want to use a cookie instead, so you get consistency across the sign in and sign out border, but not across devices. &gt;&gt; Okay. So then would you always want to use a user ID or a cookie if you can, so that you will get some type of consistency? &gt;&gt; For user visible changes, you would definitely use a cookie or a user ID. Now the thing is that, there's probably a whole host of changes that are not visible to users. This can range from latency changes to back in [INAUDIBLE] changes or honestly, ranking changes. Most of the time, users can't tell when you change the ranking function. Now, user visibility's one consideration as to whether you want to use a user ID or cookie. The other thing is what you want to measure. So for example, if you want to measure a learning effect, whether or not users adapt to change. In those cases, you also need a stateful unit of diversion like a cookie or user ID. For example, if you're making a latency where, that you're making the site slower and you're trying to see whether or not the user uses the site less. In those cases, you need to use a cookie or a user ID to see what happens across time. So even when the user doesn't notice the change, depending on what you want to measure, you may also choose a user ID or cookie. &gt;&gt; Okay. That makes sense. And how about IP based diversion, how does that compare? &gt;&gt; So generally speaking, I don't find IP based diversion very useful. You don't get the consistency that you get from a user ID or a cookie, because the user's IP address could randomly change depending on what's happening with the provider nor do you get the clean randomization that you get from like event based diversion. Now that said, there's a whole host of changes where IP based diversion may be your only choice. So let's say, for example, your testing out an infrastructure change when you're testing out one hosting provider versus a different hosting provider to understand the impact of latency. In that situation, IP based diversion may really be your only choice. Now when you're doing IP based diversion, one of the key challenges is actually in the analysis. Because what happens with IP based diversion is that you may not get a clean comparison between your experiment and your control. One example of this is, for example, modem dialups. I know it's not very common anymore, but it can still happen. And what happens for some providers is they all aggregate all of those modem dialup users into a single IP address. And so then the question is how do I find that comparable population of users in my control? And so what you're going to do analytically, when you do IP based diversion is doing a lot of post analysis to try and find those good comparisons between your experiment and control.

## üìñ Lesson 29: 6   Consistency Of Diversion: Example   Lang En Vs3

In each of the following example experiments, choose what unit of diversion you think would be needed to ensure enough consistency. That is, if a consistent user experience is not necessary, choose event level diversion. Otherwise, choose cookie diversion, if that is consistent enough. And, choose user-id diversion if that is necessary. First, suppose Audacity is testing a change to the backend which would reduce video load time by 100 milliseconds, but it would be more expensive. They want to know whether the reduced load time will be worth the money. What level of consistency would they need in this case? And second, suppose Audacity has placed the button more prominently on the page by changing the color and increasing the size. Third, suppose Audacity has a search bar that lets students search for classes. And, they test changing the order in which the search results are shown. Finally, suppose Audacity notices a low pass rate for some quizzes, and they test adding additional information to the instructor's notes below the videos before those quizzes to see if this helps. In each case, select what level of consistency Audacity would need.

## üìñ Lesson 30: 7   Consistency Of Diversion: Example Solution   Lang En Vs3

In the first case, users probably won't consciously notice if the video load time is a bit faster or slower, so event based diversion would probably be okay. Like Diane mentioned, you may have some learned effects in this case even if users don't consciously notice the change, in which case you would need to switch to cookie based diversion, but event based should be a good starting point, particularly for a short experiment. In the second case, it would be pretty distracting if the button looked different every time you reloaded the page, so event based diversion wouldn't be a good idea here. Cookie based diversion is probably good enough. If the button looks different on different devices, that's okay, since the UI usually looks a little bit different on different devices anyway. In the third case, like the first, most users won't notice ranking changes, so event based diversion is a good place to start. Again, you might need to change this to cookie based diversion if you notice a learned effect. The fourth case is something that users will almost certainly notice. Cross-device consistency will also be important here, if you want to be able to determine whether the change impacts the pass rate of the quiz. If a student watches the video on their phone, then completes the quiz on their computer, for example, you'll need them to be in the same group both times. Because of this, you'll need to use user-ID based diversion here.

## üìñ Lesson 31: 8   Ethical Considerations For Diversion   Lang En Vs3

You said consistency was the first of three main considerations. What's the second? &gt;&gt; The second is ethical considerations. Do you remember back in lesson two, when we went through the principles and questions that you should keep in mind as you're designing, running and analyzing your experiment? Well, one of those principles and questions was regarding what data you're collecting, and whether the data is identified or not. If you are using a User ID then by definition the data is identified. And so you have all of those security and confidentiality questions that you have to really answer if you're going to gather data by User ID. Now if your internal processes are really good you are going to be good tackling that challenge. But if not you are, you want to think through whether you want to go through all of that work. Now, the other issue that comes up with the choice of unit and of diversion is that of informed consent. If you're using the User ID, you know who the user is, and you can plausibly get user consent. If you're doing a vent based or a cookie based diversion, you don't really know who the user is. It's not really a, necessarily, a consistent experience. And so user consent becomes a little bit less of an issue.

## üìñ Lesson 32: 9   Ethical Considerations: Example   Lang En Vs3

Which of the following experiments might require additional ethical review? Specifically consider the data that would need to be collected in each experiment. For the first experiment, Audacity currently asks users if they would like to receive a monthly newsletter as soon as they finish the first lesson of a course. They want to test instead asking users about the newsletter as soon as they start the course. The diversion is based on user ID. The second experiment is similar, but now Audacity wants to test moving the newsletter prompt to as soon as a user views details about any particular course. Since this may happen before a user has created an account, the diversion is based on cookie rather than user ID. And users who do not have an account yet are asked for their email address, if they wish to receive the newsletter. In the third experiment, Audacity changes some of the information on a course overview page, and measures the click through probability on the enroll button. Again, since users can view the course overview page without having an account, Audacity uses cookie based diversion. In each case, check the box if you think the experiment would require additional ethical review.

# üóÇÔ∏è Section 5: Policy And Ethics For Experiments Subtitles

## üìñ Lesson 1: 1   Introduction   Lang En Vs3

Lessons three, four and five will dive into the technical details of how to run AB tests online. Before we do that, though, we wanted to talk about how to consider the ethics of your AB test and how to protect your users. &gt;&gt; This lesson gives a high level overview of the questions and principles that you should think about while designing, running, and analyzing your experiment. We'll start with some case studies to motivate the importance of thinking through the ethics. And then go through the four major principles to consider, before summarizing the main principles that you should be answering for your proposed AB test. &gt;&gt; Many people and companies don't necessarily give much thought to these issues. After all if your a streaming video service, a review site, or your selling handmade goods or meals, is the service your providing really affecting people to the degree where you need to think about ethics? &gt;&gt; Realistically in most cases that's probably right. &gt;&gt; But, you know, you can get to a gray line pretty quickly if you think about how integrated these services are in people's lives. Even for streaming video, what if those videos are used to teach the Heimlich Maneuver? &gt;&gt; Our goal in this lesson is to highlight the types of issues that you should keep in mind as you design and run experiments. Because those issues may affect little decisions, from what type of user identification you use to divert someone into the experiment, to what data you capture or whether you run the A/B test at all. &gt;&gt; That said, this will only be a high level introduction. Many of these issues are highly nuanced. And our goal is to give you an idea of the basics. If you're really running into potential tests that are questionable, which hopefully these principles will help you determine, then you should be seeking expert advice and counsel.

## üìñ Lesson 2: 10   Which Tests Need Further Review? Solution   Lang En Vs3

The answer in the first case depends primarily on what information is being gathered in the survey. If the survey collects personal information such as names, then the information received in the survey is more sensitive. However, in most cases, this survey should be fine to run if no personal information is collected. Survey data that doesn't collect personal information isn't usually considered sensitive. Since users can chose to respond either not at all or dishonestly. Users can voluntarily choose what to do and its this implicit voluntary consent that's key. The ethical considerations also increase if the user can only read the article in exchange for their survey information, even if there is still that voluntary consent. The mail delivery experiment is probably fine. No new sensitive data is being collected, and the risk to users of different menu layouts is minimal. The heart rate experiment should probably get additional review. Since physical fitness is important to good health, presentations that confuse users about their data or encourage them to exercise too much or too little could pose a serious risk. Also, the data being collected is certainly sensitive and needs to be carefully secured and anonymized. Depending on the data, it may not be possible to anonymize it while keeping the same level of granularity in which case you may need to aggregate the data.

## üìñ Lesson 3: 11   Provided Information   Lang En Vs3

If you run a website or app, which of the following pieces of information is ethically necessary to provide to users? Check all that apply. A Terms of Service, also called a TOS, or a Privacy Policy that clearly states what data you collect and how you use it. A history of how your company has been funded. A navigation bar at the top of the page, allowing users to visit the different sections of your site. A list of all the experiments you are planning on running on your users in the near future. Or a search bar that lets users search the site.

## üìñ Lesson 4: 12   Provided Information Solution   Lang En Vs3

As mentioned previously, a clear statement of what data you collect and how you use it, is ethically necessary, assuming you collect any data about your users at all. A history of your company's funding, on the other hand, is not required. And while it's probably a good idea for most sites to have a navigation bar, this isn't an ethical requirement. A list of all experiments might sounds nice, since then, users can be fully informed about what experimentation they are undergoing. However, for some experiments, giving out this information could skew your results, and for experiments with minimal risk that properly protect sensitive data, this measure is not necessary. A search bar, like a navigation bar, might be a good feature for many sites to include, but it's not ethically necessary.

## üìñ Lesson 5: 13   Internal Training   Lang En Vs3

Now, suppose you are designing a training program for people who run A/B tests at your company. Which of the following is it ethically necessary for anyone who runs A/B test to know? Should they know which questions to consider when evaluating the ethics of a proposed test? How about the history of A/B testing? Or the history of IRBs. Should you ensure that they acknowledge a data policy that details what data uses are acceptable during A/B testing? Finally, should you teach them the principles to uphold when running A/B tests? Check each that is ethically necessary.

## üìñ Lesson 6: 14   Internal Training Solution   Lang En Vs3

Knowing which questions to consider to evaluate the ethics of a proposed test is definitely important. As is a data policy so test-runners will know how they can use the data they collect. We hope you agree that the history of A/B testing and of IRBs are interesting, but they're not ethically necessary. Knowing what principles to uphold when running A/B tests is also an important thing to know. In fact, it would be hard to cover the first point well without teaching this too.

## üìñ Lesson 7: 15   Conclusion   Lang En Vs3

At this point you hopefully have a grasp of the major policy and ethical considerations to take into account when assessing your A/B test. In the next lesson, we're going to dive back into the technical details starting with how to choose which project to use for evaluating your experiment.

## üìñ Lesson 8: 2   Tuskegee And Milgram Experiments   Lang En Vs3

A/B testing in technology, like many experiments in anthropology, psychology, sociology and medicine, are on actual people. Because of this, you need to ensure that the participants are adequately protected. To explain this, let's go through a few examples of cases where participants may not have been adequately protected. One famous example from medical history is the Tuskegee syphilis experiment, where approximately 600 rural African-American men in Tuskegee, Alabama, between 1932 and 1972, thought they were receiving free healthcare from the government. But in fact, they were being cited regarding the natural progression of untreated syphilis. This study continued even after penicillin was established as a standard treatment for syphilis and the men were both not treated and not informed of a possible treatment. Another example from psychology is the Milgram experiment, done in the early 1960s, where Yale professor, Stanley Milgram, measured the willingness of studied participants to obey an authority figure who instructed them to administer electric shocks to a third person. That third person was an actor who simply pretended to be shocked with gradually increasing responses and pleas to stop. The question was, how far would the participants would go in causing pain with prompts from an authority figure? In both cases, participants were subject to a high risk for permanent harm. Untreated syphilis can cost death, and participants of the Milgram experiment may have been damaged psychologically.

## üìñ Lesson 9: 3   Facebook Experiment   Lang En Vs3

Our recent online example is from Facebook and some Cornell researchers, who studied how emotions can be spread on social media. Specifically, one set of randomly selected participants saw a newsfeed with slightly fewer negative posts and another set saw a newsfeed with slightly fewer positive posts. A week later, the researchers measured whether the posts from the participants were more negative or positive. For the Facebook experiment, the risk to the participants is substantially lower than either of the other experiments, and there is no discussion about the potential benefits that might result from such a study. Now, these are three examples of problematic, or potentially problematic, experiments. Clearly, these are the exceptions to the rule, rather than the common case. But it is because of these problematic experiments that IRBs, or institutional review boards, came to be. IRBs review possible experiments and ensure that participants are adequately protected. That said, most IRBs are tied to academic institutions and it's still a grey area about whether Internet studies should be reviewed by an IRB. However, online say you should still make protecting participants a primary concern. The four main principles to consider are, one, risk. What risk is the participant being exposed to? Two, benefit. What benefits might be the outcome of the study? Three, choice. What other choices do participants have? And finally, four, privacy. What expectation of privacy and confidentiality do participants have? We'll discuss each of these next.

## üìñ Lesson 10: 4   Assessing Risk   Lang En Vs3

Now I want you to assess the participant risk of a few possible experiments. For each experiment, choose whether you think the risk is minimal or more than minimal. That is beyond the risks that may be encountered in normal life. First, suppose you were on a search engine such as Bing and you're testing a change adding information to your results page. Specifically for shopping queries, you're adding price information available from product sites. How much risk would this cause the users of your website? Second, suppose you've created an app that helps users monitor their diet, exercise, and sleep. you're considering adding a feature that gives users dietary advice and lets them know the possible consequences of their current diets. For example, your app might let a user know that their current sugar intake puts them at high risk for developing diabetes, and advise that they eat less sugar. Would it pose beyond a minimal risk to test this change? Finally, suppose you run an online encyclopedia, such as Wikipedia, and you want to test changing the location of the search bar. What level of risk would this present? For each change, check the box if you think testing the change would pose more than a minimal risk to the website's users.

## üìñ Lesson 11: 5   Assessing Risk Solution   Lang En Vs3

The search engine case is a minimal risk. The search engine is simply providing additional information that was already publicly available. The health app case is potentially more than minimal risk. This is providing medical advice tailored to individuals. So if the advice is incorrect, it could cause users significant health risks. Also, depending on the exact information given, the app may be subject to additional FDA regulation. The encyclopedia case is minimal risk. Changing the location of a search bar is a pretty harmless change.

## üìñ Lesson 12: 6   Assessing Data Sensitivity   Lang En Vs3

Suppose your company is considering collecting or storing the following data either to use in an A/B test or for some other purpose. In which cases would the data be considered sensitive? In other words, people would be at risk if the data were exposed to non-trusted individuals. First, suppose you are considering analyzing existing census data. Containing race, age and gender distributions broken down by zip code. Is this census data sensitive? Next suppose you're interested in how many users visit some specific websites. So you use a source like comScore or HitWise to gather a count of the number of users visiting those sites each day. ComScore and Hitwise are products providing this kind of online analytics. Or what if you have a mobile app that helps diabetics track their glucose levels, and you want to store each glucose level and the time it was entered? You don't store the person's name with the data, but you want to know which data points came from the same person, so you do store an anonymous ID with each entry. Next, suppose you have an online game and you store things like each time a user gains an achievement, kills an enemy, or advances to the next level. Again, you associate this information with an anonymous ID. Or suppose you have an online shopping company, like Amazon, and you store the number of purchases made on your site and the total money spent, broken down by zip code. Finally, suppose again you have an online shopping company and you want to store your users' credit card information to save it for their next visit. In each case, check the box if you think the data is sensitive.

## üìñ Lesson 13: 7   Assessing Data Sensitivity Solution   Lang En Vs3

The census data is not sensitive. On one hand, gender and age are protected data and considered sensitive, however, the data being discussed is aggregated to a granularity that does not allow for re-identification of individuals. As such, storing and analyzing it on your own servers does not carry any privacy concerns. The data about how many people visit various web sites is also not sensitive, even if it's for a health site or a porn site. There's no way to tell who is visiting the sites based on this data. You're only collecting the total number of visitors. The glucose data could potentially be sensitive, even though it's anonymous. The exact timestamps could be enough information to link the data to a specific person in some extreme circumstances. It's pretty unlikely that anyone would actually be able to do this, even if it were theoretically possible. But glucose levels are private health data, so it's important to be extra safe. Health data is subject to additional regulation, such as HIPAA which specifies that timestamps are considered personally identifiable. Certainly if the ID you store with the data is not actually anonymous and is possible to recover a name, an email address, or a phone number from it, then this would be sensitive data. The game data is not sensitive. Again, it's possible that you could recover the person's identity from details such as the time stamps, but that's not a very likely outcome. Moreover, since game data is not sensitive, even if someone could recover the details, that would be low risk for the participant. The sales data for the shopping site broken down by zip code is not sensitive, because the totals are stored for a zip code. Typically, a zip code has enough individuals active in that area that information in that level of granularity does not have a re-identification risk. A better guideline, though, rather than using zip code, would be to ensure that there are enough data points to guarantee that the data is sufficiently anonymized. The credit card data is definitely sensitive If you entered your credit card number on a shopping site, you would definitely expect that site to be storing the data securely.

## üìñ Lesson 14: 8   Questions And Consent   Lang En Vs3

For online A/B testing then, the questions come down to, 1, are users informed about the data gathering in some form, such as via a terms and services, or a privacy policy? 2, what user identifiers are tied to the data being gathered? Especially if there is personally identifiable information, such as name, email, phone number, address, then the data collected becomes much more sensitive, and subject to review. 3, what type of data is being collected? How sensitive is it? Is there health or financial data being collected? 4, what level of confidentiality and security is the data subject to? Can anyone at the company access the data, especially if it is personally identifiable, or is the data secured with accesses logged and audited? It is worth noting that these concerns should apply to online services, period, regardless of whether AB testing is happening or not. Now, one additional question that often gets asked is, what about informed consent? Informed consent is a process by which participants are told that the risks that they may face if they participate in the study. What benefits might result. What other options they have. What data is being gathered, and how that data is being handled. Typically, informed consent is a process by which participants are given a document that has all of that information. And then, participants can choose whether to participate in the study, or not. When IRB's are assessing a study, they also get to decide whether an informed consent is required, or not. While many people may think that informed consent is required whenever a study is considered human subject research. In fact, situations where informed consent may not be required are when the risk is minimal, and the data being gathered is not identified.

## üìñ Lesson 15: 9   Which Tests Need Further Review?   Lang En Vs3

Now, consider the following hypothetical AB tests. If you where planning to run each of these tests, which cases are the most ethically concerning according to the criteria just outlined? In each case, check the box if you would seek further review or possibly obtain user consent, rather than moving forward without further review. And first, suppose you want a news website with articles that give financial advice such as whether to invest in index funds or pick individual stocks. You want users to take a survey and answer which of five broad ranges includes their net worth. You want to test various methods of getting users to take the survey. One method is an option to take the survey at the bottom of the article. And another is to prompt the user to take the survey after reading the first page, and not allow them to continue to the second page until they've responded. Would you seek out further review or move forward with this experiment? Next, suppose you work at a company that delivers meals and you want to test different layouts of the menu to see if users order food more often with particular layouts. Would you seek out review in this case? Finally, suppose your company builds devices that measure heart rate and an app that allows users to track their heart rate during various activities. You want to test whether different ways of presenting the heart rate data leads users to increase how much they exercise. Again, check the box if you would seek further ethical review of this experiment.

